{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10bd1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully using the manual method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pasin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pasin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pasin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Now, try to download the data again\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"NLTK data downloaded successfully using the manual method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7d22901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nirvana (band) /n\n",
      "Text: Nirvana was an American rock band formed in Aberdeen, Washington, in 1987. Founded by lead singer and guitarist Kurt Cobain and bassist Krist Novoselic, the band went through a succession of drummers, most notably Chad Channing, before recruiting Dave Grohl in 1990. Nirvana's success popularized alternative rock, and they were often referenced as the figurehead band of Generation X. Their music maintains a popular following and continues to influence rock culture.\n",
      "In the late 1980s, Nirvana established itself as part of the Seattle grunge scene, releasing its first album, Bleach, for the independent record label Sub Pop in 1989. They developed a sound that relied on dynamic contrasts, often between quiet verses and loud, heavy choruses. After signing to the major label DGC Records in 1990, Nirvana found unexpected mainstream success with \"Smells Like Teen Spirit\", the first single from its landmark second album, Nevermind (1991). A cultural phenomenon of the 1990s, Nevermind was certified diamond and is credited for ending the popularity of hair metal.\n",
      "Characterized by a punk aesthetic, Nirvana's fusion of pop melodies with noise, combined with its themes of abjection and social alienation, brought them global popularity. Following extensive touring and the 1992 compilation album Incesticide and EP Hormoaning, the band released its highly anticipated third studio album, In Utero (1993). The album topped both the US and UK album charts, and was acclaimed by critics. Nirvana disbanded following Cobain's suicide in April 1994. Further releases have been overseen by Novoselic, Grohl, and Cobain's widow, Courtney Love. The live album MTV Unplugged in New York (1994) won Best Alternative Music Performance at the 1996 Grammy Awards.\n",
      "Nirvana is one of the best-selling bands of all time, having sold more than 75 million records worldwide. During its three years as a mainstream act, Nirvana received an American Music Award, Brit Award, and Grammy Award, as well as seven MTV Video Music Awards and two NME Awards. The band achieved five number-one hits on the Billboard Alternative Songs chart and four number-one albums on the Billboard 200. In 2010, Rolling Stone ranked Nirvana No. 30 in its list of the \"100 Greatest Artists of All Time\". Their members were inducted into the Rock and Roll Hall of Fame in their first year of eligibility in 2014. In 2023, they were honored with the Grammy Lifetime Achievement Award.\n",
      "\n",
      "History\n",
      "1987–1988: Formation and early years\n",
      "Singer and guitarist Kurt Cobain and bassist Krist Novoselic met while attending Aberdeen High School in Washington state. The pair became friends while frequenting the practice space of the Melvins. Cobain wanted to form a band with Novoselic, but Novoselic did not respond for a long period. Cobain gave him a demo tape of his project Fecal Matter. Three years after the two first met, Novoselic notified Cobain that he had finally listened to the Fecal Matter demo and suggested they start a group. Their first band, the Sellouts, was a Creedence Clearwater Revival tribute band. The project featured Novoselic on guitar and vocals, Cobain on drums, and Steve Newman on bass but only lasted a short time. Another project, this time featuring originals, was also attempted in late 1986. Bob McFadden was enlisted to play drums, but after a month this project also fell through. In early 1987, Cobain and Novoselic recruited drummer Aaron Burckhard. They practiced material from Cobain's Fecal Matter tape but started writing new material soon after forming.\n",
      "During its initial months, the band went through a series of names, including Skid Row, Pen Cap Chew, Bliss, and Ted Ed Fred. The band played under the name Nirvana for the first time on March 19, 1988, at Community World Theater, Tacoma, Washington, together with the bands Lush and Vampire Lezbos. This concert's flyer, designed by Kurt Cobain, also mentioned all of the previous band names: \"Nirvana (also known as... Skid Row, Ted Ed Fred, Pen Cap Chew, Bliss)\". The group settled on Nirvana because, according to Cobain, \"I wanted a name that was kind of beautiful or nice and pretty instead of a mean, raunchy punk name like the Angry Samoans.\" The band were initially sued by the British band Nirvana over the usage of the name, reaching an out-of-court settlement. Novoselic moved to Tacoma and Cobain to Olympia, Washington. They temporarily lost contact with Burckhard, and instead practiced with Dale Crover of the Melvins. Nirvana recorded its first demos in January 1988.\n",
      "In early 1988, Crover moved to San Francisco but recommended Dave Foster as his replacement on drums. Foster's tenure with Nirvana was a rocky one; during a stint in jail, he was replaced by Burckhard, who again departed after telling Cobain he was too hungover to practice one day. Foster would rejoin the band, but after Cobain and Novoselic were introduced to drummer Chad Channing, the band would permanently dismiss him (although not before Foster witnessed the group play live without him). Channing continued to jam with Cobain and Novoselic; however, by Channing's account, \"They never actually said 'okay, you're in.'\" Channing played his first show with Nirvana in late May 1988.\n",
      "\n",
      "1988–1990: Early releases\n",
      "Nirvana released its first single, a cover of Shocking Blue's \"Love Buzz\", in November 1988 on the Seattle independent record label Sub Pop. They did their first interview with John Robb in Sounds, which made their release its single of the week. The following month, the band began recording its debut album, Bleach, with local producer Jack Endino. Bleach was influenced by the heavy dirge-rock of the Melvins, the 1980s punk rock of Mudhoney, and the 1970s heavy metal of Black Sabbath. The money for the recording sessions for Bleach, listed as $606.17 on the album sleeve, was supplied by Jason Everman, who was subsequently brought into the band as the second guitarist. Though Everman did not play on the album, he received a credit on Bleach because, according to Novoselic, they \"wanted to make him feel more at home in the band\". Prior to the album's release, Nirvana became the first band to sign an extended contract with Sub Pop.\n",
      "Bleach was released in June 1989, and became a favorite of college radio stations. Nirvana embarked on its first national tour, but canceled the last few dates and returned to Washington state due to increasing differences with Everman. No one told Everman he was fired; Everman later said he had quit. Although Sub Pop did not promote Bleach as much as other releases, it was a steady seller, and had initial sales of 40,000 copies. However, Cobain was upset by the label's lack of promotion and distribution. In late 1989, Nirvana recorded the Blew EP with producer Steve Fisk. In an interview with Robb, Cobain said the band's music was changing: \"The early songs were really angry... But as time goes on the songs are getting poppier and poppier as I get happier and happier. The songs are now about conflicts in relationships, emotional things with other human beings.\"\n",
      "\n",
      "In April 1990, Nirvana began working on their next album with producer Butch Vig at Smart Studios in Madison, Wisconsin. Cobain and Novoselic became disenchanted with Channing's drumming, and Channing expressed frustration at not being involved in songwriting. As bootlegs of Nirvana demos with Vig began to circulate in the music industry and draw attention from major labels, Channing left the band. That July, Nirvana recorded the single \"Sliver\" with Mudhoney drummer Dan Peters. Dale Crover filled in on drums on Nirvana's seven-date American West Coast tour with Sonic Youth that August.\n",
      "In September 1990, Buzz Osborne of the Melvins introduced the band to drummer Dave Grohl, whose Washington, D.C. band Scream had broken up. Grohl auditioned for Novoselic and Cobain days after arriving in Seattle; Novoselic later said, \"We knew in two minutes that he was the right drummer.\" Grohl told Q: \"I remember being in the same room with them and thinking, 'What? That's Nirvana? Are you kidding?' Because on their record cover they looked like psycho lumberjacks... I was like, 'What, that little dude and that big motherfucker? You're kidding me'.\"\n",
      "\n",
      "1991–1992: Nevermind and mainstream breakthrough\n",
      "Disenchanted with Sub Pop, and with the Smart Studios sessions generating interest, Nirvana sought a deal with a major record label since no indie label could buy them out of their contract. Cobain and Novoselic consulted Soundgarden and Alice in Chains manager Susan Silver for advice. They met Silver in Los Angeles and she introduced them to agent Don Muller and music business attorney Alan Mintz, who was specialized in finding deals for new bands. Mintz started sending out Nirvana's demo tape to major labels looking for deals. Following repeated recommendations by Sonic Youth's Kim Gordon, Nirvana signed to DGC Records in 1990. When Nirvana was inducted into the Rock and Roll Hall of Fame in 2014, Novoselic thanked Silver during his speech for \"introducing them to the music industry properly\".\n",
      "After signing, the band began recording its first major label album, Nevermind. The group was offered a number of producers, but held out for Vig. Rather than record at Vig's Madison studio as they had in 1990, production shifted to Sound City Studios in Van Nuys, Los Angeles, California. For two months, the band worked through a variety of songs. Some, such as \"In Bloom\" and \"Breed\", had been in Nirvana's repertoire for years, while others, including \"On a Plain\" and \"Stay Away\", lacked finished lyrics until midway through the recording process. After the recording sessions were completed, Vig and the band set out to mix the album. However, the recording sessions had run behind schedule and the resulting mixes were deemed unsatisfactory. Slayer mixer Andy Wallace was brought in to create the final mix. After the album's release, members of Nirvana expressed dissatisfaction with the polished sound that Wallace had given Nevermind.\n",
      "\n",
      "Initially, DGC Records was hoping to sell 250,000 copies of Nevermind, the same they had achieved with Sonic Youth's Goo. However, the first single, \"Smells Like Teen Spirit\", quickly gained momentum, boosted by major airplay of the music video on MTV. As it toured Europe during late 1991, the band found that its shows were dangerously oversold, that television crews were becoming a constant presence onstage, and that \"Smells Like Teen Spirit\" was almost omnipresent on radio and music television. By Christmas 1991, Nevermind was selling 400,000 copies a week in the US. In January 1992, the album displaced Michael Jackson's Dangerous at number one on the Billboard album charts, and topped the charts in numerous other countries. The month Nevermind reached number one, Billboard proclaimed, \"Nirvana is that rare band that has everything: critical acclaim, industry respect, pop radio appeal, and a rock-solid college/alternative base.\" The album eventually sold over seven million copies in the United States and over 30 million worldwide. Nirvana's sudden success was credited for popularizing alternative rock and ending the popularity of hair metal.\n",
      "Citing exhaustion, Nirvana did not undertake another American tour in support of Nevermind, and made only a handful of performances later that year. In March 1992, Cobain sought to reorganize the group's songwriting royalties (which to this point had been split equally) to better represent that he wrote the majority of the music. Grohl and Novoselic did not object, but when Cobain wanted the agreement to be retroactive to the release of Nevermind, the disagreements came close to breaking up the band. After a week of tension, Cobain received a retroactive share of 75 percent of the royalties. Bad feelings about the situation remained within the group afterward.\n",
      "Amid rumors that the band was disbanding due to Cobain's health, Nirvana headlined the closing night of the 1992 Reading Festival in England. Cobain programmed the performance lineup. Nirvana's performance at Reading is often regarded as one of the most memorable of their career. A few days later, Nirvana performed at the MTV Video Music Awards; despite the network's refusal to let the band play the new song \"Rape Me\", Cobain strummed and sang the first few bars of the song before breaking into \"Lithium\". The band received awards for the Best Alternative Video and Best New Artist categories.\n",
      "DGC had hoped to have a new Nirvana album ready for a late 1992 holiday season; instead, it released the compilation album Incesticide in December 1992. A joint venture between DGC and Sub Pop, Incesticide collected various rare Nirvana recordings and was intended to provide the material for a better price and higher quality than bootlegs. As Nevermind had been out for 15 months and had yielded a fourth single in \"In Bloom\" by that point, Geffen/DGC opted not to heavily promote Incesticide, which was certified gold by the Recording Industry Association of America the following February.\n",
      "\n",
      "1993: In Utero\n",
      "In February 1993, Nirvana released \"Puss\" / \"Oh, the Guilt\", a split single with the Jesus Lizard, on the independent label Touch & Go. For their third album, Nirvana chose producer Steve Albini, who had a reputation as principled and opinionated in the American indie music scene. While some speculated that Nirvana chose Albini for his underground credentials, Cobain said they chose him for his \"natural\" recording style, without layers of studio trickery. Albini and Nirvana recorded the album in two weeks in Pachyderm Studio in Cannon Falls, Minnesota, that February for $25,000.\n",
      "After its completion, stories ran in the Chicago Tribune and Newsweek that quoted sources claiming DGC considered the album \"unreleasable\". Fans became concerned that Nirvana's creative vision might be compromised by their label. While the stories about DGC shelving the album were untrue, the band was unhappy with certain aspects of Albini's mixes; they thought the bass levels were too low, and Cobain felt that \"Heart-Shaped Box\" and \"All Apologies\" did not sound \"perfect\". The longtime R.E.M. producer Scott Litt was called in to remix the two songs, with Cobain adding more instrumentation and backing vocals.\n",
      "In Utero topped the American and British album charts. Time critic Christopher John Farley wrote in his review, \"Despite the fears of some alternative-music fans, Nirvana hasn't gone mainstream, though this potent new album may once again force the mainstream to go Nirvana.\" In Utero went on to sell more than five million copies in the United States. That October, Nirvana embarked on its first tour of the United States in two years, with support from Half Japanese and the Breeders. For the tour, the band added Pat Smear of the punk rock band Germs as the second guitarist.\n",
      "In November, Nirvana recorded a performance for the television program MTV Unplugged. Augmented by Smear and cellist Lori Goldston, they broke convention for the show by choosing not to play their best known songs. Instead, they performed several covers, and invited Cris and Curt Kirkwood of the Meat Puppets to join them for renditions of three Meat Puppets songs.\n",
      "In early 1994, Nirvana embarked on a European tour. Their final concert took place in Munich, Germany, on March 1. In Rome, on the morning of March 4, Cobain's wife, Courtney Love, found Cobain unconscious in their hotel room and he was rushed to the hospital. Cobain had reacted to a combination of prescribed rohypnol and alcohol. The rest of the tour was canceled.\n",
      "\n",
      "1994–1996: Death of Cobain and disbandment\n",
      "In the weeks following his hospitalization in Rome, Cobain's heroin addiction resurfaced. Following an intervention, he was persuaded to enter drug rehabilitation. After less than a week, Cobain scaled the 6-foot wall and escaped, then returned to Seattle. On April 6, 1994, it was announced that Nirvana withdrew from their planned appearance at the Lollapalooza 94 tour due to Cobain's ongoing health problems, with reports that they had broken up. Two days later, on April 8, Cobain was found dead of a self-inflicted shotgun wound to the head at his home in the Denny-Blaine neighborhood of the city. He had died approximately three days earlier. Until the discovery of his body, Cobain had been missing since escaping the rehabilitation center.\n",
      "Cobain's death drew international attention and became a topic of public fascination and debate. Within hours, stocks ran low of Nirvana records in stores, and Nirvana sales rose dramatically in the United Kingdom. Unused tickets for Nirvana concerts sold for inflated prices on the used market. The inflation was triggered by the manager of Brixton Academy, who lied on BBC Radio 1 that fans were purchasing tickets as a \"piece of history\", in an effort to retain the money he stood to lose from ticket refunds. A public vigil for Cobain was held on April 10, 1994, at a park at Seattle Center, drawing approximately 7,000 mourners, followed by a final ceremony on May 31, 1999.\n",
      "In 1994, Grohl founded a new band, the Foo Fighters. He and Novoselic decided against Novoselic joining. Grohl said it would have felt \"really natural\" for them to work together again, but would have been uncomfortable for the other band members and placed more pressure on Grohl. Novoselic turned his attention to political activism.\n",
      "Plans for a live Nirvana album, Verse Chorus Verse, were canceled as Novoselic and Grohl found assembling the material so soon after Cobain's death emotionally overwhelming. Instead, in November 1994, DGC released the MTV Unplugged performance as MTV Unplugged in New York. It debuted at number one on the Billboard charts and earned Nirvana a Grammy Award for Best Alternative Music Album at the 1996 Grammys. It was followed by Nirvana's first full-length VHS live video, Live! Tonight! Sold Out!!. In 1996, the live album From the Muddy Banks of the Wishkah became the third consecutive Nirvana release to debut at the top of the Billboard album chart.\n",
      "\n",
      "1997–2005: Conflicts with Courtney Love\n",
      "In 1997, Novoselic, Grohl and Love formed the limited liability company Nirvana LLC to oversee Nirvana projects. A 45-track box set of Nirvana rarities was scheduled for release in October 2001. However, shortly before the release date, Love filed a suit to dissolve Nirvana LLC, and an injunction was issued preventing the release of any new Nirvana material until the case was resolved. Love contended that Cobain was Nirvana, that Grohl and Novoselic were sidemen, and that she had signed the partnership agreement originally under bad advice. Grohl and Novoselic countersued, asking the court to remove Love from the partnership and to replace her with another representative of Cobain's estate.\n",
      "The day before the case was set to go to trial in October 2002, Love, Novoselic, and Grohl announced that they had reached a settlement. The next month, the best-of compilation Nirvana was released, featuring the previously unreleased track \"You Know You're Right\", the last song Nirvana recorded. It debuted at number three on the Billboard album chart. The box set, With the Lights Out, was released in November 2004. The release contained early Cobain demos, rough rehearsal recordings, and live tracks. An album of selected tracks from the box set, Sliver: The Best of the Box, was released in late 2005.\n",
      "\n",
      "2006–present: Further reissues and reunions\n",
      "In April 2006, Love sold 25 percent of her stake in the Nirvana song catalog to Primary Wave for an estimated $50 million. She sought to assure Nirvana's fanbase that the music would not simply be licensed to the highest bidder: \"We are going to remain very tasteful and true to the spirit of Nirvana while taking the music to places it has never been before.\"\n",
      "Live! Tonight! Sold Out!!, was re-released on DVD in 2006, followed by the full version of MTV Unplugged in New York on DVD in 2007. In November 2009, Nirvana's performance at the 1992 Reading Festival was released on CD and DVD as Live at Reading, alongside a deluxe 20th-anniversary edition of Bleach. DGC released a number of 20th-anniversary deluxe packages of Nevermind in September 2011, which included the Live at the Paramount show, and of In Utero in September 2013, which included the Live and Loud show.\n",
      "In 2012, Grohl, Novoselic, and Smear joined Paul McCartney at 12-12-12: The Concert for Sandy Relief. The performance featured the premiere of a new song written by the four, \"Cut Me Some Slack\". A studio recording was released on the soundtrack to Sound City, a documentary film by Grohl. On July 19, 2013, the group played with McCartney again during the encore of his Safeco Field \"Out There\" concert in Seattle, the first time Nirvana members had performed together in their hometown in over 15 years.\n",
      "In 2014, Cobain, Novoselic, and Grohl were inducted into the Rock and Roll Hall of Fame. At the induction ceremony, Novoselic, Grohl and Smear performed a four-song set with guest vocalists Joan Jett, Kim Gordon, St. Vincent and Lorde. Novoselic, Grohl and Smear then performed a full show at Brooklyn's St. Vitus Bar with Jett, Gordon, St. Vincent, J Mascis and John McCauley as guest vocalists. Grohl thanked Burckhard, Crover, Peters and Channing for their time in Nirvana. Everman also attended.\n",
      "At Clive Davis' annual pre-Grammy party in 2016, Novoselic and Grohl reunited to perform the David Bowie song \"The Man Who Sold the World\", which Nirvana had covered in their MTV Unplugged performance. Beck accompanied them on acoustic guitar and vocals. In October 2018, Novoselic and Grohl reunited during the finale of the Cal Jam festival at Glen Helen Amphitheater in San Bernardino County, California, joined by Jett and John McCauley on vocals. In January 2020, Novoselic and Grohl reunited for a performance at a benefit for the Art of Elysium at the Hollywood Palladium, joined by Beck, St Vincent, and Grohl's daughter Violet.\n",
      "For the 30th anniversary of Nevermind, in September 2021, the BBC broadcast the documentary When Nirvana Came to Britain, featuring interviews with Grohl and Novoselic. That month, a 30th-anniversary edition of Nevermind was announced, containing 70 previously unreleased live tracks from four concerts and a Blu-ray of Live in Amsterdam. For the 30th anniversary of In Utero, DGC reissued it in several formats on October 27, 2023, which included the full 1993 show at the Great Western Forum in Los Angeles and the 1994 show at the Seattle Centre Arena.\n",
      "On January 30, 2025, Novoselic, Grohl and Smear reunited for the first time in five years to perform at the Fire Aid benefit concert in Los Angeles. They were joined by St. Vincent for \"Breed\", Kim Gordon for \"School\", Joan Jett for \"Territorial Pissings\", and Violet Grohl for \"All Apologies\". For the 50th anniversary celebrations for Saturday Night Live on February 14, Novoselic, Grohl and Smear reunited to perform \"Smells Like Teen Spirit\" with Post Malone.\n",
      "\n",
      "Artistry\n",
      "Musical style and influences\n",
      "Nirvana's musical style has been mainly described as grunge, alternative rock, and punk rock. They have also been labeled as hard rock. Characterized by their punk aesthetic, Nirvana often fused pop melodies with noise. Billboard described their work as a \"genius blend of Kurt Cobain's raspy voice and gnashing guitars, Dave Grohl's relentless drumming and Krist Novoselic's uniting bass-work that connected with fans in a hail of alternately melodic and hard-charging songs\".\n",
      "Cobain described Nirvana's initial sound as \"a Gang of Four and Scratch Acid ripoff\". When Nirvana recorded Bleach, Cobain felt he had to fit the expectations of the Sub Pop grunge sound to build a fanbase, and suppressed his arty and pop songwriting in favor of a more rock sound. Nirvana biographer Michael Azerrad argued, \"Ironically, it was the restrictions of the Sub Pop sound that helped the band find its musical identity.\" Azerrad stated that by acknowledging that they had grown up listening to Black Sabbath and Aerosmith, they had been able to move on from their derivative early sound.\n",
      "Nirvana used dynamic shifts that went from quiet to loud. Cobain sought to mix heavy and pop musical sounds, saying, \"I wanted to be totally Led Zeppelin in a way and then be totally extreme punk rock and then do real wimpy pop songs.\" When Cobain heard the Pixies' 1988 album, Surfer Rosa, after recording Bleach, he felt it had the sound he wanted to achieve but had been too intimidated to try. The Pixies' subsequent popularity encouraged Cobain to follow his instincts as a songwriter. Like the Pixies, Nirvana moved between \"spare bass-and-drum grooves and shrill bursts of screaming guitar and vocals\". Near the end of his life, Cobain said the band had become bored of the \"limited\" formula, but expressed doubt that they were skilled enough to try other dynamics.\n",
      "\n",
      "Instrumentation\n",
      "Cobain's rhythm guitar style, which relied on power chords, low-note riffs, and a loose left-handed technique, featured the key components to the band's songs. Cobain would often initially play a song's verse riff in a clean tone, then double it with distorted guitars when he repeated the part. In some verses, the guitar would be absent to allow the drums and bass guitar to support the vocals, or it would only play sparse melodies like the two-note pattern used in \"Smells Like Teen Spirit\". Cobain rarely played standard guitar solos, opting to play variations of the song's melody as single-note lines. Cobain's solos were mostly blues-based and discordant, which music writer Jon Chappell described as \"almost an iconoclastic parody of the traditional instrumental break\", a quality typified by the note-for-note replication of the lead melody in \"Smells Like Teen Spirit\" and the atonal solo for \"Breed\". The band had no formal musical training; Cobain said: \"I have no concept of knowing how to be a musician at all whatsoever... I couldn't even pass Guitar 101.\"\n",
      "Grohl's drumming \"took Nirvana's sound to a new level of intensity\". Azerrad stated that Grohl's \"powerful drumming propelled the band to a whole new plane, visually as well as musically\", noting, \"Although Dave is a merciless basher, his parts are also distinctly musical—it wouldn't be difficult to figure out what song he was playing even without the rest of the music\".\n",
      "Until early 1992, the band had performed live in concert pitch. They began tuning down either a half step or full step as well as concert pitch. Sometimes all three tunings would be in the same show. By the summer of that year, the band had settled on the half step down tuning (E♭). Cobain said, \"We play so hard we can't tune our guitars fast enough\". The band made a habit of destroying its equipment after shows. Novoselic said he and Cobain created the \"shtick\" in order to get off the stage sooner. Cobain stated it began as an expression of his frustration with previous drummer Channing making mistakes and dropping out entirely during performances.\n",
      "\n",
      "Songwriting and lyrics\n",
      "Everett True said in 1989, \"Nirvana songs treat the banal and pedestrian with a unique slant\". Cobain came up with the basic components of each song, usually writing them on an acoustic guitar, as well as the singing style and the lyrics. He emphasized that Novoselic and Grohl had a large part in deciding the lengths and parts of songs, and that he did not like to be considered the sole songwriter.\n",
      "Cobain usually wrote lyrics for songs minutes before recording them. Cobain said, \"When I write a song the lyrics are the least important subject. I can go through two or three different subjects in a song and the title can mean absolutely nothing at all\". Cobain told Spin in 1993 that he \"didn't give a flying f–k [sic]\" what the lyrics on Bleach were about, figuring \"Let's just scream negative lyrics, and as long as they're not sexist and don't get too embarrassing it'll be okay\", while the lyrics to Nevermind were taken from two years of poetry he had accumulated, which he cut up and chose lines he preferred from. In comparison, Cobain stated that the lyrics to In Utero were \"more focused, they're almost built on themes\". Cobain did not write in a linear fashion, instead relying on juxtapositions of contradictory images to convey emotions and ideas. Often in his lyrics, Cobain would present an idea then reject it; he said, \"I'm such a nihilistic jerk half the time and other times I'm so vulnerable and sincere [.. The songs are] like a mixture of both of them. That's how most people my age are.\"\n",
      "\n",
      "Legacy\n",
      "Combined with their themes of abjection and alienation, Nirvana became hugely popular during their short tenure and are credited with bringing alternative rock to the mainstream. Stephen Thomas Erlewine of AllMusic wrote that prior to Nirvana, \"alternative music was consigned to specialty sections of record stores, and major labels considered it to be, at the very most, a tax write-off\". Following the release of Nevermind, \"nothing was ever quite the same, for better and for worse\". While other alternative bands had achieved hits, Nirvana \"broke down the doors forever\", according to Erlewine; the breakthrough \"didn't eliminate the underground\", but rather \"just gave it more exposure\". Erlewine also wrote that Nirvana \"popularized so-called 'Generation X' and 'slacker' culture\". Following Cobain's death, numerous headlines referred to Nirvana's frontman as \"the voice of a generation\", although he had rejected such labeling during his lifetime.\n",
      "In 1992, Jon Pareles of The New York Times reported that Nirvana had made other alternative acts impatient for similar success: \"Suddenly, all bets are off. No one has the inside track on which of dozens, perhaps hundreds, of ornery, obstreperous, unkempt bands might next appeal to the mall-walking millions.\" Record company executives offered large advances and record deals to bands, and previous strategies of building audiences for alternative rock groups were replaced by the opportunity to achieve mainstream popularity quickly.\n",
      "Michael Azerrad argued in his Nirvana biography Come as You Are: The Story of Nirvana (1993) that Nevermind marked an epochal generational shift in music similar to the rock-and-roll explosion in the 1950s and the end of the baby boomer generation's dominance of the musical landscape. Azerrad wrote, \"Nevermind came along at exactly the right time. This was music by, for, and about a whole new group of young people who had been overlooked, ignored, or condescended to.\" Fugazi frontman Guy Picciotto said \"It was like our record could have been a hobo pissing in the forest for the amount of impact it had ... It felt like we were playing ukuleles all of a sudden because of the disparity of the impact of what they did.\"\n",
      "Nirvana are one of the best-selling bands of all time, having sold more than 75 million records. With 32 million RIAA-certified units, they are also one of the bestselling music artists in the United States. They have achieved 10 Top 40 hits on the Billboard Alternative Songs chart, including five number-ones. Two of their studio albums and two of their live albums have reached the top spot on the Billboard 200. Nirvana have been awarded one diamond, three multiplatinum, seven platinum and two gold-certified albums in the United States by the RIAA, and four multiplatinum, four platinum, two gold and one silver-certified albums in the UK by the BPI. Nevermind, their most successful album, has sold more than 30 million copies worldwide, making it one of the best-selling albums ever. Their most successful song, \"Smells Like Teen Spirit\", is among the bestselling singles of all time, having been certified diamond with sales of 10 million copies.\n",
      "\n",
      "Awards and accolades\n",
      "Since their breakup, Nirvana have continued to receive acclaim. In 2003, they were selected as one of the inductees of the Mojo Hall of Fame 100. The band also received a nomination in 2004 from the UK Music Hall of Fame for the title of \"Greatest Artist of the 1990s\". Rolling Stone placed Nirvana at number 27 on their list of the \"100 Greatest Artists of All Time\" in 2004, and at number 30 on their updated list in 2011. In 2003, the magazine's senior editor David Fricke picked Kurt Cobain as the 12th best guitarist of all time. Rolling Stone later ranked Cobain as the 45th greatest singer in 2008 and 73rd greatest guitarist of all time in 2011. VH1 ranked Nirvana as the 42nd greatest artists of rock and roll in 1998, the 7th greatest hard rock artists in 2000, and the 14th greatest artists of all time in 2010.\n",
      "Nirvana's contributions to music have also received recognition. The Rock and Roll Hall of Fame has inducted two of Nirvana's recordings, \"Smells Like Teen Spirit\" and \"All Apologies\", into its list of \"The Songs That Shaped Rock and Roll\". The museum also ranked Nevermind number 10 on its \"The Definitive 200 Albums of All Time\" list in 2007. In 2005, the Library of Congress added Nevermind to the National Recording Registry, which collects \"culturally, historically or aesthetically important\" sound recordings from the 20th century. In 2011, four of Nirvana's songs appeared on Rolling Stone's updated list of \"The 500 Greatest Songs of All Time\", with \"Smells Like Teen Spirit\" ranking the highest at number 9. Three of the band's albums were ranked on the magazine's 2012 list of \"The 500 Greatest Albums of All Time\", with Nevermind placing the highest at number 17. The same three Nirvana albums were also placed on Rolling Stone's 2011 list of \"The 100 Best Albums of the Nineties\", with Nevermind ranking the highest at number 1, making it the greatest album of the decade. Time included Nevermind on its list of \"The All-TIME 100 Albums\" in 2006, labeling it \"the finest album of the 1990s\". In 2011, the magazine also added \"Smells Like Teen Spirit\" on its list of \"The All-TIME 100 Songs\", and \"Heart-Shaped Box\" on its list of \"The 30 All-TIME Best Music Videos\". Pitchfork ranked Nevermind and In Utero as the sixth and thirteenth greatest albums of the 1990s, describing the band as \"the greatest and most legendary band of the 1990s.\"\n",
      "Nirvana was announced in their first year of eligibility as being part of the 2014 class of inductees into the Rock and Roll Hall of Fame on December 17, 2013. The induction ceremony was held April 10, 2014, in Brooklyn, New York, at the Barclays Center. As the accolade was only applied to Cobain, Novoselic and Grohl, former drummer Chad Channing was not included in the induction and was informed of his omission by text message. Channing attended the ceremony, where Grohl publicly thanked him for his contributions and noted that he had written some of Nirvana's most recognized drum parts. In 2023, Nirvana (represented by Novoselic, Grohl, and Smear) were awarded a Lifetime Achievement Award at the 2023 Grammy Awards.\n",
      "\n",
      "Band members\n",
      "Timeline\n",
      "Discography\n",
      "Bleach (1989)\n",
      "Nevermind (1991)\n",
      "In Utero (1993)\n",
      "\n",
      "See also\n",
      "List of alternative rock artists\n",
      "List of musicians from Seattle\n",
      "List of Nirvana concerts\n",
      "\n",
      "References\n",
      "Bibliography\n",
      "Azerrad, Michael. Come as You Are: The Story of Nirvana. Doubleday, 1994. ISBN 0-385-47199-8\n",
      "Cross, Charles R. Heavier Than Heaven: A Biography of Kurt Cobain. Hyperion, 2001. ISBN 0-7868-8402-9\n",
      "DeRogatis, Jim. Milk It!: Collected Musings on the Alternative Music Explosion of the 90's. Da Capo, 2003. ISBN 0-306-81271-1\n",
      "Gaar, Gillian G. In Utero. Continuum, 2006. ISBN 0-8264-1776-0\n",
      "Rocco, John (editor). The Nirvana Companion: Two Decades of Commentary. Schirmer, 1998. ISBN 0-02-864930-3\n",
      "True, Everett. Nirvana: The Biography. Da Capo, 2007. ISBN 0-306-81554-0\n",
      "\n",
      "External links\n",
      "\n",
      "Official website \n",
      "Nirvana discography at Discogs \n",
      "Live Nirvana – Guides to Nirvana studio sessions output and Nirvana live concerts\n",
      "Nirvana Live Guide – Guide to Nirvana's live performances and recordings\n",
      "Nirvana at IMDb /n\n"
     ]
    }
   ],
   "source": [
    "wiki_api = wikipediaapi.Wikipedia('test_pr/1.0', 'en')\n",
    "page = wiki_api.page('Nirvana (band)')\n",
    "\n",
    "if page.exists():\n",
    "    print(\"Title:\", page.title, \"/n\")\n",
    "    print(\"Text:\", page.text, \"/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf95743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Document List: ['A galaxy is a system of stars, stellar remnants, interstellar gas, dust, and dark matter bound together by gravity. The word is derived from the Greek galaxias (γαλαξίας), literally \\'milky\\', a reference to the Milky Way galaxy that contains the Solar System. Galaxies, averaging an estimated 100 million stars, range in size from dwarfs with less than a thousand stars, to the largest galaxies known – supergiants with one hundred trillion stars, each orbiting its galaxy\\'s centre of mass. Most of the mass in a typical galaxy is in the form of dark matter, with only a few per cent of that mass visible in the form of stars and nebulae. Supermassive black holes are a common feature at the centres of galaxies.\\nGalaxies are categorised according to their visual morphology as elliptical, spiral, or irregular. The Milky Way is an example of a spiral galaxy. It is estimated that there are between 200 billion (2×1011) to 2 trillion galaxies in the observable universe. Most galaxies are 1,000 to 100,000 parsecs in diameter (approximately 3,000 to 300,000 light years) and are separated by distances in the order of millions of parsecs (or megaparsecs). For comparison, the Milky Way has a diameter of at least 26,800 parsecs (87,400 ly) and is separated from the Andromeda Galaxy, its nearest large neighbour, by just over 750,000 parsecs (2.5 million ly).\\nThe space between galaxies is filled with a tenuous gas (the intergalactic medium) with an average density of less than one atom per cubic metre. Most galaxies are gravitationally organised into groups, clusters and superclusters. The Milky Way is part of the Local Group, which it dominates along with the Andromeda Galaxy. The group is part of the Virgo Supercluster. At the largest scale, these associations are generally arranged into sheets and filaments surrounded by immense voids. Both the Local Group and the Virgo Supercluster are contained in a much larger cosmic structure named Laniakea.\\n\\nEtymology\\nThe word galaxy was borrowed via French and Medieval Latin from the Greek term for the Milky Way, galaxías (kúklos) γαλαξίας (κύκλος) \\'milky (circle)\\', named after its appearance as a milky band of light in the sky.\\nIn the astronomical literature, the capitalised word \"Galaxy\" is often used to refer to the Milky Way galaxy, to distinguish it from the other galaxies in the universe. \\nGalaxies were initially discovered telescopically and were known as spiral nebulae. Most 18th- to 19th-century astronomers considered them as either unresolved star clusters or extragalactic nebulae, but their true composition and natures remained a mystery. Observations using larger telescopes of a few nearby bright galaxies, like the Andromeda Galaxy, began resolving them into huge conglomerations of stars, but based simply on the apparent faintness and sheer population of stars, the true distances of these objects placed them well beyond the Milky Way. For this reason they were popularly called island universes. Harlow Shapley began to advocate for the term \"galaxy\" and against using \"universes\" and \"nebula\" for the objects but the very influential Edwin Hubble stuck to nebulae. The nomenclature did not fully change in until Hubble\\'s death in 1953.\\n\\nNomenclature\\nMillions of galaxies have been catalogued, but only a few have well-established names, such as the Andromeda Galaxy, the Magellanic Clouds, the Whirlpool Galaxy, and the Sombrero Galaxy. Astronomers work with numbers from certain catalogues, such as the Messier catalogue, the NGC (New General Catalogue), the IC (Index Catalogue), the CGCG (Catalogue of Galaxies and of Clusters of Galaxies), the MCG (Morphological Catalogue of Galaxies), the UGC (Uppsala General Catalogue of Galaxies), and the PGC (Catalogue of Principal Galaxies, also known as LEDA). All the well-known galaxies appear in one or more of these catalogues but each time under a different number. For example, Messier 109 (or \"M109\") is a spiral galaxy having the number 109 in the catalogue of Messier.  It also has the designations NGC 3992, UGC 6937, CGCG 269–023, MCG +09-20-044, and PGC 37617 (or LEDA 37617), among others.  Millions of fainter galaxies are known by their identifiers in sky surveys such as the Sloan Digital Sky Survey.\\n\\nObservation history\\nMilky Way\\nGreek philosopher Democritus (450–370 BCE) proposed that the bright band on the night sky known as the Milky Way might consist of distant stars.\\nAristotle (384–322 BCE), however, believed the Milky Way was caused by \"the ignition of the fiery exhalation of some stars that were large, numerous and close together\" and that the \"ignition takes place in the upper part of the atmosphere, in the region of the World that is continuous with the heavenly motions.\" Neoplatonist philosopher Olympiodorus the Younger (c.\\u2009495–570 CE) was critical of this view, arguing that if the Milky Way was sublunary (situated between Earth and the Moon) it should appear different at different times and places on Earth, and that it should have parallax, which it did not. In his view, the Milky Way was celestial.\\nAccording to Mohani Mohamed, Arabian astronomer Ibn al-Haytham (965–1037) made the first attempt at observing and measuring the Milky Way\\'s parallax, and he thus \"determined that because the Milky Way had no parallax, it must be remote from the Earth, not belonging to the atmosphere.\" Persian astronomer al-Biruni (973–1048) proposed the Milky Way galaxy was \"a collection of countless fragments of the nature of nebulous stars.\" Andalusian astronomer Avempace (d. 1138) proposed that it was composed of many stars that almost touched one another, and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars as evidence of this occurring when two objects were near. In the 14th century, Syrian-born Ibn Qayyim al-Jawziyya proposed the Milky Way galaxy was \"a myriad of tiny stars packed together in the sphere of the fixed stars.\"\\n\\nActual proof of the Milky Way consisting of many stars came in 1610 when the Italian astronomer Galileo Galilei used a telescope to study it and discovered it was composed of a huge number of faint stars. In 1750, English astronomer Thomas Wright, in his An Original Theory or New Hypothesis of the Universe, correctly speculated that it might be a rotating body of a huge number of stars held together by gravitational forces, akin to the Solar System but on a much larger scale, and that the resulting disk of stars could be seen as a band on the sky from a perspective inside it. In his 1755 treatise, Immanuel Kant elaborated on Wright\\'s idea about the Milky Way\\'s structure. \\nThe first project to describe the shape of the Milky Way and the position of the Sun was undertaken by William Herschel in 1785 by counting the number of stars in different regions of the sky. He produced a diagram of the shape of the galaxy with the Solar System close to the center. Using a refined approach, Kapteyn in 1920 arrived at the picture of a small (diameter about 15 kiloparsecs) ellipsoid galaxy with the Sun close to the center. A different method by Harlow Shapley based on the cataloguing of globular clusters led to a radically different picture: a flat disk with diameter approximately 70 kiloparsecs and the Sun far from the centre. Both analyses failed to take into account the absorption of light by interstellar dust present in the galactic plane; but after Robert Julius Trumpler quantified this effect in 1930 by studying open clusters, the present picture of the Milky Way galaxy emerged.\\n\\nDistinction from other nebulae\\nA few galaxies outside the Milky Way are visible on a dark night to the unaided eye, including the Andromeda Galaxy, Large Magellanic Cloud, Small Magellanic Cloud, and the Triangulum Galaxy. In the 10th century, Persian astronomer Abd al-Rahman al-Sufi made the earliest recorded identification of the Andromeda Galaxy, describing it as a \"small cloud\". In 964, he probably mentioned the Large Magellanic Cloud in his Book of Fixed Stars, referring to \"Al Bakr of the southern Arabs\", since at a declination of about 70° south it was not visible where he lived. It was not well known to Europeans until Magellan\\'s voyage in the 16th century. The Andromeda Galaxy was later independently noted by Simon Marius in 1612.\\nIn 1734, philosopher Emanuel Swedenborg in his Principia speculated that there might be other galaxies outside that were formed into galactic clusters that were minuscule parts of the universe that extended far beyond what could be seen. Swedenborg\\'s views \"are remarkably close to the present-day views of the cosmos.\"\\nIn 1745, Pierre Louis Maupertuis conjectured that some nebula-like objects were collections of stars with unique properties, including a glow exceeding the light its stars produced on their own, and repeated Johannes Hevelius\\'s view that the bright spots were massive and flattened due to their rotation.\\nIn 1750, Thomas Wright correctly speculated that the Milky Way was a flattened disk of stars, and that some of the nebulae visible in the night sky might be separate Milky Ways.\\n\\nToward the end of the 18th century, Charles Messier compiled a catalog containing the 109 brightest celestial objects having nebulous appearance. Subsequently, William Herschel assembled a catalog of 5,000 nebulae. In 1845, Lord Rosse examined the nebulae catalogued by Herschel and observed the spiral structure of Messier object M51, now known as the Whirlpool Galaxy.\\nIn 1912, Vesto M. Slipher made spectrographic studies of the brightest spiral nebulae to determine their composition. Slipher discovered that the spiral nebulae have high Doppler shifts, indicating that they are moving at a rate exceeding the velocity of the stars he had measured. He found that the majority of these nebulae are moving away from us.\\nIn 1917, Heber Doust Curtis observed nova S Andromedae within the \"Great Andromeda Nebula\", as the Andromeda Galaxy, Messier object M31, was then known. Searching the photographic record, he found 11 more novae. Curtis noticed that these novae were, on average, 10 magnitudes fainter than those that occurred within this galaxy. As a result, he was able to come up with a distance estimate of 150,000 parsecs. He became a proponent of the so-called \"island universes\" hypothesis, which holds that spiral nebulae are actually independent galaxies.\\nIn 1920 a debate took place between Harlow Shapley and Heber Curtis, the Great Debate, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. To support his claim that the Great Andromeda Nebula is an external galaxy, Curtis noted the appearance of dark lanes resembling the dust clouds in the Milky Way, as well as the significant Doppler shift.\\nIn 1922, the Estonian astronomer Ernst Öpik gave a distance determination that supported the theory that the Andromeda Nebula is indeed a distant extra-galactic object. Using the new 100-inch Mount Wilson telescope, Edwin Hubble was able to resolve the outer parts of some spiral nebulae as collections of individual stars and identified some Cepheid variables, thus allowing him to estimate the distance to the nebulae: they were far too distant to be part of the Milky Way. In 1926 Hubble produced a classification of galactic morphology that is used to this day.\\n\\nMulti-wavelength observation\\nAdvances in astronomy have always been driven by technology. After centuries of success in optical astronomy, recent decades have seen major progress in other regions of the electromagnetic spectrum.\\nThe dust present in the interstellar medium is opaque to visual light. It is more transparent to far-infrared, which can be used to observe the interior regions of giant molecular clouds and galactic cores in great detail. Infrared is also used to observe distant, red-shifted galaxies that were formed much earlier. Water vapor and carbon dioxide absorb a number of useful portions of the infrared spectrum, so high-altitude or space-based telescopes are used for infrared astronomy.\\nThe first non-visual study of galaxies, particularly active galaxies, was made using radio frequencies. The Earth\\'s atmosphere is nearly transparent to radio between 5 MHz and 30 GHz. The ionosphere blocks signals below this range. Large radio interferometers have been used to map the active jets emitted from active nuclei.\\nUltraviolet and X-ray telescopes can observe highly energetic galactic phenomena. Ultraviolet flares are sometimes observed when a star in a distant galaxy is torn apart from the tidal forces of a nearby black hole. The distribution of hot gas in galactic clusters can be mapped by X-rays. The existence of supermassive black holes at the cores of galaxies was confirmed through X-ray astronomy.\\n\\nModern research\\nIn 1944, Hendrik van de Hulst predicted that microwave radiation with wavelength of 21 cm would be detectable from interstellar atomic hydrogen gas; and in 1951 it was observed. This radiation is not affected by dust absorption, and so its Doppler shift can be used to map the motion of the gas in this galaxy. These observations led to the hypothesis of a rotating bar structure in the center of this galaxy. With improved radio telescopes, hydrogen gas could also be traced in other galaxies.\\nIn the 1970s, Vera Rubin uncovered a discrepancy between observed galactic rotation speed and that predicted by the visible mass of stars and gas. Today, the galaxy rotation problem is thought to be explained by the presence of large quantities of unseen dark matter.\\nBeginning in the 1990s, the Hubble Space Telescope yielded improved observations. Among other things, its data helped establish that the missing dark matter in this galaxy could not consist solely of inherently faint and small stars. The Hubble Deep Field, an extremely long exposure of a relatively empty part of the sky, provided evidence that there are about 125 billion (1.25×1011) galaxies in the observable universe. Improved technology in detecting the spectra invisible to humans (radio telescopes, infrared cameras, and x-ray telescopes) allows detection of other galaxies that are not detected by Hubble. Particularly, surveys in the Zone of Avoidance (the region of sky blocked at visible-light wavelengths by the Milky Way) have revealed a number of new galaxies.\\nA 2016 study published in The Astrophysical Journal, led by Christopher Conselice of the University of Nottingham, analyzed many sources of data to estimate that the observable universe (up to z=8) contained at least two trillion (2×1012) galaxies, a factor of 10 more than are directly observed in Hubble images. However, later observations with the New Horizons space probe from outside the zodiacal light observed less cosmic optical light than Conselice while still suggesting that direct observations are missing galaxies.\\n\\nTypes and morphology\\nGalaxies come in three main types: ellipticals, spirals, and irregulars. A slightly more extensive description of galaxy types based on their appearance is given by the Hubble sequence. Since the Hubble sequence is entirely based upon visual morphological type (shape), it may miss certain important characteristics of galaxies such as star formation rate in starburst galaxies and activity in the cores of active galaxies.\\nMany galaxies are thought to contain a supermassive black hole at their center. This includes the Milky Way, whose core region is called the Galactic Center.\\n\\nEllipticals\\nThe Hubble classification system rates elliptical galaxies on the basis of their ellipticity, ranging from E0, being nearly spherical, up to E7, which is highly elongated. These galaxies have an ellipsoidal profile, giving them an elliptical appearance regardless of the viewing angle. Their appearance shows little structure and they typically have relatively little interstellar matter. Consequently, these galaxies also have a low portion of open clusters and a reduced rate of new star formation. Instead, they are dominated by generally older, more evolved stars that are orbiting the common center of gravity in random directions. The stars contain low abundances of heavy elements because star formation ceases after the initial burst. In this sense they have some similarity to the much smaller globular clusters.\\n\\nType-cD galaxies\\nThe largest galaxies are the type-cD galaxies.\\nFirst described in 1964 by a paper by Thomas A. Matthews and others, they are a subtype of the more general class of D galaxies, which are giant elliptical galaxies, except that they are much larger. They are popularly known as the supergiant elliptical galaxies and constitute the largest and most luminous galaxies known. These galaxies feature a central elliptical nucleus with an extensive, faint halo of stars extending to megaparsec scales. The profile of their surface brightnesses as a function of their radius (or distance from their cores) falls off more slowly than their smaller counterparts.\\nThe formation of these cD galaxies remains an active area of research, but the leading model is that they are the result of the mergers of smaller galaxies in the environments of dense clusters, or even those outside of clusters with random overdensities. These processes are the mechanisms that drive the formation of fossil groups or fossil clusters, where a large, relatively isolated, supergiant elliptical resides in the middle of the cluster and are surrounded by an extensive cloud of X-rays as the residue of these galactic collisions. Another older model posits the phenomenon of cooling flow, where the heated gases in clusters collapses towards their centers as they cool, forming stars in the process, a phenomenon observed in clusters such as Perseus, and more recently in the Phoenix Cluster.\\n\\nShell galaxy\\nA shell galaxy is a type of elliptical galaxy where the stars in its halo are arranged in concentric shells. About one-tenth of elliptical galaxies have a shell-like structure, which has never been observed in spiral galaxies. These structures are thought to develop when a larger galaxy absorbs a smaller companion galaxy—that as the two galaxy centers approach, they start to oscillate around a center point, and the oscillation creates gravitational ripples forming the shells of stars, similar to ripples spreading on water. For example, galaxy NGC 3923 has over 20 shells.\\n\\nSpirals\\nSpiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept.\\nSpiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense.\\n\\nIn spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or \"density waves\". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a \"wave\" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars.\\n\\nBarred spiral galaxy\\nA majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along the arms.\\nOur own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×1011) stars and has a total mass of about six hundred billion (6×1011) times the mass of the Sun.\\n\\nSuper-luminous spiral\\nRecently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way\\'s 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way.\\n\\nOther morphologies\\nPeculiar galaxies are galactic formations that develop unusual properties due to tidal interactions with other galaxies.\\nA ring galaxy has a ring-like structure of stars and interstellar medium surrounding a bare core. A ring galaxy is thought to occur when a smaller galaxy passes through the core of a spiral galaxy. Such an event may have affected the Andromeda Galaxy, as it displays a multi-ring-like structure when viewed in infrared radiation.\\nA lenticular galaxy is an intermediate form that has properties of both elliptical and spiral galaxies. These are categorized as Hubble type S0, and they possess ill-defined spiral arms with an elliptical halo of stars (barred lenticular galaxies receive Hubble classification SB0).\\nIrregular galaxies are galaxies that can not be readily classified into an elliptical or spiral morphology.\\nAn Irr-I galaxy has some structure but does not align cleanly with the Hubble classification scheme.\\nIrr-II galaxies do not possess any structure that resembles a Hubble classification, and may have been disrupted. Nearby examples of (dwarf) irregular galaxies include the Magellanic Clouds.\\nA dark or \"ultra diffuse\" galaxy is an extremely-low-luminosity galaxy. It may be the same size as the Milky Way, but have a visible star count only one percent of the Milky Way\\'s. Multiple mechanisms for producing this type of galaxy have been proposed, and it is possible that different dark galaxies formed by different means. One candidate explanation for the low luminosity is that the galaxy lost its star-forming gas at an early stage, resulting in old stellar populations.\\n\\nDwarfs\\nDespite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across.\\nMany dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be discovered.\\nMost of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies.\\nA study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale.\\n\\nVariants\\nInteracting\\nInteractions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust.\\nCollisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies\\' shapes, forming bars, rings or tail-like structures.\\nAt the extreme of interactions are galactic mergers, where the galaxies\\' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies\\' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy.\\n\\nStarburst\\nStars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy\\'s lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy\\'s history. Starburst galaxies were more common during the universe\\'s early history, but still contribute an estimated 15% to total star production.\\nStarburst galaxies are characterized by dusty concentrations of gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end.\\nStarbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity.\\n\\nRadio galaxy\\nA radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ.\\nRadio galaxies can also be classified as giant radio galaxies (GRGs), whose radio emissions can extend to scales of megaparsecs (3.26 million light-years). Alcyoneus is an FR II class low-excitation radio galaxy which has the largest observed radio emission, with lobed structures spanning 5 megaparsecs (16×106 ly). For comparison, another similarly sized giant radio galaxy is 3C 236, with lobes 15 million light-years across. It should however be noted that radio emissions are not always considered part of the main galaxy itself.\\nA giant radio galaxy is a special class of objects characterized by the presence of radio lobes generated by relativistic jets powered by the central galaxy\\'s supermassive black hole. Giant radio galaxies are different from ordinary radio galaxies in that they can extend to much larger scales, reaching upwards to several megaparsecs across, far larger than the diameters of their host galaxies.\\nA \"normal\" radio galaxy do not have a source that is a supermassive black hole or monster neutron star; instead the source is synchrotron radiation from relativistic electrons accelerated by supernova. These sources are comparatively short lived, making the radio spectrum from normal radio galaxies an especially good way to study star formation.\\n\\nActive galaxy\\nSome observable galaxies are classified as \"active\" if they contain an active galactic nucleus (AGN). A significant portion of the galaxy\\'s total energy output is emitted by the active nucleus instead of its stars, dust and interstellar medium. There are multiple classification and naming schemes for AGNs, but those in the lower ranges of luminosity are called Seyfert galaxies, while those with luminosities much greater than that of the host galaxy are known as quasi-stellar objects or quasars. Models of AGNs suggest that a significant fraction of their light is shifted to far-infrared frequencies because optical and UV emission in the nucleus is absorbed and remitted by dust and gas surrounding it.\\nThe standard model for an active galactic nucleus is based on an accretion disc that forms around a supermassive black hole (SMBH) at the galaxy\\'s core region. The radiation from an active galactic nucleus results from the gravitational energy of matter as it falls toward the black hole from the disc. The AGN\\'s luminosity depends on the SMBH\\'s mass and the rate at which matter falls onto it.\\nIn about 10% of these galaxies, a diametrically opposed pair of energetic jets ejects particles from the galaxy core at velocities close to the speed of light. The mechanism for producing these jets is not well understood.\\n\\nSeyfert galaxy\\nSeyfert galaxies are one of the two largest groups of active galaxies, along with quasars. They have quasar-like nuclei (very luminous, distant and bright sources of electromagnetic radiation) with very high surface brightnesses; but unlike quasars, their host galaxies are clearly detectable. Seen through a telescope, a Seyfert galaxy appears like an ordinary galaxy with a bright star superimposed atop the core. Seyfert galaxies are divided into two principal subtypes based on the frequencies observed in their spectra.\\n\\nQuasar\\nQuasars are the most energetic and distant members of active galactic nuclei. Extremely luminous, they were first identified as high redshift sources of electromagnetic energy, including radio waves and visible light, that appeared more similar to stars than to extended sources similar to galaxies. Their luminosity can be 100 times that of the Milky Way. The nearest known quasar, Markarian 231, is about 581 million light-years from Earth, while others have been discovered as far away as UHZ1, roughly 13.2 billion light-years distant. Quasars are noteworthy for providing the first demonstration of the phenomenon that gravity can act as a lens for light.\\n\\nOther AGNs\\nBlazars are believed to be active galaxies with a relativistic jet pointed in the direction of Earth. A radio galaxy emits radio frequencies from relativistic jets. A unified model of these types of active galaxies explains their differences based on the observer\\'s position.\\nPossibly related to active galactic nuclei (as well as starburst regions) are low-ionization nuclear emission-line regions (LINERs). The emission from LINER-type galaxies is dominated by weakly ionized elements. The excitation sources for the weakly ionized lines include post-AGB stars, AGN, and shocks. Approximately one-third of nearby galaxies are classified as containing LINER nuclei.\\n\\nLuminous infrared galaxy\\nLuminous infrared galaxies (LIRGs) are galaxies with luminosities—the measurement of electromagnetic power output—above 1011 L☉ (solar luminosities). In most cases, most of their energy comes from large numbers of young stars which heat surrounding dust, which reradiates the energy in the infrared.  Luminosity high enough to be a LIRG requires a star formation rate of at least 18 M☉ yr−1. Ultra-luminous infrared galaxies (ULIRGs) are at least ten times more luminous still and form stars at rates >180 M☉ yr−1.  Many LIRGs also emit radiation from an AGN. Infrared galaxies emit more energy in the infrared than all other wavelengths combined, with peak emission typically at wavelengths of 60 to 100 microns. LIRGs are believed to be created from the strong interaction and merger of   spiral galaxies. While uncommon in the local universe, LIRGs and ULIRGS were more prevalent when the universe was younger.\\n\\nPhysical diameters\\nGalaxies do not have a definite boundary by their nature, and are characterized by a gradually decreasing stellar density as a function of increasing distance from their center, making measurements of their true extents difficult. Nevertheless, astronomers over the past few decades have made several criteria  in defining the sizes of galaxies.\\n\\nAngular diameter\\nAs early as the time of Edwin Hubble in 1936, there have been attempts to characterize the diameters of galaxies. The earliest efforts were based on the observed angle subtended by the galaxy and its estimated distance, leading to an angular diameter (also called \"metric diameter\").\\n\\nIsophotal diameter\\nThe isophotal diameter is introduced as a conventional way of measuring a galaxy\\'s size based on its apparent surface brightness. Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy.  The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec2; sometimes expressed as mag arcsec−2), which defines the brightness depth of the isophote. To illustrate how this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec2 at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky. The isophotal diameter is typically defined as the region enclosing all the light down to 25 mag/arcsec2 in the blue B-band, which is then referred to as the D25 standard.\\n\\nEffective radius (half-light) and its variations\\nThe half-light radius (also known as effective radius; Re) is a measure that is based on the galaxy\\'s overall brightness flux. This is the radius upon which half, or 50%, of the total brightness flux of the galaxy was emitted. This was first proposed by Gérard de Vaucouleurs in 1948. The choice of using 50% was arbitrary, but proved to be useful in further works by R. A. Fish in 1963, where he established a luminosity concentration law that relates the brightnesses of elliptical galaxies and their respective Re, and by José Luis Sérsic in 1968 that defined a mass-radius relation in galaxies.\\nIn defining Re, it is necessary that the overall brightness flux galaxy should be captured, with a method employed by Bershady in 2000 suggesting to measure twice the size where the brightness flux of an arbitrarily chosen radius, defined as the local flux, divided by the overall average flux equals to 0.2. Using half-light radius allows a rough estimate of a galaxy\\'s size, but is not particularly helpful in determining its morphology.\\nVariations of this method exist. In particular, in the ESO-Uppsala Catalogue of Galaxies values of 50%, 70%, and 90% of the total blue light (the light detected through a B-band specific filter) had been used to calculate a galaxy\\'s diameter.\\n\\nPetrosian magnitude\\nFirst described by Vahe Petrosian in 1976, a modified version of this method has been used by the Sloan Digital Sky Survey (SDSS). This method employs a mathematical model on a galaxy whose radius is determined by the azimuthally (horizontal) averaged profile of its brightness flux. In particular, the SDSS employed the Petrosian magnitude in the R-band (658 nm, in the red part of the visible spectrum) to ensure that the brightness flux of a galaxy would be captured as much as possible while counteracting the effects of background noise. For a galaxy whose brightness profile is exponential, it is expected to capture all of its brightness flux, and 80% for galaxies that follow a profile that follows de Vaucouleurs\\'s law.\\nPetrosian magnitudes have the advantage of being redshift and distance independent, allowing the measurement of the galaxy\\'s apparent size since the Petrosian radius is defined in terms of the galaxy\\'s overall luminous flux.\\nA critique of an earlier version of this method has been issued by the Infrared Processing and Analysis Center, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy\\'s overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sérsic\\'s law.\\n\\nNear-infrared method\\nThis method has been used by 2MASS as an adaptation from the previously used methods of isophotal measurement. Since 2MASS operates in the near infrared, which has the advantage of being able to recognize dimmer, cooler, and older stars, it has a different form of approach compared to other methods that normally use B-filter. The detail of the method used by 2MASS has been described thoroughly in a document by Jarrett et al., with the survey measuring several parameters.\\nThe standard aperture ellipse (area of detection) is defined by the infrared isophote at the Ks band (roughly 2.2 μm wavelength) of 20 mag/arcsec2. Gathering the overall luminous flux of the galaxy has been employed by at least four methods: the first being a circular aperture extending 7 arcseconds from the center, an isophote at 20 mag/arcsec2, a \"total\" aperture defined by the radial light distribution that covers the supposed extent of the galaxy, and the Kron aperture (defined as 2.5 times the first-moment radius, an integration of the flux of the \"total\" aperture).\\n\\nLarger-scale structures\\nDeep-sky surveys show that galaxies are often found in groups and clusters. Solitary galaxies that have not significantly interacted with other galaxies of comparable mass in the past few billion years are relatively scarce. Only about 5% of the galaxies surveyed are isolated in this sense. However, they may have interacted and even merged with other galaxies in the past, and may still be orbited by smaller satellite galaxies.\\nOn the largest scale, the universe is continually expanding, resulting in an average increase in the separation between individual galaxies (see Hubble\\'s law). Associations of galaxies can overcome this expansion on a local scale through their mutual gravitational attraction. These associations formed early, as clumps of dark matter pulled their respective galaxies together. Nearby groups later merged to form larger-scale clusters. This ongoing merging process, as well as an influx of infalling gas, heats the intergalactic gas in a cluster to very high temperatures of 30–100 megakelvins. About 70–80% of a cluster\\'s mass is in the form of dark matter, with 10–30% consisting of this heated gas and the remaining few percent in the form of galaxies.\\nMost galaxies are gravitationally bound to a number of other galaxies. These form a fractal-like hierarchical distribution of clustered structures, with the smallest such associations being termed groups. A group of galaxies is the most common type of galactic cluster; these formations contain the majority of galaxies (as well as most of the baryonic mass) in the universe. To remain gravitationally bound to such a group, each member galaxy must have a sufficiently low velocity to prevent it from escaping (see Virial theorem). If there is insufficient kinetic energy, however, the group may evolve into a smaller number of galaxies through mergers.\\nClusters of galaxies consist of hundreds to thousands of galaxies bound together by gravity. Clusters of galaxies are often dominated by a single giant elliptical galaxy, known as the brightest cluster galaxy, which, over time, tidally destroys its satellite galaxies and adds their mass to its own.\\n\\nSuperclusters contain tens of thousands of galaxies, which are found in clusters, groups and sometimes individually. At the supercluster scale, galaxies are arranged into sheets and filaments surrounding vast empty voids. Above this scale, the universe appears to be the same in all directions (isotropic and homogeneous), though this notion has been challenged in recent years by numerous findings of large-scale structures that appear to be exceeding this scale. The Hercules–Corona Borealis Great Wall, currently the largest structure in the universe found so far, is 10 billion light-years (three gigaparsecs) in length.\\nThe Milky Way galaxy is a member of an association named the Local Group, a relatively small group of galaxies that has a diameter of approximately one megaparsec. The Milky Way and the Andromeda Galaxy are the two brightest galaxies within the group; many of the other member galaxies are dwarf companions of these two. The Local Group itself is a part of a cloud-like structure within the Virgo Supercluster, a large, extended structure of groups and clusters of galaxies centered on the Virgo Cluster. In turn, the Virgo Supercluster is a portion of the Laniakea Supercluster.\\n\\nMagnetic fields\\nGalaxies have magnetic fields of their own. A galaxy\\'s magnetic field influences its dynamics in multiple ways, including affecting the formation of spiral arms and transporting angular momentum in gas clouds. The latter effect is particularly important, as it is a necessary factor for the gravitational collapse of those clouds, and thus for star formation.\\nThe typical average equipartition strength for spiral galaxies is about 10 μG (microgauss) or 1 nT (nanotesla). By comparison, the Earth\\'s magnetic field has an average strength of about 0.3 G (Gauss) or 30 μT (microtesla). Radio-faint galaxies like M 31 and M33, the Milky Way\\'s neighbors, have weaker fields (about 5 μG), while gas-rich galaxies with high star-formation rates, like M 51, M 83 and NGC 6946, have 15 μG on average. In prominent spiral arms, the field strength can be up to 25 μG, in regions where cold gas and dust are also concentrated. The strongest total equipartition fields (50–100 μG) were found in starburst galaxies—for example, in M 82 and the Antennae; and in nuclear starburst regions, such as the centers of NGC 1097 and other barred galaxies.\\n\\nFormation and evolution\\nFormation\\nCurrent models of the formation of galaxies in the early universe are based on the ΛCDM model. About 300,000 years after the Big Bang, atoms of hydrogen and helium began to form, in an event called recombination. Nearly all the hydrogen was neutral (non-ionized) and readily absorbed light, and no stars had yet formed. As a result, this period has been called the \"dark ages\". It was from density fluctuations (or anisotropic irregularities) in this primordial matter that larger structures began to appear. As a result, masses of baryonic matter started to condense within cold dark matter halos. These primordial structures allowed gasses to condense in to protogalaxies, large scale gas clouds that were precursors to the first galaxies.\\nAs gas falls in to the gravity of the dark matter halos, its pressure and temperature rise. To condense further, the gas must radiate energy. This process was slow in the early universe dominated by hydrogen atoms and molecules which are inefficient radiators compared to heavier elements. As clumps of gas aggregate forming rotating disks, temperatures and pressures continue to increase. Some places within the disk reach high enough density to form stars.\\n\\nOnce protogalaxies began to form and contract, the first halo stars, called Population III stars, appeared within them. These were composed of primordial gas, almost entirely of hydrogen and helium.\\nEmission from the first stars heats the remaining gas helping to trigger additional star formation; the ultraviolet light emission from the first generation of stars re-ionized the surrounding neutral hydrogen in expanding spheres eventually reaching the entire universe, an event called reionization. The most massive stars collapse in violent supernova explosions releasing heavy elements (\"metals\") into the interstellar medium. This metal content is incorporated into population II stars.\\nTheoretical models for early galaxy formation have been verified and informed by a large number and variety of sophisticated astronomical observations. The photometric observations generally need spectroscopic confirmation due the large number mechanisms that can introduce systematic errors. For example, a high redshift (z ~ 16) photometric observation by James Webb Space Telescope (JWST) was later corrected to be closer to z ~ 5.\\nNevertheless, confirmed observations from the JWST and other observatories are accumulating, allowing systematic comparison of early galaxies to predictions of theory.\\nEvidence for individual Population III stars in early galaxies is even more challenging. Even seemingly confirmed spectroscopic evidence may turn out to have other origins. For example, astronomers reported HeII emission evidence for Population III stars in the Cosmos Redshift 7 galaxy, with a redshift value of 6.60. Subsequent observations found metallic emission lines, OIII, inconsistent with an early-galaxy star.\\n\\nEvolution\\nOnce stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood.\\nWithin a billion years of a galaxy\\'s formation, key structures begin to appear. Globular clusters, the central supermassive black hole, and a galactic bulge of metal-poor Population II stars form. The creation of a supermassive black hole appears to play a key role in actively regulating the growth of galaxies by limiting the total amount of additional matter added. During this early epoch, galaxies undergo a major burst of star formation.\\nDuring the following two billion years, the accumulated matter settles into a galactic disc. A galaxy will continue to absorb infalling material from high-velocity clouds and dwarf galaxies throughout its life. This matter is mostly hydrogen and helium. The cycle of stellar birth and death slowly increases the abundance of heavy elements, eventually allowing the formation of planets.\\n\\nStar formation rates in galaxies depend upon their local environment. Isolated \\'void\\' galaxies have highest rate per stellar mass, with \\'field\\' galaxies associated with spiral galaxies having lower rates and galaxies in dense cluster having the lowest rates.\\nThe evolution of galaxies can be significantly affected by interactions and collisions. Mergers of galaxies were common during the early epoch, and the majority of galaxies were peculiar in morphology. Given the distances between the stars, the great majority of stellar systems in colliding galaxies will be unaffected. However, gravitational stripping of the interstellar gas and dust that makes up the spiral arms produces a long train of stars known as tidal tails. Examples of these formations can be seen in NGC 4676 or the Antennae Galaxies.\\nThe Milky Way galaxy and the nearby Andromeda Galaxy are moving toward each other at about 130 km/s, and—depending upon the lateral movements—the two might collide in about five to six billion years. Although the Milky Way has never collided with a galaxy as large as Andromeda before, it has collided and merged with other galaxies in the past. Cosmological simulations indicate that, 11 billion years ago, it merged with a particularly large galaxy that has been labeled the Kraken.\\nSuch large-scale interactions are rare. As time passes, mergers of two systems of equal size become less common. Most bright galaxies have remained fundamentally unchanged for the last few billion years, and the net rate of star formation probably also peaked about ten billion years ago.\\n\\nFuture trends\\nSpiral galaxies, like the Milky Way, produce new generations of stars as long as they have dense molecular clouds of interstellar hydrogen in their spiral arms. Elliptical galaxies are largely devoid of this gas, and so form few new stars. The supply of star-forming material is finite; once stars have converted the available supply of hydrogen into heavier elements, new star formation will come to an end.\\nThe current era of star formation is expected to continue for up to one hundred billion years, and then the \"stellar age\" will wind down after about ten trillion to one hundred trillion years (1013–1014 years), as the smallest, longest-lived stars in the visible universe, tiny red dwarfs, begin to fade. At the end of the stellar age, galaxies will be composed of compact objects: brown dwarfs, white dwarfs that are cooling or cold (\"black dwarfs\"), neutron stars, and black holes. Eventually, as a result of gravitational relaxation, all stars will either fall into central supermassive black holes or be flung into intergalactic space as a result of collisions.\\n\\nGallery\\nSee also\\nNotes\\nReferences\\nBibliography\\nExternal links\\n\\nNASA/IPAC Extragalactic Database (NED)\\nNED Redshift-Independent Distances\\nGalaxies on In Our Time at the BBC\\nAn Atlas of The Universe\\nGalaxies – Information and amateur observations\\nGalaxy Zoo – citizen science galaxy classification project\\nA Flight Through the Universe, by the Sloan Digital Sky Survey – animated video from Berkeley Lab', 'A black hole is a massive, compact astronomical object so dense that its gravity prevents anything from escaping, even light. Albert Einstein\\'s theory of general relativity predicts that a sufficiently compact mass will form a black hole. The boundary of no escape is called the event horizon. A black hole has a great effect on the fate and circumstances of an object crossing it, but has no locally detectable features according to general relativity. In many ways, a black hole acts like an ideal black body, as it reflects no light.  Quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly.\\nObjects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. In 1916, Karl Schwarzschild found the first modern solution of general relativity that would characterise a black hole. Due to his influential research, the Schwarzschild metric is named after him. David Finkelstein, in 1958, first published the interpretation of \"black hole\" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The first black hole known was Cygnus X-1, identified by several researchers independently in 1971.\\nBlack holes typically form when massive stars collapse at the end of their life cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses may form by absorbing other stars and merging with other black holes, or via direct collapse of gas clouds. There is consensus that supermassive black holes exist in the centres of most galaxies.\\nThe presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. In extreme cases, this creates a quasar, some of the brightest objects in the universe. Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being \"swallowed.\" If other stars are orbiting a black hole, their orbits can be used to determine the black hole\\'s mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses.\\n\\nHistory\\nThe idea of a body so big that even light could not escape was briefly proposed by English astronomical pioneer and clergyman John Michell and independently by French scientist Pierre-Simon Laplace. Both scholars proposed very large stars rather than the modern model of stars with extraordinary density.\\nMichell\\'s idea, in a short part of a letter published in 1784, calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed the speed of light. Michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies.\\nIn 1796, Laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the Solar System in his book Exposition du Système du Monde. Franz Xaver von Zach asked Laplace for a mathematical analysis, which Laplace provided and published in journal edited by von Zach.\\nScholars of the time were initially excited by the proposal that giant but invisible \\'dark stars\\' might be hiding in plain view, but enthusiasm dampened when the wavelike nature of light became apparent in the early nineteenth century, since light was understood as a wave rather than a particle, it was unclear what, if any, influence gravity would have on escaping light waves.\\n\\nGeneral relativity\\nIn 1915, Albert Einstein developed his theory of general relativity, having earlier shown that gravity does influence light\\'s motion. Only a few months later, Karl Schwarzschild found a solution to the Einstein field equations that describes the gravitational field of a point mass and a spherical mass. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently gave the same solution for the point mass and wrote more extensively about its properties. This solution had a peculiar behaviour at what is now called the Schwarzschild radius, where it became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this surface was not quite understood at the time.\\nIn 1924, Arthur Eddington showed that the singularity disappeared after a change of coordinates. In 1933, Georges Lemaître realised that this meant the singularity at the Schwarzschild radius was a non-physical coordinate singularity. Arthur Eddington commented on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein\\'s theory allows us to rule out overly large densities for visible stars like Betelgeuse because \"a star of 250 million km radius could not possibly have so high a density as the Sun. Firstly, the force of gravitation would be so great that light would be unable to escape from it, the rays falling back to the star like a stone to the earth. Secondly, the red shift of the spectral lines would be so great that the spectrum would be shifted out of existence. Thirdly, the mass would produce so much curvature of the spacetime metric that space would close up around the star, leaving us outside (i.e., nowhere).\"\\nIn 1931, Subrahmanyan Chandrasekhar calculated, using special relativity, that a non-rotating body of electron-degenerate matter above a certain limiting mass (now called the Chandrasekhar limit at 1.4 M☉) has no stable solutions. His arguments were opposed by many of his contemporaries like Eddington and Lev Landau, who argued that some yet unknown mechanism would stop the collapse. They were partly correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable.\\nIn 1939, Robert Oppenheimer and George Volkoff predicted that neutron stars above another limit, the Tolman–Oppenheimer–Volkoff limit, would collapse further for the reasons presented by Chandrasekhar, and concluded that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes. Their original calculations, based on the Pauli exclusion principle, gave it as 0.7 M☉. Subsequent consideration of neutron-neutron repulsion mediated by the strong force raised the estimate to approximately 1.5 M☉ to 3.0 M☉. Observations of the neutron star merger GW170817, which is thought to have generated a black hole shortly afterward, have refined the TOV limit estimate to ~2.17 M☉.\\nOppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped.  This is a valid point of view for external observers, but not for infalling observers.  The hypothetical collapsed stars were called \"frozen stars\", because an outside observer would see the surface of the star frozen in time at the instant where its collapse takes it to the Schwarzschild radius.\\nAlso in 1939, Einstein attempted to prove that black holes were impossible in his publication \"On a Stationary System with Spherical Symmetry Consisting of Many Gravitating Masses\", using his theory of general relativity to defend his argument. Months later, Oppenheimer and his student Hartland Snyder provided the Oppenheimer–Snyder model in their paper \"On Continued Gravitational Contraction\", which predicted the existence of black holes. In the paper, which made no reference to Einstein\\'s recent publication, Oppenheimer and Snyder used Einstein\\'s own theory of general relativity to show the conditions on how a black hole could develop, for the first time in contemporary physics.\\nIn 1958, David Finkelstein identified the Schwarzschild surface as an event horizon, \"a perfect unidirectional membrane: causal influences can cross it in only one direction\". Finkelstein created a new reference frame to include the point of view of infalling observers. Finkelstein\\'s solution extended the Schwarzschild solution for the future of observers falling into a black hole.  A similar concept had already been found by Martin Kruskal but Wheeler had not understood its significance. Eventually the Kruskal–Szekeres coordinates helped physics understand black holes from multiple perspectives.\\n\\nGolden age\\nThe era from the mid-1960s to the mid-1970s was the \"golden age of black hole research\", when general relativity and black holes became mainstream subjects of research. \\nIn this period more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the axisymmetric solution for a black hole that is both rotating and electrically charged. Through the work of Werner Israel, Brandon Carter, and David Robinson the no-hair theorem emerged, stating that a stationary black hole solution is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge.\\nAt first, it was suspected that the strange features of the black hole solutions were pathological artefacts from the symmetry conditions imposed, and that the singularities would not appear in generic situations. This view was held in particular by Vladimir Belinsky, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions. However, in the late 1960s Roger Penrose and Stephen Hawking used global techniques to prove that singularities appear generically. For this work, Penrose received half of the 2020 Nobel Prize in Physics, Hawking having died in 2018.\\nAstronomical observations also made great strides during this era. In 1967 Antony Hewish and Jocelyn Bell Burnell discovered pulsars and by 1969, these were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities; but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse.\\nBased on observations in Greenwich and Toronto in the early 1970s, Cygnus X-1, a galactic X-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole.\\nWork by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as Hawking radiation.\\n\\nObservation\\nOn 11 February 2016, the LIGO Scientific Collaboration and the Virgo collaboration announced the first direct detection of gravitational waves, representing the first observation of a black hole merger. On 10 April 2019, the first direct image of a black hole and its vicinity was published, following observations made by the Event Horizon Telescope (EHT) in 2017 of the supermassive black hole in Messier 87\\'s galactic centre.  Gaia mission observations have found evidence of a Sun-like star orbiting a black hole named Gaia BH1 around 1,560 light-years (480 parsecs) away; evidence suggests a brown dwarf star orbits Gaia BH2. Though only a couple dozen black holes have been found so far in the Milky Way, there are thought to be hundreds of millions, most of which are solitary and do not cause emission of radiation. Therefore, they would only be detectable by gravitational lensing.\\n\\nEtymology\\nIn December 1967, a student reportedly suggested the phrase \"black hole\" at a lecture by John Wheeler; Wheeler adopted the term for its brevity and \"advertising value\", and Wheeler\\'s stature in the field ensured it quickly caught on. leading some to credit Wheeler with coining the phrase.\\nHowever the term was used by others around that time. Science writer Marcia Bartusiak traces the term \"black hole\" to physicist Robert H. Dicke, who in the early 1960s reportedly compared the phenomenon to the Black Hole of Calcutta, notorious as a prison where people entered but never left alive.\\nThe term \"black hole\" was used in print by Life and Science News magazines in 1963, and by science journalist Ann Ewing in her article \"\\'Black Holes\\' in Space\", dated 18 January 1964, which was a report on a meeting of the American Association for the Advancement of Science held in Cleveland, Ohio.\\n\\nProperties and structure\\nThe escape velocity from a black hole exceeds the speed of light. The formula for escape velocity is \\n\\n  \\n    \\n      \\n        V\\n        =\\n        \\n          \\n            2\\n            M\\n            G\\n            \\n              /\\n            \\n            R\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle V={\\\\sqrt {2MG/R}}}\\n  \\n \\nfor an object at radius R from a spherical mass M, with G being the gravitational constant. When the velocity is the speed of light, c, the radius, \\n  \\n    \\n      \\n        \\n          R\\n          \\n            s\\n          \\n        \\n        =\\n        2\\n        M\\n        G\\n        \\n          /\\n        \\n        \\n          c\\n          \\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle R_{s}=2MG/c^{2},}\\n  \\n is called the Schwarzschild radius.\\nA technical definition of a black hole is any object whose mass is contained in a radius smaller than its Schwarzschild radius, a limit derived from one solution to the equations of general relativity.\\nThe no-hair theorem postulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. If the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. The degree to which the conjecture is true for real black holes under the laws of modern physics is currently an unsolved problem.\\nThese properties are special because they are visible from outside a black hole. For example, a charged black hole repels other like charges just like any other charged object. Similarly, the total mass inside a sphere containing a black hole can be found by using the gravitational analogue of Gauss\\'s law (through the ADM mass), far away from the black hole. Likewise, the angular momentum (or spin) can be measured from far away using frame dragging by the gravitomagnetic field, through for example the Lense–Thirring effect.\\n\\nWhen an object falls into a black hole, any information about the shape of the object or distribution of charge on it is evenly distributed along the horizon of the black hole, and is lost to outside observers. The behaviour of the horizon in this situation is a dissipative system that is closely analogous to that of a conductive stretchy membrane with friction and electrical resistance—the membrane paradigm. This is different from other field theories such as electromagnetism, which do not have any friction or resistivity at the microscopic level, because they are time-reversible.\\nBecause a black hole eventually achieves a stable state with only three parameters, there is no way to avoid losing information about the initial conditions: the gravitational and electric fields of a black hole give very little information about what went in. The information that is lost includes every quantity that cannot be measured far away from the black hole horizon, including approximately conserved quantum numbers such as the total baryon number and lepton number. This behaviour is so puzzling that it has been called the black hole information loss paradox.\\n\\nPhysical properties\\nThe simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff\\'s theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole \"sucking in everything\" in its surroundings is therefore correct only near a black hole\\'s horizon; far away, the external gravitational field is identical to that of any other body of the same mass.\\nSolutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.\\nWhile the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              Q\\n              \\n                2\\n              \\n            \\n            \\n              4\\n              π\\n              \\n                ϵ\\n                \\n                  0\\n                \\n              \\n            \\n          \\n        \\n        +\\n        \\n          \\n            \\n              \\n                c\\n                \\n                  2\\n                \\n              \\n              \\n                J\\n                \\n                  2\\n                \\n              \\n            \\n            \\n              G\\n              \\n                M\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        ≤\\n        G\\n        \\n          M\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {Q^{2}}{4\\\\pi \\\\epsilon _{0}}}+{\\\\frac {c^{2}J^{2}}{GM^{2}}}\\\\leq GM^{2}}\\n  \\n\\nfor a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein\\'s equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.\\nDue to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is\\n\\n  \\n    \\n      \\n        J\\n        ≤\\n        \\n          \\n            \\n              G\\n              \\n                M\\n                \\n                  2\\n                \\n              \\n            \\n            c\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle J\\\\leq {\\\\frac {GM^{2}}{c}},}\\n  \\n\\nallowing definition of a dimensionless spin parameter such that\\n\\n  \\n    \\n      \\n        0\\n        ≤\\n        \\n          \\n            \\n              c\\n              J\\n            \\n            \\n              G\\n              \\n                M\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        ≤\\n        1.\\n      \\n    \\n    {\\\\displaystyle 0\\\\leq {\\\\frac {cJ}{GM^{2}}}\\\\leq 1.}\\n  \\n\\nBlack holes are commonly classified according to their mass, independent of angular momentum, J. The size of a black hole, as determined by the radius of the event horizon, or Schwarzschild radius, is proportional to the mass, M, through\\n\\n  \\n    \\n      \\n        \\n          r\\n          \\n            \\n              s\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              2\\n              G\\n              M\\n            \\n            \\n              c\\n              \\n                2\\n              \\n            \\n          \\n        \\n        ≈\\n        2.95\\n        \\n        \\n          \\n            M\\n            \\n              M\\n              \\n                ⊙\\n              \\n            \\n          \\n        \\n         \\n        \\n          k\\n          m\\n          ,\\n        \\n      \\n    \\n    {\\\\displaystyle r_{\\\\mathrm {s} }={\\\\frac {2GM}{c^{2}}}\\\\approx 2.95\\\\,{\\\\frac {M}{M_{\\\\odot }}}~\\\\mathrm {km,} }\\n  \\n\\nwhere rs is the Schwarzschild radius and M☉ is the mass of the Sun. For a black hole with nonzero spin or electric charge, the radius is smaller, until an extremal black hole could have an event horizon close to\\n\\n  \\n    \\n      \\n        \\n          r\\n          \\n            \\n              +\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              G\\n              M\\n            \\n            \\n              c\\n              \\n                2\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle r_{\\\\mathrm {+} }={\\\\frac {GM}{c^{2}}}.}\\n\\nEvent horizon\\nThe defining feature of a black hole is the appearance of an event horizon—a boundary in spacetime through which matter and light can pass only inward towards the mass of the black hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach an outside observer, making it impossible to determine whether such an event occurred.\\nAs predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass. At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.\\nIn a thought experiment, a distant observer can imagine clocks near a black hole which would appear to tick more slowly than those farther away from the black hole. This effect, known as gravitational time dilation, would also cause an object falling into a black hole to appear to slow as it approaches the event horizon, taking an infinite amount of time to reach it. All processes on this object would appear to slow down, from the viewpoint of a fixed outside observer, and any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift. Eventually, the falling object fades away until it can no longer be seen. Typically this process happens very rapidly with an object disappearing from view within less than a second.\\nOn the other hand, imaginary, indestructible observers falling into a black hole would not notice any of these effects as they cross the event horizon. Their own clocks appear to them to tick normally, they cross the event horizon after a finite time without noting any singular behaviour.\\nIn general relativity, it is impossible to determine the location of the event horizon from local observations, due to Einstein\\'s equivalence principle.\\nThe topology of the event horizon of a black hole at equilibrium is always spherical. For non-rotating (static) black holes the geometry of the event horizon is precisely spherical, while for rotating black holes the event horizon is oblate.\\n\\nSingularity\\nAt the centre of the unrealistically simple Schwarzschild model of a black hole is a gravitational singularity  a region where the spacetime curvature becomes infinite. For a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form a ring singularity that lies in the plane of rotation. In both cases, the singular region has zero volume. It can also be shown that the singular region contains all the mass of the black hole solution. The singular region can thus be thought of as having infinite density.\\nEven though the Schwarzschild model is not valid at the singularity, observers falling into a Schwarzschild black hole (i.e., non-rotating and not charged) cannot avoid being carried into the singularity once they cross the event horizon. They can prolong the experience by accelerating away to slow their descent, but only up to a limit. When they reach the singularity, they are crushed to infinite density and their mass is added to the total of the black hole. Before that happens, they will have been torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the \"noodle effect\". \\nIn the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole. The possibility of travelling to another universe is, however, only theoretical since any perturbation would destroy this possibility. It also appears to be possible to follow closed timelike curves (returning to one\\'s own past) around the Kerr singularity, which leads to problems with causality like the grandfather paradox. It is expected that none of these peculiar effects would survive in a proper quantum treatment of rotating and charged black holes.\\nThe appearance of singularities in general relativity signals the breakdown of the theory. This breakdown occurs where quantum effects should describe these actions, due to the extremely high density and therefore particle interactions. To date, it has not been possible to combine quantum and gravitational effects into a single theory, although there exist attempts to formulate such a theory of quantum gravity. It is generally expected that such a theory will not feature singularities.\\n\\nPhoton sphere\\nThe photon sphere is a spherical boundary where photons that move on tangents to that sphere would be trapped in a non-stable but circular orbit around the black hole.\\nFor non-rotating black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius. Their orbits would be dynamically unstable, hence any small perturbation, such as a particle of infalling matter, would cause an instability that would grow over time, either setting the photon on an outward trajectory causing it to escape the black hole, or on an inward spiral where it would eventually cross the event horizon.\\nWhile light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Hence any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon. For a Kerr black hole the radius of the photon sphere depends on the spin parameter and on the details of the photon orbit, which can be prograde (the photon rotates in the same sense of the black hole spin) or retrograde.\\n\\nErgosphere\\nRotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly \"drag\" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still.\\nThe ergosphere of a black hole is a volume bounded by the black hole\\'s event horizon and the ergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator.\\nObjects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.\\n\\nInnermost stable circular orbit (ISCO)\\nIn Newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. In general relativity, however, there exists an innermost stable circular orbit (often called the ISCO), for which any infinitesimal inward perturbations to a circular orbit will lead to spiraling into the black hole, and any outward perturbations will, depending on the energy, result in spiraling in, stably orbiting between apastron and periastron, or escaping to infinity. The location of the ISCO depends on the spin of the black hole, in the case of a Schwarzschild black hole (spin zero) is:\\n\\n  \\n    \\n      \\n        \\n          r\\n          \\n            \\n              I\\n              S\\n              C\\n              O\\n            \\n          \\n        \\n        =\\n        3\\n        \\n        \\n          r\\n          \\n            s\\n          \\n        \\n        =\\n        \\n          \\n            \\n              6\\n              \\n              G\\n              M\\n            \\n            \\n              c\\n              \\n                2\\n              \\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle r_{\\\\rm {ISCO}}=3\\\\,r_{s}={\\\\frac {6\\\\,GM}{c^{2}}},}\\n  \\n\\nand decreases with increasing black hole spin for particles orbiting in the same direction as the spin.\\n\\nPlunging region\\nThe final observable region of spacetime around a black hole is called the plunging region. In this area it is no longer possible for matter to follow circular orbits or to stop a final descent into the black hole. Instead it will rapidly plunge toward the black hole close to the speed of light.\\n\\nFormation and evolution\\nGiven the bizarre character of black holes, it was long questioned whether such objects could actually exist in nature or whether they were merely pathological solutions to Einstein\\'s equations. Einstein himself wrongly thought black holes would not form, because he held that the angular momentum of collapsing particles would stabilise their motion at some radius. This led the general relativity community to dismiss all results to the contrary for many years. However, a minority of relativists continued to contend that black holes were physical objects, and by the end of the 1960s, they had persuaded the majority of researchers in the field that there is no obstacle to the formation of an event horizon.\\nPenrose demonstrated that once an event horizon forms, general relativity without quantum mechanics requires that a singularity will form within. Shortly afterwards, Hawking showed that many cosmological solutions that describe the Big Bang have singularities without scalar fields or other exotic matter. The Kerr solution, the no-hair theorem, and the laws of black hole thermodynamics showed that the physical properties of black holes were simple and comprehensible, making them respectable subjects for research. Conventional black holes are formed by gravitational collapse of heavy objects such as stars, but they can also in theory be formed by other processes.\\n\\nGravitational collapse\\nGravitational collapse occurs when an object\\'s internal pressure is insufficient to resist the object\\'s own gravity. For stars this usually occurs either because a star has too little \"fuel\" left to maintain its temperature through stellar nucleosynthesis, or because a star that would have been stable receives extra matter in a way that does not raise its core temperature. In either case the star\\'s temperature is no longer high enough to prevent it from collapsing under its own weight.\\nThe collapse may be stopped by the degeneracy pressure of the star\\'s constituents, allowing the condensation of matter into an exotic denser state. The result is one of the various types of compact star. Which type forms depends on the mass of the remnant of the original star left if the outer layers have been blown away (for example, in a Type II supernova). The mass of the remnant, the collapsed object that survives the explosion, can be substantially less than that of the original star. Remnants exceeding 5 M☉ are produced by stars that were over 20 M☉ before the collapse.\\nIf the mass of the remnant exceeds about 3–4 M☉ (the Tolman–Oppenheimer–Volkoff limit), either because the original star was very heavy or because the remnant collected additional mass through accretion of matter, even the degeneracy pressure of neutrons is insufficient to stop the collapse. No known mechanism (except possibly quark degeneracy pressure) is powerful enough to stop the implosion and the object will inevitably collapse to form a black hole.\\nThe gravitational collapse of heavy stars is assumed to be responsible for the formation of stellar mass black holes. Star formation in the early universe may have resulted in very massive stars, which upon their collapse would have produced black holes of up to 103 M☉. These black holes could be the seeds of the supermassive black holes found in the centres of most galaxies. It has further been suggested that massive black holes with typical masses of ~105 M☉ could have formed from the direct collapse of gas clouds in the young universe. These massive objects have been proposed as the seeds that eventually formed the earliest quasars observed already at redshift \\n  \\n    \\n      \\n        z\\n        ∼\\n        7\\n      \\n    \\n    {\\\\displaystyle z\\\\sim 7}\\n  \\n. Some candidates for such objects have been found in observations of the young universe.\\nWhile most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the light emitted just before the event horizon forms delayed an infinite amount of time. Thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.\\n\\nPrimordial black holes and the Big Bang\\nGravitational collapse requires great density. In the current epoch of the universe these high densities are found only in stars, but in the early universe shortly after the Big Bang densities were much greater, possibly allowing for the creation of black holes. High density alone is not enough to allow black hole formation since a uniform mass distribution will not allow the mass to bunch up. In order for primordial black holes to have formed in such a dense medium, there must have been initial density perturbations that could then grow under their own gravity. Different models for the early universe vary widely in their predictions of the scale of these fluctuations. Various models predict the creation of primordial black holes ranging in size from a Planck mass (\\n  \\n    \\n      \\n        \\n          m\\n          \\n            P\\n          \\n        \\n        =\\n        \\n          \\n            ℏ\\n            c\\n            \\n              /\\n            \\n            G\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle m_{P}={\\\\sqrt {\\\\hbar c/G}}}\\n  \\n ≈ 1.2×1019 GeV/c2 ≈ 2.2×10−8 kg) to hundreds of thousands of solar masses.\\nDespite the early universe being extremely dense, it did not re-collapse into a black hole during the Big Bang, since the expansion rate was greater than the attraction. Following inflation theory, general relativity predicts that the gravitational field slows the expansion of the universe.The negative pressure overcomes the positive energy density to produce a net repulsive gravitational field.\\nModels for the gravitational collapse of objects of relatively constant size, such as stars, do not necessarily apply in the same way to rapidly expanding space such as the Big Bang.\\n\\nHigh-energy collisions\\nGravitational collapse is not the only process that could create black holes. In principle, black holes could be formed in high-energy collisions that achieve sufficient density. As of 2002, no such events have been detected, either directly or indirectly as a deficiency of the mass balance in particle accelerator experiments. This suggests that there must be a lower limit for the mass of black holes. Theoretically, this boundary is expected to lie around the Planck mass, where quantum effects are expected to invalidate the predictions of general relativity.\\nThis would put the creation of black holes firmly out of reach of any high-energy process occurring on or near the Earth. However, certain developments in quantum gravity suggest that the minimum black hole mass could be much lower: some braneworld scenarios for example put the boundary as low as 1 TeV/c2. This would make it conceivable for micro black holes to be created in the high-energy collisions that occur when cosmic rays hit the Earth\\'s atmosphere, or possibly in the Large Hadron Collider at CERN. These theories are very speculative, and the creation of black holes in these processes is deemed unlikely by many specialists. Even if micro black holes could be formed, it is expected that they would evaporate in about 10−25 seconds, posing no threat to the Earth.\\n\\nGrowth\\nOnce a black hole has formed, it can continue to grow by absorbing additional matter. Any black hole will continually absorb gas and interstellar dust from its surroundings. This growth process is one possible way through which some supermassive black holes may have been formed, although the formation of supermassive black holes is still an open field of research. A similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters. Black holes can also merge with other objects such as stars or even other black holes. This is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many smaller objects. The process has also been proposed as the origin of some intermediate-mass black holes.\\n\\nEvaporation\\nIn 1974, Hawking predicted that black holes are not entirely black but emit small amounts of thermal radiation at a temperature ħc3/(8πGMkB); this effect has become known as Hawking radiation. By applying quantum field theory to a static black hole background, he determined that a black hole should emit particles that display a perfect black body spectrum. Since Hawking\\'s publication, many others have verified the result through various approaches. If Hawking\\'s theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles. The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which, for a Schwarzschild black hole, is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes.\\nA stellar black hole of 1 M☉ has a Hawking temperature of 62 nanokelvins. This is far less than the 2.7 K temperature of the cosmic microwave background radiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through Hawking radiation and thus will grow instead of shrinking. To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than the Moon. Such a black hole would have a diameter of less than a tenth of a millimetre.\\nIf a black hole is very small, the radiation effects are expected to become very strong. A black hole with the mass of a car would have a diameter of about 10−24 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster; for example, a black hole of mass 1 TeV/c2 would take less than 10−88 seconds to evaporate completely. For such a small black hole, quantum gravity effects are expected to play an important role and could hypothetically make such a small black hole stable, although current developments in quantum gravity do not indicate this is the case.\\nThe Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from Earth. A possible exception, however, is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes. NASA\\'s Fermi Gamma-ray Space Telescope launched in 2008 will continue the search for these flashes.\\nIf black holes evaporate via Hawking radiation, a solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064 years. A supermassive black hole with a mass of 1011 M☉ will evaporate in around 2×10100 years. During the collapse of a supercluster of galaxies, supermassive black holes are predicted to grow to perhaps 1014 M☉. Even these would evaporate over a timescale of up to 10106 years.\\n\\nObservational evidence\\nBy nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. For example, a black hole\\'s existence can sometimes be inferred by observing its gravitational influence on its surroundings.\\n\\nDirect interferometry\\nThe Event Horizon Telescope (EHT) is an active program that directly observes the immediate environment of black holes\\' event horizons, such as the black hole at the centre of the Milky Way. In April 2017, EHT began observing the black hole at the centre of Messier 87. \"In all, eight radio observatories on six mountains and four continents observed the galaxy in Virgo on and off for 10 days in April 2017\" to provide the data yielding the image in April 2019.\\nAfter two years of data processing, EHT released its first  image of a black hole, at the center of the Messier 87 galaxy. What is visible is not the black hole—which shows as black because of the loss of all light within this dark region. Instead, it is the gases at the edge of the event horizon, displayed as orange or red, that define the black hole.\\nOn 12 May 2022, the EHT released the first image of Sagittarius A*, the supermassive black hole at the centre of the Milky Way galaxy. The published image displayed the same ring-like structure and \"shadow\" seen in the M87* black hole. The boundary of the shadow or area of less brightness matches the predicted gravitationally lensed photon orbits. The image was created using the same techniques as for the M87 black hole. The imaging process for Sagittarius A*, which is more than a thousand times smaller and less massive than M87*, was significantly more complex because of the instability of its surroundings. The image of Sagittarius A* was partially blurred by turbulent plasma on the way to the galactic centre, an effect which prevents resolution of the image at longer wavelengths.\\nThe brightening of this material in the \\'bottom\\' half of the processed EHT image is thought to be caused by Doppler beaming, whereby material approaching the viewer at relativistic speeds is perceived as brighter than material moving away. In the case of a black hole, this phenomenon implies that the visible material is rotating at relativistic speeds (>1,000 km/s [2,200,000 mph]), the only speeds at which it is possible to centrifugally balance the immense gravitational attraction of the singularity, and thereby remain in orbit above the event horizon. This configuration of bright material implies that the EHT observed M87* from a perspective catching the black hole\\'s accretion disc nearly edge-on, as the whole system rotated clockwise.\\nThe extreme gravitational lensing associated with black holes produces the illusion of a perspective that sees the accretion disc from above. In reality, most of the ring in the EHT image was created when the light emitted by the far side of the accretion disc bent around the black hole\\'s gravity well and escaped, meaning that most of the possible perspectives on M87* can see the entire disc, even that directly behind the \"shadow\".\\nIn 2015, the EHT detected magnetic fields just outside the event horizon of Sagittarius A* and even discerned some of their properties. The field lines that pass through the accretion disc were a complex mixture of ordered and tangled. Theoretical studies of black holes had predicted the existence of magnetic fields.\\nIn April 2023, an image of the shadow of the Messier 87 black hole and the related high-energy jet, viewed together for the first time, was presented.\\n\\nDetection of gravitational waves from merging black holes\\nOn 14 September 2015, the LIGO gravitational wave observatory made the first-ever successful direct observation of gravitational waves. The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses. This observation provides the most concrete evidence for the existence of black holes to date. For instance, the gravitational wave signal suggests that the separation of the two objects before the merger was just 350 km, or roughly four times the Schwarzschild radius corresponding to the inferred masses. The objects must therefore have been extremely compact, leaving black holes as the most plausible interpretation.\\nMore importantly, the signal observed by LIGO also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state. Arguably, the ringdown is the most direct way of observing a black hole. From the LIGO signal, it is possible to extract the frequency and damping time of the dominant mode of the ringdown. From these, it is possible to infer the mass and angular momentum of the final object, which match independent predictions from numerical simulations of the merger. The frequency and decay time of the dominant mode are determined by the geometry of the photon sphere. Hence, observation of this mode confirms the presence of a photon sphere; however, it cannot exclude possible exotic alternatives to black holes that are compact enough to have a photon sphere.\\nThe observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.\\nSince then, many more gravitational wave events have been observed.\\n\\nStars orbiting Sagittarius A*\\nThe proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×106 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars.\\nSince then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×106 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object\\'s size is still too large to test whether it is smaller than its Schwarzschild radius. Nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.\\n\\nAccretion of matter\\nDue to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disk-like structure around the object. Artists\\' impressions such as the accompanying representation of a black hole with corona commonly depict the black hole as if it were a flat-space body hiding the part of the disk just behind it, but in reality gravitational lensing would greatly distort the image of the accretion disk.\\nWithin such a disk, friction would cause angular momentum to be transported outward, allowing matter to fall farther inward, thus releasing potential energy and increasing the temperature of the gas.\\nWhen the accreting object is a neutron star or a black hole, the gas in the inner accretion disk orbits at very high speeds because of its proximity to the compact object. The resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainly X-rays). These bright X-ray sources may be detected by telescopes. This process of accretion is one of the most efficient energy-producing processes known. Up to 40% of the rest mass of the accreted material can be emitted as radiation. In nuclear fusion only about 0.7% of the rest mass will be emitted as energy. In many cases, accretion disks are accompanied by relativistic jets that are emitted along the poles, which carry away much of the energy. The mechanism for the creation of these jets is currently not well understood, in part due to insufficient data.\\nAs such, many of the universe\\'s more energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei and quasars are believed to be the accretion disks of supermassive black holes. Similarly, X-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion. It has also been suggested that some ultraluminous X-ray sources may be the accretion disks of intermediate-mass black holes.\\nStars have been observed to get torn apart by tidal forces in the immediate vicinity of supermassive black holes in galaxy nuclei, in what is known as a  tidal disruption event (TDE). Some of the material from the disrupted star forms an accretion disk around the black hole, which emits observable electromagnetic radiation.\\nIn November 2011 the first direct observation of a quasar accretion disk around a supermassive black hole was reported.\\n\\nX-ray binaries\\nX-ray binaries are binary star systems that emit a majority of their radiation in the X-ray part of the spectrum. These X-ray emissions are generally thought to result when one of the stars (compact object) accretes matter from another (regular) star. The presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole.\\nIf such a system emits signals that can be directly traced back to the compact object, it cannot be a black hole. The absence of such a signal does, however, not exclude the possibility that the compact object is a neutron star. By studying the companion star it is often possible to obtain the orbital parameters of the system and to obtain an estimate for the mass of the compact object. If this is much larger than the Tolman–Oppenheimer–Volkoff limit (the maximum mass a star can have without collapsing) then the object cannot be a neutron star and is generally expected to be a black hole.\\nThe first strong candidate for a black hole, Cygnus X-1, was discovered in this way by Charles Thomas Bolton, Louise Webster, and Paul Murdin in 1972. Some doubt remained, due to the uncertainties that result from the companion star being much heavier than the candidate black hole. Currently, better candidates for black holes are found in a class of X-ray binaries called soft X-ray transients. In this class of system, the companion star is of relatively low mass allowing for more accurate estimates of the black hole mass. These systems actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission, called quiescence, the accretion disk is extremely faint, allowing detailed observation of the companion star during this period. One of the best such candidates is V404 Cygni.\\n\\nQuasi-periodic oscillations\\nThe X-ray emissions from accretion disks sometimes flicker at certain frequencies. These signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). As such their frequency is linked to the mass of the compact object. They can thus be used as an alternative way to determine the mass of candidate black holes.\\n\\nGalactic nuclei\\nAstronomers use the term \"active galaxy\" to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. Theoretical and observational studies have shown that the activity in these active galactic nuclei (AGN) may be explained by the presence of supermassive black holes, which can be millions of times more massive than stellar ones. The models of these AGN consist of a central black hole that may be millions or billions of times more massive than the Sun; a disk of interstellar gas and dust called an accretion disk; and two jets perpendicular to the accretion disk.\\nAlthough supermassive black holes are expected to be found in most AGN, only some galaxies\\' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include the Andromeda Galaxy, M32, M87, NGC 3115, NGC 3377, NGC 4258, NGC 4889, NGC 1277, OJ 287, APM 08279+5255 and the Sombrero Galaxy.\\nIt is now widely accepted that the centre of nearly every galaxy, not just active ones, contains a supermassive black hole. The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy\\'s bulge, known as the M–sigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself.\\n\\nMicrolensing\\nAnother way the black hole nature of an object may be tested is through observation of effects caused by a strong gravitational field in their vicinity. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected, such as light passing through an optic lens. Observations have been made of weak gravitational lensing, in which light rays are deflected by only a few arcseconds. Microlensing occurs when the sources are unresolved and the observer sees a small brightening. The turn of the millennium saw the first 3 candidate detections of black holes in this way, and in January 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole.\\nAnother possibility for observing gravitational lensing by a black hole would be to observe stars orbiting the black hole. There are several candidates for such an observation in orbit around Sagittarius A*.\\n\\nAlternatives\\nThe evidence for stellar black holes strongly relies on the existence of an upper limit for the mass of a neutron star. The size of this limit heavily depends on the assumptions made about the properties of dense matter. New exotic phases of matter could push up this bound. A phase of free quarks at high density might allow the existence of dense quark stars, and some supersymmetric models predict the existence of Q stars. Some extensions of the standard model posit the existence of preons as fundamental building blocks of quarks and leptons, which could hypothetically form preon stars. These hypothetical models could potentially explain a number of observations of stellar black hole candidates. However, it can be shown from arguments in general relativity that any such object will have a maximum mass.\\nSince the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes. The average density of a 108 M☉ black hole is comparable to that of water. Consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates.\\nThe evidence for the existence of stellar and supermassive black holes implies that in order for black holes not to form, general relativity must fail as a theory of gravity, perhaps due to the onset of quantum mechanical corrections. A much anticipated feature of a theory of quantum gravity is that it will not feature singularities or event horizons and thus black holes would not be real artefacts. For example, in the fuzzball model based on string theory, the individual states of a black hole solution do not generally have an event horizon or singularity, but for a classical/semiclassical observer the statistical average of such states appears just as an ordinary black hole as deduced from general relativity.\\nA few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically, but which function via a different mechanism. These include the gravastar, the black star, related nestar and the dark-energy star.\\n\\nOpen questions\\nEntropy and thermodynamics\\nIn 1971, Hawking showed under general conditions that the total area of the event horizons of any collection of classical black holes can never decrease, even if they collide and merge. This result, now known as the second law of black hole mechanics, is remarkably similar to the second law of thermodynamics, which states that the total entropy of an isolated system can never decrease. As with classical objects at absolute zero temperature, it was assumed that black holes had zero entropy. If this were the case, the second law of thermodynamics would be violated by entropy-laden matter entering a black hole, resulting in a decrease in the total entropy of the universe. Therefore, Bekenstein proposed that a black hole should have an entropy, and that it should be proportional to its horizon area.\\nThe link with the laws of thermodynamics was further strengthened by Hawking\\'s discovery in 1974 that quantum field theory predicts that a black hole radiates blackbody radiation at a constant temperature. This seemingly causes a violation of the second law of black hole mechanics, since the radiation will carry away energy from the black hole causing it to shrink. The radiation also carries away entropy, and it can be proven under general assumptions that the sum of the entropy of the matter surrounding a black hole and one quarter of the area of the horizon as measured in Planck units is in fact always increasing. This allows the formulation of the first law of black hole mechanics as an analogue of the first law of thermodynamics, with the mass acting as energy, the surface gravity as temperature and the area as entropy.\\nOne puzzling feature is that the entropy of a black hole scales with its area rather than with its volume, since entropy is normally an extensive quantity that scales linearly with the volume of the system. This odd property led Gerard \\'t Hooft and Leonard Susskind to propose the holographic principle, which suggests that anything that happens in a volume of spacetime can be described by data on the boundary of that volume.\\nAlthough general relativity can be used to perform a semiclassical calculation of black hole entropy, this situation is theoretically unsatisfying. In statistical mechanics, entropy is understood as counting the number of microscopic configurations of a system that have the same macroscopic qualities, such as mass, charge, pressure, etc. Without a satisfactory theory of quantum gravity, one cannot perform such a computation for black holes. Some progress has been made in various approaches to quantum gravity. In 1995, Andrew Strominger and Cumrun Vafa showed that counting the microstates of a specific supersymmetric black hole in string theory reproduced the Bekenstein–Hawking entropy. Since then, similar results have been reported for different black holes both in string theory and in other approaches to quantum gravity like loop quantum gravity.\\n\\nInformation loss paradox\\nBecause a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.\\nThe question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.\\nOne attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the \"firewall paradox\" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted.\\nThis seemingly creates a paradox: a principle called \"monogamy of entanglement\" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein\\'s equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a \"firewall\" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate.\\n\\nIn science fiction\\nChristopher Nolan\\'s 2014 science fiction epic Interstellar features a black hole known as Gargantua, which is the central object of a planetary system in a distant galaxy. Humanity accessed this system via a wormhole in the outer solar system, near Saturn.\\n\\nSee also\\nNotes\\nReferences\\nSources\\nCarroll, Sean M. (2004). Spacetime and Geometry. Addison Wesley. ISBN 978-0-8053-8732-2., the lecture notes on which the book was based are available for free from Sean Carroll\\'s website Archived 23 March 2017 at the Wayback Machine\\nHawking, S. W.; Ellis, G. F. R. (1973). Large Scale Structure of space time. Cambridge University Press. ISBN 978-0-521-09906-6. Archived from the original on 21 July 2020. Retrieved 16 May 2020.\\nMisner, Charles; Thorne, Kip S.; Wheeler, John (1973). Gravitation. W. H. Freeman and Company. ISBN 978-0-7167-0344-0.\\nThorne, Kip S. (1994). Black Holes and Time Warps. Norton, W. W. & Company, Inc. ISBN 978-0-393-31276-8.\\nWald, Robert M. (1984). General Relativity. University of Chicago Press. ISBN 978-0-226-87033-5. Archived from the original on 11 August 2016. Retrieved 23 February 2016.\\nWheeler, J. Craig (2007). Cosmic Catastrophes (2nd ed.). Cambridge University Press. ISBN 978-0-521-85714-7.\\n\\nFurther reading\\nPopular reading\\nUniversity textbooks and monographs\\nReview papers\\nExternal links\\nBlack Holes on In Our Time at the BBC\\nStanford Encyclopedia of Philosophy: \"Singularities and Black Holes\" by Erik Curiel and Peter Bokulich.\\nBlack Holes: Gravity\\'s Relentless Pull – Interactive multimedia Web site about the physics and astronomy of black holes from the Space Telescope Science Institute (HubbleSite)\\nESA\\'s Black Hole Visualization Archived 3 May 2019 at the Wayback Machine\\nFrequently Asked Questions (FAQs) on Black Holes\\nSchwarzschild Geometry\\nBlack holes - basic (NYT; April 2021)\\n\\nVideos\\n16-year-long study tracks stars orbiting Sagittarius A*\\nMovie of Black Hole Candidate from Max Planck Institute\\nCowen, Ron (20 April 2015). \"3D simulations of colliding black holes hailed as most realistic yet\". Nature. doi:10.1038/nature.2015.17360.\\nComputer visualisation of the signal detected by LIGO\\nTwo Black Holes Merge into One (based upon the signal GW150914)', 'A supernova (pl.: supernovae or supernovas) is a powerful and luminous explosion of a star. A supernova occurs during the last evolutionary stages of a massive star, or when a white dwarf is triggered into runaway nuclear fusion. The original object, called the progenitor, either collapses to a neutron star or black hole, or is completely destroyed to form a diffuse nebula. The peak optical luminosity of a supernova can be comparable to that of an entire galaxy before fading over several weeks or months.\\nThe last supernova directly observed in the Milky Way was Kepler\\'s Supernova in 1604, appearing not long after Tycho\\'s Supernova in 1572, both of which were visible to the naked eye.  Observations of recent supernova remnants within the Milky Way, coupled with studies of supernovae in other galaxies, suggest that these powerful stellar explosions occur in our galaxy approximately three times per century on average.  A supernova in the Milky Way would almost certainly be observable through modern astronomical telescopes. The most recent naked-eye supernova was SN 1987A, which was the explosion of a blue supergiant star in the Large Magellanic Cloud, a satellite galaxy of the Milky Way in 1987.\\nTheoretical studies indicate that most supernovae are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a white dwarf, or the sudden gravitational collapse of a massive star\\'s core.\\n\\nIn the re-ignition of a white dwarf, the object\\'s temperature is raised enough to trigger runaway nuclear fusion, completely disrupting the star. Possible causes are an accumulation of material from a binary companion through accretion, or by a stellar merger.\\nIn the case of a massive star\\'s sudden implosion, the core of a massive star will undergo sudden collapse once it is unable to produce sufficient energy from fusion to counteract the star\\'s own gravity, which must happen once the star begins fusing iron, but may happen during an earlier stage of metal fusion.\\nSupernovae can expel several solar masses of material at speeds up to several percent of the speed of light. This drives an expanding shock wave into the surrounding interstellar medium, sweeping up an expanding shell of gas and dust observed as a supernova remnant. Supernovae are a major source of elements in the interstellar medium from oxygen to rubidium. The expanding shock waves of supernovae can trigger the formation of new stars. Supernovae are a major source of cosmic rays. They might also produce gravitational waves.\\n\\nEtymology\\nThe word supernova has the plural form supernovae () or supernovas and is often abbreviated as SN or SNe. It is derived from the Latin word nova, meaning \\'new\\', which refers to what appears to be a temporary new bright star. Adding the prefix \"super-\" distinguishes supernovae from ordinary novae, which are far less luminous. The word supernova was coined by Walter Baade and Fritz Zwicky, who began using it in astrophysics lectures in 1931. Its first use in a journal article came the following year in a publication by Knut Lundmark, who may have coined it independently.\\n\\nObservation history\\nCompared to a star\\'s entire history, the visual appearance of a supernova is very brief, sometimes spanning several months, so that the chances of observing one with the naked eye are roughly once in a lifetime. Only a tiny fraction of the 100 billion stars in a typical galaxy have the capacity to become a supernova, the ability being restricted to those having high mass and those in rare kinds of binary star systems with at least one white dwarf.\\n\\nEarly discoveries\\nThe earliest record of a possible supernova, known as HB9, was likely viewed by an unknown prehistoric people of the Indian subcontinent and recorded on a rock carving in the Burzahama region of Kashmir, dated to 4500±1000 BC. Later, SN 185 was documented by Chinese astronomers in 185 AD. The brightest recorded supernova was SN 1006, which was observed in AD 1006 in the constellation of Lupus. This event was described by observers in China, Japan, Iraq, Egypt and Europe. The widely observed supernova SN 1054 produced the Crab Nebula.\\nSupernovae SN 1572 and SN 1604, the latest Milky Way supernovae to be observed with the naked eye, had a notable influence on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was static and unchanging. Johannes Kepler began observing SN 1604 at its peak on 17 October 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation, after Tycho Brahe observed SN 1572 in Cassiopeia.\\nThere is some evidence that the youngest known supernova in our galaxy, G1.9+0.3, occurred in the late 19th century, considerably more recently than Cassiopeia A from around 1680. Neither was noted at the time. In the case of G1.9+0.3, high extinction from dust along the plane of the galactic disk could have dimmed the event sufficiently for it to go unnoticed. The situation for Cassiopeia A is less clear; infrared light echoes have been detected showing that it was not in a region of especially high extinction.\\n\\nTelescope findings\\nWith the development of the astronomical telescope, observation and discovery of fainter and more distant supernovae became possible. The first such observation was of SN 1885A in the Andromeda Galaxy. A second supernova, SN 1895B, was discovered in NGC 5253 a decade later. Early work on what was originally believed to be simply a new category of novae was performed during the 1920s. These were variously called \"upper-class Novae\", \"Hauptnovae\", or \"giant novae\". The name \"supernovae\" is thought to have been coined by Walter Baade and Zwicky in lectures at Caltech in 1931. It was used, as \"super-Novae\", in a journal paper published by Knut Lundmark in 1933, and in a 1934 paper by Baade and Zwicky. By 1938, the hyphen was no longer used and the modern name was in use.\\nAmerican astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. During the 1960s, astronomers found that the maximum intensities of supernovae could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae observed in 2003 appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernovae events that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the radioactive decay of titanium-44.\\n\\nThe most luminous supernova ever recorded is ASASSN-15lh, at a distance of 3.82 gigalight-years. It was first detected in June 2015 and peaked at 570 billion L☉, which is twice the bolometric luminosity of any other known supernova. The nature of this supernova is debated and several alternative explanations, such as tidal disruption of a star by a black hole, have been suggested.\\nSN 2013fs was recorded three hours after the supernova event on 6 October 2013, by the Intermediate Palomar Transient Factory. This is among the earliest supernovae caught after detonation, and it is the earliest for which spectra have been obtained, beginning six hours after the actual explosion. The star is located in a spiral galaxy named NGC 7610, 160 million light-years away in the constellation of Pegasus.\\nThe supernova SN 2016gkg was detected by amateur astronomer Victor Buso from Rosario, Argentina, on 20 September 2016. It was the first time that the initial \"shock breakout\" from an optical supernova had been observed. The progenitor star has been identified in Hubble Space Telescope images from before its collapse. Astronomer Alex Filippenko noted: \"Observations of stars in the first moments they begin exploding provide information that cannot be directly obtained in any other way.\"\\n\\nDiscovery programs\\nBecause supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies. Today, amateur and professional astronomers are finding about two thousand every year, some when near maximum brightness, others on old astronomical photographs or plates. Supernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. To use supernovae as standard candles for measuring distance, observation of their peak luminosity is required. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.\\nToward the end of the 20th century, astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. The Supernova Early Warning System (SNEWS) project uses a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are subatomic particles that are produced in great quantities by a supernova, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.\\n\\nSupernova searches fall into two classes: those focused on relatively nearby events and those looking farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more-distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of z=0.1–0.3, where z is a dimensionless measure of the spectrum\\'s frequency shift. \\nHigh redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies.\\nAs survey programmes rapidly increase the number of detected supernovae, collated collections of observations (light decay curves, astrometry, pre-supernova observations, spectroscopy) have been assembled. The Pantheon data set, assembled in 2018, detailed 1048 supernovae. In 2021, this data set was expanded to 1701 light curves for 1550 supernovae taken from 18 different surveys, a 50% increase in under 3 years.\\n\\nNaming convention\\nSupernova discoveries are reported to the International Astronomical Union\\'s Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is formed from the prefix SN, followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from A to Z. Next, pairs of lower-case letters are used: aa, ab, and so on. Hence, for example, SN 2003C designates the third supernova reported in the year 2003. The last supernova of 2005, SN 2005nc, was the 367th (14 × 26 + 3 = 367). Since 2000, professional and amateur astronomers have been finding several hundred supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).\\nHistorical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called Tycho\\'s Nova) and SN 1604 (Kepler\\'s Star). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (for example, SN 1885A, SN 1907A, etc.); this last happened with SN 1947A. SN, for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, they have been needed every year. Since 2016, the increasing number of discoveries has regularly led to the additional use of three-letter designations. After zz comes aaa, then aab, aac, and so on. For example, the last supernova retained in the Asiago Supernova Catalogue\\u2009 when it was terminated on 31 December 2017 bears the designation SN 2017jzp.\\n\\nClassification\\nAstronomers classify supernovae according to their light curves and the absorption lines of different chemical elements that appear in their spectra. If a supernova\\'s spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified Type II; otherwise it is Type I. In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova\\'s apparent magnitude as a function of time).\\n\\nType I\\nType I supernovae are subdivided on the basis of their spectra, with Type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as Type Ib and Ic, with Type Ib showing strong neutral helium lines and Type Ic lacking them. Historically, the light curves of Type I supernovae were seen as all broadly similar, too much so to make useful distinctions. While variations in light curves have been studied, classification continues to be made on spectral grounds rather than light-curve shape.\\nA small number of Type Ia supernovae exhibit unusual features, such as non-standard luminosity or broadened light curves, and these are typically categorised by referring to the earliest example showing similar features. For example, the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.\\nA small proportion of Type Ic supernovae show highly broadened and blended emission lines which are taken to indicate very high expansion velocities for the ejecta. These have been classified as Type Ic-BL or Ic-bl.\\nCalcium-rich supernovae are a rare type of very fast supernova with unusually strong calcium lines in their spectra. Models suggest they occur when material is accreted from a helium-rich companion rather than a hydrogen-rich star. Because of helium lines in their spectra, they can resemble Type Ib supernovae, but are thought to have very different progenitors.\\n\\nType II\\nThe supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the \"n\" stands for \"narrow\".\\nA few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term \"Type IIb\" is used to describe the combination of features normally associated with Type II and Type Ib.\\nType II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive \"plateau\" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called Type II-P referring to the plateau. Less common are Type II-L supernovae that lack a distinct plateau. The \"L\" signifies \"linear\" although the light curve is not actually a straight line.\\nSupernovae that do not fit into the normal classifications are designated peculiar, or \"pec\".\\n\\nTypes III, IV and V\\nZwicky defined additional supernovae types based on a very few examples that did not cleanly fit the parameters for Type I or Type II supernovae. SN 1961i in NGC 4303 was the prototype and only member of the Type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the Type IV class, with a light curve similar to a Type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The Type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova impostor with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible Type IV or Type V supernovae.\\nThese types would now all be treated as peculiar Type II supernovae (IIpec), of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an impostor.\\n\\nCurrent models\\nSupernova type codes, as summarised in the table above, are taxonomic: the type number is based on the light observed from the supernova, not necessarily its cause. For example, Type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors, while the spectrally similar Type Ib/c are produced from massive stripped progenitor stars by core collapse.\\n\\nThermal runaway\\nA white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorised to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites the core. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high redshift, and for small variations in brightness identified by light curve shape or spectrum.\\n\\nNormal Type Ia\\nThere are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses (for a non-rotating star), it would no longer be able to support the bulk of its mass through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%) before collapse is initiated. In contrast, for a core primarily composed of oxygen, neon and magnesium, the collapsing white dwarf will typically form a neutron star. In this case, only a fraction of the star\\'s mass will be ejected during the collapse.\\n\\nWithin a few seconds of the collapse process, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–2×1044 J) to unbind the star in a supernova. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.\\nThe model for the formation of this category of supernova is a close binary star system. The larger of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point, it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually, the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. The exact details of initiation and of the heavy elements produced in the catastrophic event remain unclear.\\nType Ia supernovae produce a characteristic light curve—the graph of luminosity as a function of time—after the event. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about −19.3. This is because typical Type Ia supernovae arise from a consistent type of progenitor star by gradual mass acquisition, and explode when they acquire a consistent typical mass, giving rise to very similar supernova conditions and behaviour. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.\\nA second model for the formation of Type Ia supernovae involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. This is sometimes referred to as the double-degenerate model, as both stars are degenerate white dwarfs. Due to the possible combinations of mass and chemical composition of the pair there is much variation in this type of event, and, in many cases, there may be no supernova at all, in which case they will have a less luminous light curve than the more normal SN Type Ia.\\n\\nNon-standard Type Ia\\nAbnormally bright Type Ia supernovae occur when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy. This super-Chandrasekhar-mass scenario can occur, for example, when the extra mass is supported by differential rotation.\\nThere is no formal sub-classification for non-standard Type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as Type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.\\nOne specific type of supernova originates from exploding white dwarfs, like Type Ia, but contains hydrogen lines in their spectra, possibly because the white dwarf is surrounded by an envelope of hydrogen-rich circumstellar material. These supernovae have been dubbed Type Ia/IIn, Type Ian, Type IIa and Type IIan.\\nThe quadruple star HD 74438, belonging to the open cluster IC 2391 the Vela constellation, has been predicted to become a non-standard Type Ia supernova.\\n\\nCore collapse\\nVery massive stars can undergo core collapse when nuclear fusion becomes unable to sustain the core against its own gravity; passing this threshold is the cause of all types of supernova except Type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova. However, if the release of gravitational potential energy is insufficient, the star may instead collapse into a black hole or neutron star with little radiated energy. \\nCore collapse can be caused by several different mechanisms: exceeding the Chandrasekhar limit; electron capture; pair-instability; or photodisintegration.\\n\\nWhen a massive star develops an iron core larger than the Chandrasekhar mass it will no longer be able to support itself by electron degeneracy pressure and will collapse further to a neutron star or black hole.\\nElectron capture by magnesium in a degenerate O/Ne/Mg core (8–10 solar mass progenitor star) removes support and causes gravitational collapse followed by explosive oxygen fusion, with very similar results.\\nElectron-positron pair production in a large post-helium burning core removes thermodynamic support and causes initial collapse followed by runaway fusion, resulting in a pair-instability supernova.\\nA sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.\\nThe table below lists the known reasons for core collapse in massive stars, the types of stars in which they occur, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun\\'s mass, although the mass at the time of the supernova may be much lower.\\nType IIn supernovae are not listed in the table. They can be produced by various types of core collapse in different progenitor stars, possibly even by Type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed Type IIn supernovae are supernova impostors, massive eruptions of LBV-like stars similar to the Great Eruption of Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.\\n\\nDetailed process\\nWhen a stellar core is no longer supported against gravity, it collapses in on itself with velocities reaching 70,000 km/s (0.23c), resulting in a rapid increase in temperature and density. What follows depends on the mass and structure of the collapsing core, with low-mass degenerate cores forming neutron stars, higher-mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.\\nThe initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30 km in diameter with a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about 15 solar masses then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova.\\nIn lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100 billion kelvins, 6,000 times the temperature of the Sun\\'s core. At this temperature, neutrino-antineutrino pairs of all flavours are efficiently formed by thermal emission. These thermal neutrinos are several times more abundant than the electron-capture neutrinos. About 1046 joules, approximately 10% of the star\\'s rest mass, is converted into a ten-second burst of neutrinos, which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls in the outer core within milliseconds as energy is lost through the dissociation of heavy elements. A process that is not clearly understood is necessary to allow the outer layers of the core to reabsorb around 1044 joules (1 foe) from the neutrino pulse, producing the visible brightness, although there are other theories that could power the explosion.\\nSome material from the outer envelope falls back onto the neutron star, and, for cores beyond about 8 M☉, there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy created and the mass of expelled radioactive material, but in some situations, it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.\\nThe collapse of a massive non-degenerate core will ignite further fusion. When the core collapse is initiated by pair instability (photons turning into electron-positron pairs, thereby reducing the radiation pressure) oxygen fusion begins and the collapse may be halted. For core masses of 40–60 M☉, the collapse halts and the star remains intact, but collapse will occur again when a larger core has formed. For cores of around 60–130 M☉, the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected 56Ni. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.\\n\\nType II\\nStars with initial masses less than about 8 M☉ never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9 M☉ (possibly as much as 12 M☉) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super-AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a Type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a supernova of Type II. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\\nStars with an initial mass up to about 90 times the Sun, or a little less at high metallicity, result in a Type II-P supernova, which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a Type II-L supernova. At very low metallicity, stars of around 140–250 M☉ will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with Type II characteristics but a very large mass of ejected 56Ni and high luminosity.\\n\\nType Ib and Ic\\nThese supernovae, like those of Type II, are massive stars that undergo core collapse. Unlike the progenitors of Type II supernovae, the stars which become Type Ib and Type Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass-loss rates. Observations of Type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars. Alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed.\\nType Ib supernovae are the more common and result from Wolf–Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining, and these are the progenitors of Type Ic supernovae.\\nA few percent of the Type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. 0.1 M☉). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be the first observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core. Ultra-stripped supernovae are believed to be associated with the second supernova explosion in a binary system, producing for example a tight double neutron star system.\\nIn 2022 a team of astronomers led by researchers from the Weizmann Institute of Science reported the first supernova explosion showing direct evidence for a Wolf-Rayet progenitor star. SN 2019hgp was a Type Icn supernova and is also the first in which the element neon has been detected.\\n\\nElectron-capture supernovae\\nIn 1980, a \"third type\" of supernova was predicted by Ken\\'ichi Nomoto of the University of Tokyo, called an electron-capture supernova. It would arise when a star \"in the transitional range (~8 to 10 solar masses) between white dwarf formation and iron core-collapse supernovae\", and with a degenerate O+Ne+Mg core, imploded after its core ran out of nuclear fuel, causing gravity to compress the electrons in the star\\'s core into their atomic nuclei, leading to a supernova explosion and leaving behind a neutron star. In June 2021, a paper in the journal Nature Astronomy reported that the 2018 supernova SN 2018zd (in the galaxy NGC 2146, about 31 million light-years from Earth) appeared to be the first observation of an electron-capture supernova. The 1054 supernova explosion that created the Crab Nebula in our galaxy had been thought to be the best candidate for an electron-capture supernova, and the 2021 paper makes it more likely that this was correct.\\n\\nFailed supernovae\\nThe core collapse of some massive stars may not result in a visible supernova. This happens if the initial core collapse cannot be reversed by the mechanism that produces an explosion, usually because the core is too massive. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star\\'s location.\\n\\nLight curves\\nThe ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy—which can maintain the optical supernova glow for months—was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a Type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of 847 keV and 1,238 keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\\n\\nThe late-time decay phase of visual light curves for different supernova types all depend on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.\\nThe light curves for Type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of Type Ia, which are the \"standard candles\" of cosmology but whose diagnostic 847 keV and 1,238 keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\\nType Ib and Ic light curves are similar to Type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional Type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous Type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\\nThe light curves for Type II supernovae are characterised by a much slower decline than Type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of Type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in Type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\\nIn Type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In Type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a Type I supernova and the hydrogen even disappears from the spectrum after several weeks.\\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\\n\\nNotes:\\n\\nAsymmetry\\nA long-standing puzzle surrounding Type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\\nInitial asymmetries have also been confirmed in Type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\\n\\nEnergy output\\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In Type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\\nStandard Type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half-life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from Type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\\nCore collapse supernovae are on average visually fainter than Type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\\n\\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of Type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause Type IIn hypernovae.\\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to Type II-P, the nature after core collapse is more like that of a giant Type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\\n\\nProgenitor\\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star\\'s metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of Type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\\n\\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18 M☉ and 100,000 L☉, respectively. Most progenitors of Type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for Type II-P supernovae to form to be 8.5+1−1.5 M☉ and 16.5±1.5 M☉, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at 19+4−2 M☉.\\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of Type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for Type IIb supernovae, and almost all Type IIb supernovae near enough to observe have shown such progenitors.\\n\\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a Type IIn supernova. Several examples of hot luminous progenitors of Type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\\nThe progenitors of Type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed Type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal Type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic Type Ic-BL events that are associated with long-duration gamma-ray bursts.\\n\\nExternal impact\\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\\n\\nSource of heavy elements\\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than Type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by Type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova\\'s collapse for elements heavier than iron.  The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\\n\\nRole in stellar evolution\\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\\n\\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as \"metals\". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star\\'s life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system.\\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\\n\\nCosmic rays\\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\\n\\nGravitational waves\\nSupernovae are potentially strong galactic sources of gravitational waves, but none have so far been detected. The only gravitational wave events so far detected are from mergers of black holes and neutron stars, probable remnants of supernovae. Like the neutrino emissions, the gravitational waves produced by a core-collapse supernova are expected to arrive without the delay that affects light. Consequently, they may provide information about the core-collapse process that is unavailable by other means. Most gravitational-wave signals predicted by supernova models are short in duration, lasting less than a second, and thus difficult to detect. Using the arrival of a neutrino signal may provide a trigger that can identify the time window in which to seek the gravitational wave, helping to distinguish the latter from background noise.\\n\\nEffect on Earth\\nA near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3,000 light-years away.\\nIn 1996 it was theorised that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted atmospheric levels of nitrogen oxides, which became trapped in the ice.\\nHistorically, nearby supernovae may have influenced the biodiversity of life on the planet. Geological records suggest that nearby supernova events have led to an increase in cosmic rays, which in turn produced a cooler climate. A greater temperature difference between the poles and the equator created stronger winds, increased ocean mixing, and resulted in the transport of nutrients to shallow waters along the continental shelves. This led to greater biodiversity.\\nType Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars in binary systems, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest-known candidate is IK Pegasi (HR 8210), about 150 light-years away, but observations suggest that it could be as long as 1.9 billion years before the white dwarf can accrete the critical mass required to become a Type Ia supernova. \\nAccording to a 2003 estimate, a Type II supernova would have to be closer than 8 parsecs (26 light-years) to destroy half of the Earth\\'s ozone layer, and there are no such candidates closer than about 500 light-years.\\n\\nMilky Way candidates\\nThe next supernova in the Milky Way will likely be detectable even if it occurs on the far side of the galaxy. It is likely to be produced by the collapse of an unremarkable red supergiant, and it is very probable that it will already have been catalogued in infrared surveys such as 2MASS. There is a smaller chance that the next core collapse supernova will be produced by a different type of massive star such as a yellow hypergiant, luminous blue variable, or Wolf–Rayet. The chances of the next supernova being a Type Ia produced by a white dwarf are calculated to be about a third of those for a core collapse supernova. Again it should be observable wherever it occurs, but it is less likely that the progenitor will ever have been observed. It is not even known exactly what a Type Ia progenitor system looks like, and it is difficult to detect them beyond a few parsecs. The total supernova rate in the Milky Way is estimated to be between 2 and 12 per century, although one has not actually been observed for several centuries.\\nStatistically, the most common variety of core-collapse supernova is Type II-P, and the progenitors of this type are red supergiants. It is difficult to identify which of those supergiants are in the final stages of heavy element fusion in their cores and which have millions of years left. The most-massive red supergiants shed their atmospheres and evolve to Wolf–Rayet stars before their cores collapse. All Wolf–Rayet stars end their lives from the Wolf–Rayet phase within a million years or so, but again it is difficult to identify those that are closest to core collapse. One class that is expected to have no more than a few thousand years before exploding are the WO Wolf–Rayet stars, which are known to have exhausted their core helium. Only eight of them are known, and only four of those are in the Milky Way.\\nA number of close or well-known stars have been identified as possible core collapse supernova candidates: the high-mass blue stars Spica, Rigel and Deneb, the red supergiants Betelgeuse, Antares, and VV Cephei A; the yellow hypergiant Rho Cassiopeiae; the luminous blue variable Eta Carinae that has already produced a supernova impostor; and both components, a blue supergiant and a Wolf–Rayet star, of the Regor or Gamma Velorum system. Mimosa, Acrux and Hadar or Beta Centauri, three bright star systems in the southern constellation of Crux and Centaurus respectively, each contain blue stars with sufficient masses to explode as supernovae. Others have gained notoriety as possible, although not very likely, progenitors for a gamma-ray burst; for example WR 104.\\nIdentification of candidates for a Type Ia supernova is much more speculative. Any binary with an accreting white dwarf might produce a supernova, although the exact mechanism and timescale is still debated. These systems are faint and difficult to identify, but the novae and recurrent novae are such systems that conveniently advertise themselves. One example is U Scorpii.\\n\\nSee also\\nKilonova – Neutron star merger\\nList of supernovae\\nList of supernova remnants\\nList of supernova candidates\\nQuark-nova – Hypothetical violent explosion resulting from conversion of a neutron star to a quark star\\nSuperluminous supernova – Supernova at least ten times more luminous than a standard supernova\\nSupernovae in fiction\\nTimeline of white dwarfs, neutron stars, and supernovae – Chronological list of developments in knowledge and records\\nCollapsar – Star that has undergone gravitational collapse\\nHypernova – Supernova that ejects a large mass at unusually high velocity\\n\\nReferences\\nFurther reading\\nAthem W. Alsabti; Paul Murdin, eds. (2017). Handbook of Supernovae. Springer Cham. Bibcode:2017hsn..book.....A. doi:10.1007/978-3-319-21846-5. ISBN 978-3-319-21845-8.\\nBranch, D.; Wheeler, J. C. (2017). Supernova Explosions. Springer. p. 721. ISBN 978-3-662-55052-6.\\nIliadis, Christian (2015). Nuclear Physics of Stars, 2nd ed. Weinheim: Wiley-VCH. doi:10.1002/9783527692668. ISBN 9783527692668.\\nTakahashi, K.; Sato, K.; Burrows, A.; Thompson, T. A. (2003). \"Supernova Neutrinos, Neutrino Oscillations, and the Mass of the Progenitor Star\". Physical Review D. 68 (11): 77–81. arXiv:hep-ph/0306056. Bibcode:2003PhRvD..68k3009T. doi:10.1103/PhysRevD.68.113009. S2CID 119390151.\\nWoosley, S. E.; Janka, H.-T. (2005). \"The Physics of Core-Collapse Supernovae\". Nature Physics. 1 (3): 147–154. arXiv:astro-ph/0601261. Bibcode:2005NatPh...1..147W. CiteSeerX 10.1.1.336.2176. doi:10.1038/nphys172. S2CID 118974639.\\n\\nExternal links\\n\\nTsvetkov, D. Yu.; Pavlyuk, N. N.; Bartunov, O. S.; Pskovskii, Y. P. \"Sternberg Astronomical Institute Supernova Catalogue\". Sternberg Astronomical Institute, Moscow University. Retrieved 28 November 2006. A searchable catalogue\\n\"The Open Supernova Catalog\". GitHub. 6 October 2022. An open-access catalog of supernova light curves and spectra.\\n\"List of Supernovae with IAU Designations\". IAU: Central Bureau for Astronomical Telegrams. Retrieved 25 October 2010.', 'Deoxyribonucleic acid ( ; DNA) is a polymer composed of two polynucleotide chains that coil around each other to form a double helix. The polymer carries genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.\\nThe two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phosphodiester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, the single-ringed pyrimidines and the double-ringed purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.\\nBoth strands of double-stranded DNA store the same biological information. This information is replicated when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (or bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U). Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation.\\nWithin eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\\n\\nProperties\\nDNA is a long polymer made from repeating units called nucleotides. The structure of DNA is dynamic along its length, being capable of coiling into tight loops and other shapes. In all species it is composed of two helical chains, bound to each other by hydrogen bonds. Both chains are coiled around the same axis, and have the same pitch of 34 ångströms (3.4 nm). The pair of chains have a radius of 10 Å (1.0 nm). According to another study, when measured in a different solution, the DNA chain measured 22–26 Å (2.2–2.6 nm) wide, and one nucleotide unit measured 3.3 Å (0.33 nm) long. The buoyant density of most DNA is 1.7g/cm3.\\nDNA does not usually exist as a single strand, but instead as a pair of strands that are held tightly together. These two long strands coil around each other, in the shape of a double helix. The nucleotide contains both a segment of the backbone of the molecule (which holds the chain together) and a nucleobase (which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called a nucleoside, and a base linked to a sugar and to one or more phosphate groups is called a nucleotide. A biopolymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.\\nThe backbone of the DNA strand is made from alternating phosphate and sugar groups. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These are known as the 3′-end (three prime end), and 5′-end (five prime end) carbons, the prime symbol being used to distinguish these carbon atoms from those of the base to which the deoxyribose forms a glycosidic bond.\\nTherefore, any DNA strand normally has one end at which there is a phosphate group attached to the 5′ carbon of a ribose (the 5′ phosphoryl) and another end at which there is a free hydroxyl group attached to the 3′ carbon of a ribose (the 3′ hydroxyl). The orientation of the 3′ and 5′ carbons along the sugar-phosphate backbone confers directionality (sometimes called polarity) to each DNA strand. In a nucleic acid double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are antiparallel. The asymmetric ends of DNA strands are said to have a directionality of five prime end (5′ ), and three prime end (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the related pentose sugar ribose in RNA.\\n\\nThe DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. The four bases found in DNA are adenine (A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine, forming A-T and G-C base pairs.\\n\\nNucleobase classification\\nThe nucleobases are classified into two types: the purines, A and G, which are fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA, many artificial nucleic acid analogues have been created to study the properties of nucleic acids, or for use in biotechnology.\\n\\nNon-canonical bases\\nModified bases occur in DNA. The first of these recognized was 5-methylcytosine, which was found in the genome of Mycobacterium tuberculosis in 1925. The reason for the presence of these noncanonical bases in bacterial viruses (bacteriophages) is to avoid the restriction enzymes present in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses. Modifications of the bases cytosine and adenine, the more common and modified DNA bases, play vital roles in the epigenetic control of gene expression in plants and animals.\\nA number of noncanonical bases are known to occur in DNA. Most of these are modifications of the canonical bases plus uracil.\\n\\nModified Adenine\\nN6-carbamoyl-methyladenine\\nN6-methyadenine\\nModified Guanine\\n7-Deazaguanine\\n7-Methylguanine\\nModified Cytosine\\nN4-Methylcytosine\\n5-Carboxylcytosine\\n5-Formylcytosine\\n5-Glycosylhydroxymethylcytosine\\n5-Hydroxycytosine\\n5-Methylcytosine\\nModified Thymidine\\nα-Glutamythymidine\\nα-Putrescinylthymine\\nUracil and modifications\\nBase J\\nUracil\\n5-Dihydroxypentauracil\\n5-Hydroxymethyldeoxyuracil\\nOthers\\nDeoxyarchaeosine\\n2,6-Diaminopurine (2-Aminoadenine)\\n\\nGrooves\\nTwin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. The major groove is 22 ångströms (2.2 nm) wide, while the minor groove is 12 Å (1.2 nm) in width. Due to the larger width of the major groove, the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such as transcription factors that can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell (see below), but the major and minor grooves are always named to reflect the differences in width that would be seen if the DNA was twisted back into the ordinary B form.\\n\\nBase pairing\\nIn a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix (from six-carbon ring to six-carbon ring) is called a Watson-Crick base pair. DNA with high GC-content is more stable than DNA with low GC-content. A Hoogsteen base pair (hydrogen bonding the 6-carbon ring to the 5-carbon ring) is a rare variation of base-pairing. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can thus be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this base pair complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. This reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in organisms.\\n\\nssDNA vs. dsDNA\\nMost DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double-stranded (dsDNA) structure is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart—a process known as melting—to form two single-stranded DNA (ssDNA) molecules. Melting occurs at high temperatures, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).\\nThe stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the melting temperature (also called Tm value), which is the temperature at which 50% of the double-strand molecules are converted to single-strand molecules; melting temperature is dependent on ionic strength and the concentration of DNA. As a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have more strongly interacting strands, while short helices with high AT content have more weakly interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.\\nIn the laboratory, the strength of this interaction can be measured by finding the melting temperature Tm necessary to break half of the hydrogen bonds. When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules have no single common shape, but some conformations are more stable than others.\\n\\nAmount\\nIn humans, the total female diploid nuclear genome per cell extends for 6.37 Gigabase pairs (Gbp), is 208.23 cm long and weighs 6.51 picograms (pg). Male values are 6.27 Gbp, 205.00 cm, 6.41 pg. Each DNA polymer can contain hundreds of millions of nucleotides, such as in chromosome 1. Chromosome 1 is the largest human chromosome with approximately 220 million base pairs, and would be 85 mm long if straightened.\\nIn eukaryotes, in addition to nuclear DNA, there is also mitochondrial DNA (mtDNA) which encodes certain proteins used by the mitochondria. The mtDNA is usually relatively small in comparison to the nuclear DNA. For example, the human mitochondrial DNA forms closed circular molecules, each of which contains 16,569 DNA base pairs, with each such molecule normally containing a full set of the mitochondrial genes. Each human mitochondrion contains, on average, approximately 5 such mtDNA molecules. Each human cell contains approximately 100 mitochondria, giving a total number of mtDNA molecules per human cell of approximately 500. However, the amount of mitochondria per cell also varies by cell type, and an egg cell can contain 100,000 mitochondria, corresponding to up to 1,500,000 copies of the mitochondrial genome (constituting up to 90% of the DNA of the cell).\\n\\nSense and antisense\\nA DNA sequence is called a \"sense\" sequence if it is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the \"antisense\" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e., both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.\\nA few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.\\n\\nSupercoiling\\nDNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its \"relaxed\" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.\\n\\nAlternative DNA structures\\nDNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, and the presence of polyamines in solution.\\nThe first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson functions that provided only a limited amount of structural information for oriented fibers of DNA. An alternative analysis was proposed by Wilkins et al. in 1953 for the in vivo B-DNA X-ray diffraction-scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double helix.\\nAlthough the B-DNA form is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.\\nCompared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partly dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, and in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.\\n\\nAlternative DNA chemistry\\nFor many years, exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1 was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.\\n\\nQuadruplex structures\\nAt the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.\\nThese guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases, known as a guanine tetrad, form a flat plate. These flat four-base units then stack on top of each other to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.\\nIn addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.\\n\\nBranched DNA\\nIn DNA, fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.\\n\\nArtificial bases\\nSeveral artificial nucleobases have been synthesized, and successfully incorporated in the eight-base DNA analogue named Hachimoji DNA. Dubbed S, B, P, and Z, these artificial bases are capable of bonding with each other in a predictable way (S–B and P–Z), maintain the double helix structure of DNA, and be transcribed to RNA. Their existence could be seen as an indication that there is nothing special about the four natural nucleobases that evolved on Earth. On the other hand, DNA is tightly related to RNA which does not only act as a transcript of DNA but also performs as molecular machines many tasks in cells. For this purpose it has to fold into a structure. It has been shown that to allow to create all possible structures at least four bases are required for the corresponding RNA, while a higher number is also possible but this would be against the natural principle of least effort.\\n\\nAcidity\\nThe phosphate groups of DNA give it similar acidic properties to phosphoric acid and it can be considered as a strong acid. It will be fully ionized at a normal cellular pH, releasing protons which leave behind negative charges on the phosphate groups. These negative charges protect DNA from breakdown by hydrolysis by repelling nucleophiles which could hydrolyze it.\\n\\nMacroscopic appearance\\nPure DNA extracted from cells forms white, stringy clumps.\\n\\nChemical modifications and altered DNA packaging\\nBase modifications and DNA packaging\\nThe expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.\\nFor one example, cytosine methylation produces 5-methylcytosine, which is important for X-inactivation of chromosomes. The average level of methylation varies between organisms—the worm Caenorhabditis elegans lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the \"J-base\" in kinetoplastids.\\n\\nDamage\\nDNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions, deletions from the DNA sequence, and chromosomal translocations. These mutations can cause cancer. Because of inherent limits in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.\\nMany mutagens fit into the space between two adjacent base pairs, this is called intercalation. Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo[a]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.\\n\\nBiological functions\\nDNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In an alternative fashion, a cell may copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.\\n\\nGenes and genomes\\nGenomic DNA is tightly and orderly packed in the process called DNA condensation, to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete set of this information in an organism is called its genotype. A gene is a unit of heredity and is a region of DNA that influences a particular characteristic in an organism. Genes contain an open reading frame that can be transcribed, and regulatory sequences such as promoters and enhancers, which control transcription of the open reading frame.\\nIn many species, only a small fraction of the total sequence of the genome encodes protein. For example, only about 1.5% of the human genome consists of protein-coding exons, with over 50% of human DNA consisting of non-coding repetitive sequences. The reasons for the presence of so much noncoding DNA in eukaryotic genomes and the extraordinary differences in genome size, or C-value, among species, represent a long-standing puzzle known as the \"C-value enigma\". However, some DNA sequences that do not code protein may still encode functional non-coding RNA molecules, which are involved in the regulation of gene expression.\\n\\nSome noncoding DNA sequences play structural roles in chromosomes. Telomeres and centromeres typically contain few genes but are important for the function and stability of chromosomes. An abundant form of noncoding DNA in humans are pseudogenes, which are copies of genes that have been disabled by mutation. These sequences are usually just molecular fossils, although they can occasionally serve as raw genetic material for the creation of new genes through the process of gene duplication and divergence.\\n\\nTranscription and translation\\nA gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter \\'words\\' called codons formed from a sequence of three nucleotides (e.g., ACT, CAG, TTT).\\nIn transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (43 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three \\'stop\\' or \\'nonsense\\' codons signifying the end of the coding region; these are the TAG, TAA, and TGA codons, (UAG, UAA, and UGA on the mRNA).\\n\\nReplication\\nCell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand\\'s complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.\\n\\nExtracellular nucleic acids\\nNaked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L. Various possible functions have been proposed for eDNA: it may be involved in horizontal gene transfer; it may provide nutrients; and it may act as a buffer to recruit or titrate ions or antibiotics. Extracellular DNA acts as a functional extracellular matrix component in the biofilms of several bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm; it may contribute to biofilm formation; and it may contribute to the biofilm\\'s physical strength and resistance to biological stress.\\nCell-free fetal DNA is found in the blood of the mother, and can be sequenced to determine a great deal of information about the developing fetus.\\nUnder the name of environmental DNA eDNA has seen increased use in the natural sciences as a survey tool for ecology, monitoring the movements and presence of species in water, air, or on land, and assessing an area\\'s biodiversity.\\n\\nNeutrophil extracellular traps\\nNeutrophil extracellular traps (NETs) are networks of extracellular fibers, primarily composed of DNA, which allow neutrophils, a type of white blood cell, to kill extracellular pathogens while minimizing damage to the host cells.\\n\\nInteractions with proteins\\nAll the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.\\n\\nDNA-binding proteins\\nStructural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes, this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones, making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are thus largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation, and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.\\nA distinct group of DNA-binding proteins is the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination, and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.\\n\\nIn contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.\\nAs these DNA targets can occur throughout an organism\\'s genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors\\' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to \"read\" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.\\n\\nDNA-modifying enzymes\\nNucleases and ligases\\nNucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the horizontal line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.\\nEnzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.\\n\\nTopoisomerases and helicases\\nTopoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.\\nHelicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly adenosine triphosphate (ATP), to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.\\n\\nPolymerases\\nPolymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products is created based on existing polynucleotide chains—which are called templates. These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.\\nIn DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. To preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.\\nRNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. For example, HIV reverse transcriptase is an enzyme for AIDS virus replication. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure. It synthesizes telomeres at the ends of chromosomes. Telomeres prevent fusion of the ends of neighboring chromosomes and protect chromosome ends from damage.\\nTranscription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.\\n\\nGenetic recombination\\nA DNA helix usually does not interact with other segments of DNA, and in human cells, the different chromosomes even occupy separate areas in the nucleus called \"chromosome territories\". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is in chromosomal crossover which occurs during sexual reproduction, when genetic recombination occurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.\\nRecombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell\\'s response to double-strand breaks.\\nThe most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA. Only strands of like polarity exchange DNA during recombination. There are two types of cleavage: east-west cleavage and north–south cleavage. The north–south cleavage nicks both strands of DNA, while the east–west cleavage has one strand of DNA intact. The formation of a Holliday junction during recombination makes it possible for genetic diversity, genes to exchange on chromosomes, and expression of wild-type viral genomes.\\n\\nEvolution\\nDNA contains the genetic information that allows all forms of life to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes. However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.\\nBuilding blocks of DNA (adenine, guanine, and related organic molecules) may have been formed extraterrestrially in outer space. Complex DNA and RNA organic compounds of life, including uracil, cytosine, and thymine, have also been formed in the laboratory under conditions mimicking those found in outer space, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar cosmic dust and gas clouds.\\nAncient DNA has been recovered from ancient organisms at a timescale where genome evolution can be directly observed, including from extinct organisms up to millions of years old, such as the woolly mammoth.\\n\\nUses in technology\\nGenetic engineering\\nMethods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.\\n\\nDNA profiling\\nForensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator. This process is formally termed DNA profiling, also called DNA fingerprinting. In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.\\nThe development of forensic science and the ability to now obtain genetic matching on minute samples of blood, skin, saliva, or hair has led to re-examining many cases. Evidence can now be uncovered that was scientifically impossible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where prior trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defense to DNA matches obtained forensically is to claim that cross-contamination of evidence has occurred. This has resulted in meticulous strict handling procedures with new cases of serious crime.\\nDNA profiling is also used successfully to positively identify victims of mass casualty incidents, bodies or body parts in serious accidents, and individual victims in mass war graves, via matching to family members.\\nDNA profiling is also used in DNA paternity testing to determine if someone is the biological parent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. Normal DNA sequencing methods happen after birth, but there are new methods to test paternity while a mother is still pregnant.\\n\\nDNA enzymes or catalytic DNA\\nDeoxyribozymes, also called DNAzymes or catalytic DNA, were first discovered in 1994. They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach called in vitro selection or systematic evolution of ligands by exponential enrichment (SELEX). DNAzymes catalyze variety of chemical reactions including RNA-DNA cleavage, RNA-DNA ligation, amino acids phosphorylation-dephosphorylation, carbon-carbon bond formation, etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction. The most extensively studied class of DNAzymes is RNA-cleaving types which have been used to detect different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific). The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in cells.\\n\\nBioinformatics\\nBioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning, and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes\\' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.\\n\\nDNA nanotechnology\\nDNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the DNA origami method) and three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins. DNA and other nucleic acids are the basis of aptamers, synthetic oligonucleotide ligands for specific target molecules used in a range of biotechnology and biomedical applications.\\n\\nHistory and anthropology\\nBecause DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology.\\n\\nInformation storage\\nDNA as a storage device for information has enormous potential since it has much higher storage density compared to electronic devices. However, high costs, slow read and write times (memory latency), and insufficient reliability has prevented its practical use.\\n\\nHistory\\nDNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it \"nuclein\". In 1878, Albrecht Kossel isolated the non-protein component of \"nuclein\", nucleic acid, and later isolated its five primary nucleobases.\\nIn 1909, Phoebus Levene identified the base, sugar, and phosphate nucleotide unit of RNA (then named \"yeast nucleic acid\"). In 1929, Levene identified deoxyribose sugar in \"thymus nucleic acid\" (DNA). Levene suggested that DNA consisted of a string of four nucleotide units linked together through the phosphate groups (\"tetranucleotide hypothesis\"). Levene thought the chain was short and the bases repeated in a fixed order. In 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a \"giant hereditary molecule\" made up of \"two mirror strands that would replicate in a semi-conservative fashion using each strand as a template\". In 1928, Frederick Griffith in his experiment discovered that traits of the \"smooth\" form of Pneumococcus could be transferred to the \"rough\" form of the same bacteria by mixing killed \"smooth\" bacteria with the live \"rough\" form. This system provided the first clear suggestion that DNA carries genetic information.\\nIn 1933, while studying virgin sea urchin eggs, Jean Brachet suggested that DNA is found in the cell nucleus and that RNA is present exclusively in the cytoplasm. At the time, \"yeast nucleic acid\" (RNA) was thought to occur only in plants, while \"thymus nucleic acid\" (DNA) only in animals. The latter was thought to be a tetramer, with the function of buffering cellular pH. In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.\\nIn 1943, Oswald Avery, along with co-workers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle, supporting Griffith\\'s suggestion (Avery–MacLeod–McCarty experiment). Erwin Chargaff developed and published observations now known as Chargaff\\'s rules, stating that in DNA from any species of any organism, the amount of guanine should be equal to cytosine and the amount of adenine should be equal to thymine. \\n\\nBy 1951, Alec Todd and collaborators at the University of Cambridge had determined by biochemical methods how the backbone of DNA is structured via the successive linking of carbon atoms 3 and 5 of the sugar to phosphates. This would help to corroborate Watson and Crick\\'s later X-ray structural work. Todd would later be awarded the 1957 Nobel Prize in Chemistry for this and other discoveries related to DNA.\\nLate in 1951, Francis Crick started working with James Watson at the Cavendish Laboratory within the University of Cambridge. DNA\\'s role in heredity was confirmed in 1952 when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the enterobacteria phage T2.\\n\\nIn May 1952, Raymond Gosling, a graduate student working under the supervision of Rosalind Franklin, took an X-ray diffraction image, labeled as \"Photo 51\", at high hydration levels of DNA. This photo was given to Watson and Crick by Maurice Wilkins and was critical to their obtaining the correct structure of DNA. Franklin told Crick and Watson that the backbones had to be on the outside. Before then, Linus Pauling, and Watson and Crick, had erroneous models with the chains inside and the bases pointing outwards. Franklin\\'s identification of the space group for DNA crystals proved her correct. In February 1953, Linus Pauling and Robert Corey proposed a model for nucleic acids containing three intertwined chains, with the phosphates near the axis, and the bases on the outside. Watson and Crick completed their model, which is now accepted as the first correct model of the double helix of DNA. On 28 February 1953 Crick interrupted patrons\\' lunchtime at The Eagle pub in Cambridge, England to announce that he and Watson had \"discovered the secret of life\".\\n\\nThe 25 April 1953 issue of the journal Nature published a series of five articles giving the Watson and Crick double-helix structure DNA and evidence supporting it. The structure was reported in a letter titled \"MOLECULAR STRUCTURE OF NUCLEIC ACIDS A Structure for Deoxyribose Nucleic Acid\", in which they said, \"It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material.\" This letter was followed by a letter from Franklin and Gosling, which was the first publication of their own X-ray diffraction data and of their original analysis method. Then followed a letter by Wilkins and two of his colleagues, which contained an analysis of in vivo B-DNA X-ray patterns, and which supported the presence in vivo of the Watson and Crick structure.\\nIn April 2023, scientists, based on new evidence, concluded that Rosalind Franklin was a contributor and \"equal player\" in the discovery process of DNA, rather than otherwise, as may have been presented subsequently after the time of the discovery. In 1962, after Franklin\\'s death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.\\nIn an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the \"adaptor hypothesis\". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and co-workers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley, and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.\\n\\nIn 1986, DNA analysis was first used in a criminal investigation when police in the UK requested Alec Jeffreys of the University of Leicester to prove or disprove the involvement in a particular case of a suspect who claimed innocence in the matter. Although the suspect had already confessed to committing a recent rape-murder, he was denying any involvement in a similar crime committed three years earlier. Yet the details of the two cases were so alike that the police concluded both crimes had been committed by the same person. However, all charges against the suspect were dropped when Jeffreys\\' DNA testing exonerated the suspect — from both the earlier murder and the one to which he\\'d confessed. Further such DNA profiling led to positive identification of another suspect (Colin Pitchfork) who, in 1988, was found guilty of both rape-murders.\\n\\nSee also\\nReferences\\nFurther reading\\nExternal links\\n\\nDNA binding site prediction on protein\\nDNA the Double Helix Game From the official Nobel Prize web site\\nDNA under electron microscope\\nDolan DNA Learning Center\\nDouble Helix: 50 years of DNA, Nature\\nProteopedia DNA\\nProteopedia Forms_of_DNA\\nENCODE threads explorer ENCODE home page at Nature\\nDouble Helix 1953–2003 National Centre for Biotechnology Education\\nGenetic Education Modules for Teachers – DNA from the Beginning Study Guide\\nPDB Molecule of the Month DNA\\n\"Clue to chemistry of heredity found\". The New York Times, June 1953. First American newspaper coverage of the discovery of the DNA structure\\nDNA from the Beginning Another DNA Learning Center site on DNA, genes, and heredity from Mendel to the human genome project.\\nThe Register of Francis Crick Personal Papers 1938 – 2007 at Mandeville Special Collections Library, University of California, San Diego\\nSeven-page, handwritten letter that Crick sent to his 12-year-old son Michael in 1953 describing the structure of DNA. See Crick\\'s medal goes under the hammer, Nature, 5 April 2013.', 'Photosynthesis ( FOH-tə-SINTH-ə-sis) is a system of biological processes by which photopigment-bearing autotrophic organisms, such as most plants, algae and cyanobacteria, convert light energy — typically from sunlight — into the chemical energy necessary to fuel their metabolism. The term photosynthesis usually refers to oxygenic photosynthesis, a process that releases oxygen as a byproduct of water splitting. Photosynthetic organisms store the converted chemical energy within the bonds of intracellular organic compounds (complex compounds containing carbon), typically carbohydrates like sugars (mainly glucose, fructose and sucrose), starches, phytoglycogen and cellulose. When needing to use this stored energy, an organism\\'s cells then metabolize the organic compounds through cellular respiration. Photosynthesis plays a critical role in producing and maintaining the oxygen content of the Earth\\'s atmosphere, and it supplies most of the biological energy necessary for complex life on Earth.\\nSome organisms also perform anoxygenic photosynthesis, which does not produce oxygen. Some bacteria (e.g. purple bacteria) uses bacteriochlorophyll to split hydrogen sulfide as a reductant instead of water, releasing sulfur instead of oxygen, which was a dominant form of photosynthesis in the euxinic Canfield oceans during the Boring Billion. Archaea such as Halobacterium also perform a type of non-carbon-fixing anoxygenic photosynthesis, where the simpler photopigment retinal and its microbial rhodopsin derivatives are used to absorb green light and produce a proton (hydron) gradient across the cell membrane, and the subsequent ion movement powers transmembrane proton pumps to directly synthesize adenosine triphosphate (ATP), the \"energy currency\" of cells. Such archaeal photosynthesis might have been the earliest form of photosynthesis that evolved on Earth, as far back as the Paleoarchean, preceding that of cyanobacteria (see Purple Earth hypothesis).\\nWhile the details may differ between species, the process always begins when light energy is absorbed by the reaction centers, proteins that contain photosynthetic pigments or chromophores. In plants, these pigments are chlorophylls (a porphyrin derivative that absorbs the red and blue spectra of light, thus reflecting green) held inside chloroplasts, abundant in leaf cells. In cyanobacteria, they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two important molecules that participate in energetic processes: reduced nicotinamide adenine dinucleotide phosphate (NADPH) and ATP.\\nIn plants, algae, and cyanobacteria, sugars are synthesized by a subsequent sequence of light-independent reactions called the Calvin cycle. In this process, atmospheric carbon dioxide is incorporated into already existing organic compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose. In other bacteria, different mechanisms like the reverse Krebs cycle are used to achieve the same end.\\nThe first photosynthetic organisms probably evolved early in the evolutionary history of life using reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. The average rate of energy captured by global photosynthesis is approximately 130 terawatts, which is about eight times the total power consumption of human civilization. Photosynthetic organisms also convert around 100–115 billion tons (91–104 Pg petagrams, or billions of metric tons), of carbon into biomass per year. Photosynthesis was discovered in 1779 by Jan Ingenhousz who showed that plants need light, not just soil and water.\\n\\nOverview\\nMost photosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from carbon dioxide and water using energy from light. However, not all organisms use carbon dioxide as a source of carbon atoms to carry out photosynthesis; photoheterotrophs use organic compounds, rather than carbon dioxide, as a source of carbon.\\nIn plants, algae, and cyanobacteria, photosynthesis releases oxygen. This oxygenic photosynthesis is by far the most common type of photosynthesis used by living organisms. Some shade-loving plants (sciophytes) produce such low levels of oxygen during photosynthesis that they use all of it themselves instead of releasing it to the atmosphere.\\nAlthough there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties of anoxygenic photosynthesis, used mostly by bacteria, which consume carbon dioxide but do not release oxygen or which produce elemental sulfur instead of molecular oxygen.\\nCarbon dioxide is converted into sugars in a process called carbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide into carbohydrates. Carbon fixation is an endothermic redox reaction. In general outline, photosynthesis is the opposite of cellular respiration: while photosynthesis is a process of reduction of carbon dioxide to carbohydrates, cellular respiration is the oxidation of carbohydrates or other nutrients to carbon dioxide. Nutrients used in cellular respiration include carbohydrates, amino acids and fatty acids. These nutrients are oxidized to produce carbon dioxide and water, and to release chemical energy to drive the organism\\'s metabolism.\\nPhotosynthesis and cellular respiration are distinct processes, as they take place through different sequences of chemical reactions and in different cellular compartments (cellular respiration in mitochondria).\\nThe general equation for photosynthesis as first proposed by Cornelis van Niel is:\\n\\nCO2carbondioxide + 2H2Aelectron donor + photonslight energy → [CH2O]carbohydrate + 2Aoxidizedelectrondonor + H2Owater\\nSince water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:\\n\\nCO2carbondioxide + 2H2Owater + photonslight energy → [CH2O]carbohydrate + O2oxygen + H2Owater\\nThis equation emphasizes that water is both a reactant in the light-dependent reaction and a product of the light-independent reaction, but canceling n water molecules from each side gives the net equation:\\n\\nCO2carbondioxide + H2O water  + photonslight energy → [CH2O]carbohydrate + O2 oxygen \\nOther processes substitute other compounds (such as arsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite to arsenate: The equation for this reaction is:\\n\\nCO2carbondioxide + (AsO3−3)arsenite + photonslight energy → (AsO3−4)arsenate + COcarbonmonoxide(used to build other compounds in subsequent reactions)\\nPhotosynthesis occurs in two stages. In the first stage, light-dependent reactions or light reactions capture the energy of light and use it to make the hydrogen carrier NADPH and the energy-storage molecule ATP. During the second stage, the light-independent reactions use these products to capture and reduce carbon dioxide.\\nMost organisms that use oxygenic photosynthesis use visible light for the light-dependent reactions, although at least three use shortwave infrared or, more specifically, far-red radiation.\\nSome organisms employ even more radical variants of photosynthesis. Some archaea use a simpler method that employs a pigment similar to those used for vision in animals. The bacteriorhodopsin changes its configuration in response to sunlight, acting as a proton pump. This produces a proton gradient more directly, which is then converted to chemical energy. The process does not involve carbon dioxide fixation and does not release oxygen, and seems to have evolved separately from the more common types of photosynthesis.\\n\\nPhotosynthetic membranes and organelles\\nIn photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded in cell membranes. In its simplest form, this involves the membrane surrounding the cell itself. However, the membrane may be tightly folded into cylindrical sheets called thylakoids, or bunched up into round vesicles called intracytoplasmic membranes. These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb.\\nIn plants and algae, photosynthesis takes place in organelles called chloroplasts. A typical plant cell contains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral and peripheral membrane protein complexes of the photosynthetic system.\\nPlants absorb light primarily using the pigment chlorophyll. The green part of the light spectrum is not absorbed but is reflected, which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such as carotenes and xanthophylls. Algae also use chlorophyll, but various other pigments are present, such as phycocyanin, carotenes, and xanthophylls in green algae, phycoerythrin in red algae (rhodophytes) and fucoxanthin in brown algae and diatoms resulting in a wide variety of colors.\\nThese pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\\n\\nLight-dependent reactions\\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.\\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\\n\\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\\n\\nZ scheme\\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram \"Z-scheme\"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\\nThe cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.\\n\\nWater photolysis\\nLinear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok\\'s S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.\\n\\nLight-independent reactions\\nCalvin cycle\\nIn the light-independent (or \"dark\") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is\\n\\nCarbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.\\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus \"recycled\" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\\n\\nCarbon concentrating mechanisms\\nOn land\\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.\\nPlants that use the C4 carbon fixation process chemically fix carbon dioxide in the cells of the mesophyll by adding it to the three-carbon molecule phosphoenolpyruvate (PEP), a reaction catalyzed by an enzyme called PEP carboxylase, creating the four-carbon organic acid oxaloacetic acid. Oxaloacetic acid or malate synthesized by this process is then translocated to specialized bundle sheath cells where the enzyme RuBisCO and other Calvin cycle enzymes are located, and where CO2 released by decarboxylation of the four-carbon acids is then fixed by RuBisCO activity to the three-carbon 3-phosphoglyceric acids. The physical separation of RuBisCO from the oxygen-generating light reactions reduces photorespiration and increases CO2 fixation and, thus, the photosynthetic capacity of the leaf. C4 plants can produce more sugar than C3 plants in conditions of high light and temperature. Many important crop plants are C4 plants, including maize, sorghum, sugarcane, and millet. Plants that do not use PEP-carboxylase in carbon fixation are called C3 plants because the primary carboxylation reaction, catalyzed by RuBisCO, produces the three-carbon 3-phosphoglyceric acids directly in the Calvin-Benson cycle. Over 90% of plants use C3 carbon fixation, compared to 3% that use C4 carbon fixation; however, the evolution of C4 in over sixty plant lineages makes it a striking example of convergent evolution. C2 photosynthesis, which involves carbon-concentration by selective breakdown of photorespiratory glycine, is both an evolutionary precursor to C4 and a useful carbon-concentrating mechanism in its own right.\\nXerophytes, such as cacti and most succulents, also use PEP carboxylase to capture carbon dioxide in a process called Crassulacean acid metabolism (CAM). In contrast to C4 metabolism, which spatially separates the CO2 fixation to PEP from the Calvin cycle, CAM temporally separates these two processes. CAM plants have a different leaf anatomy from C3 plants, and fix the CO2 at night, when their stomata are open. CAM plants store the CO2 mostly in the form of malic acid via carboxylation of phosphoenolpyruvate to oxaloacetate, which is then reduced to malate. Decarboxylation of malate during the day releases CO2 inside the leaves, thus allowing carbon fixation to 3-phosphoglycerate by RuBisCO. CAM is used by 16,000 species of plants.\\nCalcium-oxalate-accumulating plants, such as Amaranthus hybridus and Colobanthus quitensis, show a variation of photosynthesis where calcium oxalate crystals function as dynamic carbon pools, supplying carbon dioxide (CO2) to photosynthetic cells when stomata are partially or totally closed. This process was named alarm photosynthesis. Under stress conditions (e.g., water deficit), oxalate released from calcium oxalate crystals is converted to CO2 by an oxalate oxidase enzyme, and the produced CO2 can support the Calvin cycle reactions. Reactive hydrogen peroxide (H2O2), the byproduct of oxalate oxidase reaction, can be neutralized by catalase. Alarm photosynthesis represents a photosynthetic variant to be added to the well-known C4 and CAM pathways. However, alarm photosynthesis, in contrast to these pathways, operates as a biochemical pump that collects carbon from the organ interior (or from the soil) and not from the atmosphere.\\n\\nIn water\\nCyanobacteria possess carboxysomes, which increase the concentration of CO2 around RuBisCO to increase the rate of photosynthesis. An enzyme, carbonic anhydrase, located within the carboxysome, releases CO2 from dissolved hydrocarbonate ions (HCO−3). Before the CO2 can diffuse out, RuBisCO concentrated within the carboxysome quickly sponges it up. HCO−3 ions are made from CO2 outside the cell by another carbonic anhydrase and are actively pumped into the cell by a membrane protein. They cannot cross the membrane as they are charged, and within the cytosol they turn back into CO2 very slowly without the help of carbonic anhydrase. This causes the HCO−3 ions to accumulate within the cell from where they diffuse into the carboxysomes. Pyrenoids in algae and hornworts also act to concentrate CO2 around RuBisCO.\\n\\nOrder and kinetics\\nThe overall process of photosynthesis takes place in four stages:\\n\\nEfficiency\\nPlants usually convert light into chemical energy with a photosynthetic efficiency of 3–6%. \\nAbsorbed light that is unconverted is dissipated primarily as heat, with a small fraction (1–2%) reemitted as chlorophyll fluorescence at longer (redder) wavelengths. This fact allows measurement of the light reaction of photosynthesis by using chlorophyll fluorometers.\\nActual plants\\' photosynthetic efficiency varies with the frequency of the light being converted, light intensity, temperature, and proportion of carbon dioxide in the atmosphere, and can vary from 0.1% to 8%. By comparison, solar panels convert light into electric energy at an efficiency of approximately 6–20% for mass-produced panels, and above 40% in laboratory devices.\\nScientists are studying photosynthesis in hopes of developing plants with increased yield.\\nThe efficiency of both light and dark reactions can be measured, but the relationship between the two can be complex. For example, the light reaction creates ATP and NADPH energy molecules, which C3 plants can use for carbon fixation or photorespiration. Electrons may also flow to other electron sinks. For this reason, it is not uncommon for authors to differentiate between work done under non-photorespiratory conditions and under photorespiratory conditions.\\nChlorophyll fluorescence of photosystem II can measure the light reaction, and infrared gas analyzers can measure the dark reaction. An integrated chlorophyll fluorometer and gas exchange system can investigate both light and dark reactions when researchers use the two separate systems together. Infrared gas analyzers and some moisture sensors are sensitive enough to measure the photosynthetic assimilation of CO2 and of ΔH2O using reliable methods. CO2 is commonly measured in μmols/(m2/s), parts per million, or volume per million; and H2O is commonly measured in mmols/(m2/s) or in mbars. By measuring CO2 assimilation, ΔH2O, leaf temperature, barometric pressure, leaf area, and photosynthetically active radiation (PAR), it becomes possible to estimate, \"A\" or carbon assimilation, \"E\" or transpiration, \"gs\" or stomatal conductance, and \"Ci\" or intracellular CO2. However, it is more common to use chlorophyll fluorescence for plant stress measurement, where appropriate, because the most commonly used parameters FV/FM and Y(II) or F/FM\\' can be measured in a few seconds, allowing the investigation of larger plant populations.\\nGas exchange systems that offer control of CO2 levels, above and below ambient, allow the common practice of measurement of A/Ci curves, at different CO2 levels, to characterize a plant\\'s photosynthetic response.\\nIntegrated chlorophyll fluorometer – gas exchange systems allow a more precise measure of photosynthetic response and mechanisms. While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO2 levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of CC, the estimation of CO2 concentration at the site of carboxylation in the chloroplast, to replace Ci. CO2 concentration in the chloroplast becomes possible to estimate with the measurement of mesophyll conductance or gm using an integrated system.\\nPhotosynthesis measurement systems are not designed to directly measure the amount of light the leaf absorbs, but analysis of chlorophyll fluorescence, P700- and P515-absorbance, and gas exchange measurements reveal detailed information about, e.g., the photosystems, quantum efficiency and the CO2 assimilation rates. With some instruments, even wavelength dependency of the photosynthetic efficiency can be analyzed.\\nA phenomenon known as quantum walk increases the efficiency of the energy transport of light significantly. In the photosynthetic cell of an alga, bacterium, or plant, there are light-sensitive molecules called chromophores arranged in an antenna-shaped structure called a photocomplex. When a photon is absorbed by a chromophore, it is converted into a quasiparticle referred to as an exciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form accessible to the cell\\'s metabolism. The exciton\\'s wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously \"choose\" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time.\\nBecause that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances. Obstacles in the form of destructive interference cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic \"hop\". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks.\\n\\nEvolution\\nFossils of what are thought to be filamentous photosynthetic organisms have been dated at 3.4 billion years old. More recent studies also suggest that photosynthesis may have begun about 3.4 billion years ago, though the first direct evidence of photosynthesis comes from thylakoid membranes preserved in 1.75-billion-year-old cherts.\\nOxygenic photosynthesis is the main source of oxygen in the Earth\\'s atmosphere, and its earliest appearance is sometimes referred to as the oxygen catastrophe. Geological evidence suggests that oxygenic photosynthesis, such as that in cyanobacteria, became important during the Paleoproterozoic era around two billion years ago. Modern photosynthesis in plants and most photosynthetic prokaryotes is oxygenic, using water as an electron donor, which is oxidized to molecular oxygen in the photosynthetic reaction center.\\n\\nSymbiosis and the origin of chloroplasts\\nSeveral groups of animals have formed symbiotic relationships with photosynthetic algae. These are most common in corals, sponges, and sea anemones. Scientists presume that this is due to the particularly simple body plans and large surface areas of these animals compared to their volumes. In addition, a few marine mollusks, such as Elysia viridis and Elysia chlorotica, also maintain a symbiotic relationship with chloroplasts they capture from the algae in their diet and then store in their bodies (see Kleptoplasty). This allows the mollusks to survive solely by photosynthesis for several months at a time. Some of the genes from the plant cell nucleus have even been transferred to the slugs, so that the chloroplasts can be supplied with proteins they need to survive.\\nAn even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosynthetic bacteria, including a circular chromosome, prokaryotic-type ribosome, and similar proteins in the photosynthetic reaction center. The endosymbiotic theory suggests that photosynthetic bacteria were acquired (by endocytosis) by early eukaryotic cells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Like mitochondria, chloroplasts possess their own DNA, separate from the nuclear DNA of their plant host cells and the genes in this chloroplast DNA resemble those found in cyanobacteria. DNA in chloroplasts codes for redox proteins such as those found in the photosynthetic reaction centers. The CoRR Hypothesis proposes that this co-location of genes with their gene products is required for redox regulation of gene expression, and accounts for the persistence of DNA in bioenergetic organelles.\\n\\nPhotosynthetic eukaryotic lineages\\nSymbiotic and kleptoplastic organisms excluded:\\n\\nThe glaucophytes and the red and green algae—clade Archaeplastida (uni- and multicellular)\\nThe cryptophytes—clade Cryptista (unicellular)\\nThe haptophytes—clade Haptista (unicellular)\\nThe dinoflagellates and chromerids in the superphylum Myzozoa, and Pseudoblepharisma in the phylum Ciliophora—clade Alveolata (unicellular)\\nThe ochrophytes—clade Stramenopila (uni- and multicellular)\\nThe chlorarachniophytes and three species of Paulinella in the phylum Cercozoa—clade Rhizaria (unicellular)\\nThe euglenids—clade Excavata (unicellular)\\nExcept for the euglenids, which are found within the Excavata, all of these belong to the Diaphoretickes. Archaeplastida and the photosynthetic Paulinella got their plastids, which are surrounded by two membranes, through primary endosymbiosis in two separate events, by engulfing a cyanobacterium. The plastids in all the other groups have either a red or green algal origin, and are referred to as the \"red lineages\" and the \"green lineages\". The only known exception is the ciliate Pseudoblepharisma tenue, which in addition to its plastids that originated from green algae also has a purple sulfur bacterium as symbiont. In dinoflagellates and euglenids the plastids are surrounded by three membranes, and in the remaining lines by four. A nucleomorph, remnants of the original algal nucleus located between the inner and outer membranes of the plastid, is present in the cryptophytes (from a red alga) and chlorarachniophytes (from a green alga).\\nSome dinoflagellates that lost their photosynthetic ability later regained it again through new endosymbiotic events with different algae.\\nWhile able to perform photosynthesis, many of these eukaryotic groups are mixotrophs and practice heterotrophy to various degrees.\\n\\nPhotosynthetic prokaryotic lineages\\nEarly photosynthetic systems, such as those in green and purple sulfur and green and purple nonsulfur bacteria, are thought to have been anoxygenic, and used various other molecules than water as electron donors. Green and purple sulfur bacteria are thought to have used hydrogen and sulfur as electron donors. Green nonsulfur bacteria used various amino and other organic acids as electron donors. Purple nonsulfur bacteria used a variety of nonspecific organic molecules. The use of these molecules is consistent with the geological evidence that Earth\\'s early atmosphere was highly reducing at that time.\\nWith a possible exception of Heimdallarchaeota, photosynthesis is not found in archaea. Haloarchaea are photoheterotrophic; they can absorb energy from the sun, but do not harvest carbon from the atmosphere and are therefore not photosynthetic. Instead of chlorophyll they use rhodopsins, which convert light-energy to ion gradients but cannot mediate electron transfer reactions.\\nIn bacteria eight photosynthetic lineages are currently known:\\n\\nCyanobacteria, the only prokaryotes performing oxygenic photosynthesis and the only prokaryotes that contain two types of photosystems (type I (RCI), also known as Fe-S type, and type II (RCII), also known as quinone type). The seven remaining prokaryotes have anoxygenic photosynthesis and use versions of either type I or type II.\\nChlorobi (green sulfur bacteria) Type I\\nHeliobacteria Type I\\nChloracidobacterium Type I\\nProteobacteria (purple sulfur bacteria and purple non-sulfur bacteria) Type II (see: Purple bacteria)\\nChloroflexota (green non-sulfur bacteria) Type II\\nGemmatimonadota Type II\\nEremiobacterota Type II\\n\\nCyanobacteria and the evolution of photosynthesis\\nThe biochemical capacity to use water as the source for electrons in photosynthesis evolved once, in a common ancestor of extant cyanobacteria (formerly called blue-green algae). The geological record indicates that this transforming event took place early in Earth\\'s history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier. Because the Earth\\'s atmosphere contained almost no oxygen during the estimated development of photosynthesis, it is believed that the first photosynthetic cyanobacteria did not generate oxygen. Available evidence from geobiological studies of Archean (>2500 Ma) sedimentary rocks indicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterial evolution opened about 2000 Ma, revealing an already-diverse biota of cyanobacteria. Cyanobacteria remained the principal primary producers of oxygen throughout the Proterozoic Eon (2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable of nitrogen fixation. Green algae joined cyanobacteria as the major primary producers of oxygen on continental shelves near the end of the Proterozoic, but only with the Mesozoic (251–66 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did the primary production of oxygen in marine shelf waters take modern form. Cyanobacteria remain critical to marine ecosystems as primary producers of oxygen in oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as the plastids of marine algae.\\n\\nExperimental history\\nDiscovery\\nAlthough some of the steps in photosynthesis are still not completely understood, the overall photosynthetic equation has been known since the 19th century.\\n\\nJan van Helmont began the research of the process in the mid-17th century when he carefully measured the mass of the soil a plant was using and the mass of the plant as it grew. After noticing that the soil mass changed very little, he hypothesized that the mass of the growing plant must come from the water, the only substance he added to the potted plant. His hypothesis was partially accurate – much of the gained mass comes from carbon dioxide as well as water. However, this was a signaling point to the idea that the bulk of a plant\\'s biomass comes from the inputs of photosynthesis, not the soil itself.\\nJoseph Priestley, a chemist and minister, discovered that when he isolated a volume of air under an inverted jar and burned a candle in it (which gave off CO2), the candle would burn out very quickly, much before it ran out of wax. He further discovered that a mouse could similarly \"injure\" air. He then showed that a plant could restore the air the candle and the mouse had \"injured.\"\\nIn 1779, Jan Ingenhousz repeated Priestley\\'s experiments. He discovered that it was the influence of sunlight on the plant that could cause it to revive a mouse in a matter of hours.\\nIn 1796, Jean Senebier, a Swiss pastor, botanist, and naturalist, demonstrated that green plants consume carbon dioxide and release oxygen under the influence of light. Soon afterward, Nicolas-Théodore de Saussure showed that the increase in mass of the plant as it grows could not be due only to uptake of CO2 but also to the incorporation of water. Thus, the basic reaction by which organisms use photosynthesis to produce food (such as glucose) was outlined.\\n\\nRefinements\\nCornelis Van Niel made key discoveries explaining the chemistry of photosynthesis. By studying purple sulfur bacteria and green bacteria, he was the first to demonstrate that photosynthesis is a light-dependent redox reaction in which hydrogen reduces (donates its atoms as electrons and protons to) carbon dioxide.\\nRobert Emerson discovered two light reactions by testing plant productivity using different wavelengths of light. With the red alone, the light reactions were suppressed. When blue and red were combined, the output was much more substantial. Thus, there were two photosystems, one absorbing up to 600 nm wavelengths, the other up to 700 nm. The former is known as PSII, the latter is PSI. PSI contains only chlorophyll \"a\", PSII contains primarily chlorophyll \"a\" with most of the available chlorophyll \"b\", among other pigments. These include phycobilins, which are the red and blue pigments of red and blue algae, respectively, and fucoxanthol for brown algae and diatoms. The process is most productive when the absorption of quanta is equal in both PSII and PSI, assuring that input energy from the antenna complex is divided between the PSI and PSII systems, which in turn powers the photochemistry.\\nRobert Hill thought that a complex of reactions consisted of an intermediate to cytochrome b6 (now a plastoquinone), and that another was from cytochrome f to a step in the carbohydrate-generating mechanisms. These are linked by plastoquinone, which does require energy to reduce cytochrome f. Further experiments to prove that the oxygen developed during the photosynthesis of green plants came from water were performed by Hill in 1937 and 1939. He showed that isolated chloroplasts give off oxygen in the presence of unnatural reducing agents like iron oxalate, ferricyanide or benzoquinone after exposure to light. In the Hill reaction:\\n\\n2 H2O + 2 A + (light, chloroplasts) → 2 AH2 + O2\\nA is the electron acceptor. Therefore, in light, the electron acceptor is reduced and oxygen is evolved. Samuel Ruben and Martin Kamen used radioactive isotopes to determine that the oxygen liberated in photosynthesis came from the water.\\n\\nMelvin Calvin and Andrew Benson, along with James Bassham, elucidated the path of carbon assimilation (the photosynthetic carbon reduction cycle) in plants. The carbon reduction cycle is known as the Calvin cycle, but many scientists refer to it as the Calvin-Benson, Benson-Calvin, or even Calvin-Benson-Bassham (or CBB) Cycle.\\nNobel Prize–winning scientist Rudolph A. Marcus was later able to discover the function and significance of the electron transport chain.\\nOtto Heinrich Warburg and Dean Burk discovered the I-quantum photosynthesis reaction that splits CO2, activated by the respiration.\\nIn 1950, first experimental evidence for the existence of photophosphorylation in vivo was presented by Otto Kandler using intact Chlorella cells and interpreting his findings as light-dependent ATP formation.\\nIn 1954, Daniel I. Arnon et al. discovered photophosphorylation in vitro in isolated chloroplasts with the help of P32.\\nLouis N. M. Duysens and Jan Amesz discovered that chlorophyll \"a\" will absorb one light, oxidize cytochrome f, while chlorophyll \"a\" (and other pigments) will absorb another light but will reduce this same oxidized cytochrome, stating the two light reactions are in series.\\n\\nDevelopment of the concept\\nIn 1893, the American botanist Charles Reid Barnes proposed two terms, photosyntax and photosynthesis, for the biological process of synthesis of complex carbon compounds out of carbonic acid, in the presence of chlorophyll, under the influence of light. The term photosynthesis is derived from the Greek phōs (φῶς, gleam) and sýnthesis (σύνθεσις, arranging together), while another word that he designated was photosyntax, from sýntaxis (σύνταξις, configuration). Over time, the term photosynthesis came into common usage. Later discovery of anoxygenic photosynthetic bacteria and photophosphorylation necessitated redefinition of the term.\\n\\nC3 : C4 photosynthesis research\\nIn the late 1940s at the University of California, Berkeley, the details of photosynthetic carbon metabolism were sorted out by the chemists Melvin Calvin, Andrew Benson, James Bassham and a score of students and researchers utilizing the carbon-14 isotope and paper chromatography techniques. The pathway of CO2 fixation by the algae Chlorella in a fraction of a second in light resulted in a three carbon molecule called phosphoglyceric acid (PGA). For that original and ground-breaking work, a Nobel Prize in Chemistry was awarded to Melvin Calvin in 1961. In parallel, plant physiologists studied leaf gas exchanges using the new method of infrared gas analysis and a leaf chamber where the net photosynthetic rates ranged from 10 to 13 μmol CO2·m−2·s−1, with the conclusion that all terrestrial plants have the same photosynthetic capacities, that are light saturated at less than 50% of sunlight.\\nLater in 1958–1963 at Cornell University, field grown maize was reported to have much greater leaf photosynthetic rates of 40 μmol CO2·m−2·s−1 and not be saturated at near full sunlight. This higher rate in maize was almost double of those observed in other species such as wheat and soybean, indicating that large differences in photosynthesis exist among higher plants. At the University of Arizona, detailed gas exchange research on more than 15 species of monocots and dicots uncovered for the first time that differences in leaf anatomy are crucial factors in differentiating photosynthetic capacities among species. In tropical grasses, including maize, sorghum, sugarcane, Bermuda grass and in the dicot amaranthus, leaf photosynthetic rates were around 38−40 μmol CO2·m−2·s−1, and the leaves have two types of green cells, i.e. outer layer of mesophyll cells surrounding a tightly packed cholorophyllous vascular bundle sheath cells. This type of anatomy was termed Kranz anatomy in the 19th century by the botanist Gottlieb Haberlandt while studying leaf anatomy of sugarcane. Plant species with the greatest photosynthetic rates and Kranz anatomy showed no apparent photorespiration, very low CO2 compensation point, high optimum temperature, high stomatal resistances and lower mesophyll resistances for gas diffusion and rates never saturated at full sun light. The research at Arizona was designated a Citation Classic in 1986. These species were later termed C4 plants as the first stable compound of CO2 fixation in light has four carbons as malate and aspartate. Other species that lack Kranz anatomy were termed C3 type such as cotton and sunflower, as the first stable carbon compound is the three-carbon PGA. At 1000 ppm CO2 in measuring air, both the C3 and C4 plants had similar leaf photosynthetic rates around 60 μmol CO2·m−2·s−1 indicating the suppression of photorespiration in C3 plants.\\n\\nFactors\\nThere are four main factors influencing photosynthesis and several corollary factors. The four main are:\\n\\nLight irradiance and wavelength\\nWater absorption\\nCarbon dioxide concentration\\nTemperature.\\nTotal photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), the rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.\\n\\nLight intensity (irradiance), wavelength and temperature\\nThe process of photosynthesis provides the main input of free energy into the biosphere, and is one of four main ways in which radiation is important for plant life.\\nThe radiation climate within plant communities is extremely variable, in both time and space.\\nIn the early 20th century, Frederick Blackman and Gabrielle Matthaei investigated the effects of light intensity (irradiance) and temperature on the rate of carbon assimilation.\\n\\nAt constant temperature, the rate of carbon assimilation varies with irradiance, increasing as the irradiance increases, but reaching a plateau at higher irradiance.\\nAt low irradiance, increasing the temperature has little influence on the rate of carbon assimilation. At constant high irradiance, the rate of carbon assimilation increases as the temperature is increased.\\nThese two experiments illustrate several important points: First, it is known that, in general, photochemical reactions are not affected by temperature. However, these experiments clearly show that temperature affects the rate of carbon assimilation, so there must be two sets of reactions in the full process of carbon assimilation. These are the light-dependent \\'photochemical\\' temperature-independent stage, and the light-independent, temperature-dependent stage. Second, Blackman\\'s experiments illustrate the concept of limiting factors. Another limiting factor is the wavelength of light. Cyanobacteria, which reside several meters underwater, cannot receive the correct wavelengths required to cause photoinduced charge separation in conventional photosynthetic pigments. To combat this problem, Cyanobacteria have a light-harvesting complex called Phycobilisome. This complex is made up of a series of proteins with different pigments which surround the reaction center.\\n\\nCarbon dioxide levels and photorespiration\\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\\n\\nOne product of oxygenase activity is phosphoglycolate (2 carbon) instead of 3-phosphoglycerate (3 carbon). Phosphoglycolate cannot be metabolized by the Calvin-Benson cycle and represents carbon lost from the cycle. A high oxygenase activity, therefore, drains the sugars that are required to recycle ribulose 5-bisphosphate and for the continuation of the Calvin-Benson cycle.\\nPhosphoglycolate is quickly metabolized to glycolate that is toxic to a plant at a high concentration; it inhibits photosynthesis.\\nSalvaging glycolate is an energetically expensive process that uses the glycolate pathway, and only 75% of the carbon is returned to the Calvin-Benson cycle as 3-phosphoglycerate. The reactions also produce ammonia (NH3), which is able to diffuse out of the plant, leading to a loss of nitrogen.\\nA highly simplified summary is:\\n2 glycolate + ATP → 3-phosphoglycerate + carbon dioxide + ADP + NH3\\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\\n\\nSee also\\nReferences\\nFurther reading\\nBooks\\nPapers\\nExternal links\\nA collection of photosynthesis pages for all levels from a renowned expert (Govindjee)\\nIn depth, advanced treatment of photosynthesis, also from Govindjee\\nScience Aid: Photosynthesis Article appropriate for high school science\\nMetabolism, Cellular Respiration and Photosynthesis – The Virtual Library of Biochemistry and Cell Biology\\nOverall examination of Photosynthesis at an intermediate level\\nOverall Energetics of Photosynthesis\\nThe source of oxygen produced by photosynthesis Interactive animation, a textbook tutorial\\nMarshall J (2011-03-29). \"First practical artificial leaf makes debut\". Discovery News. Archived from the original on 2012-03-22. Retrieved 2011-03-29.\\nPhotosynthesis – Light Dependent & Light Independent Stages Archived 2011-09-10 at the Wayback Machine\\nKhan Academy, video introduction', 'Evolution is the change in the heritable characteristics of biological populations over successive generations. It occurs when evolutionary processes such as natural selection and genetic drift act on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations. The process of evolution has given rise to biodiversity at every level of biological organisation.\\nThe scientific theory of evolution by natural selection was conceived independently by two British naturalists, Charles Darwin and Alfred Russel Wallace, in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin\\'s book On the Origin of Species. Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2) traits vary among individuals with respect to their morphology, physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics for that environment.\\nIn the early 20th century, competing ideas of evolution were refuted and evolution was combined with Mendelian inheritance and population genetics to give rise to modern evolutionary theory. In this synthesis the basis for heredity is in DNA molecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift, mutation, and gene flow.\\nAll life on Earth—including humanity—shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits tend to be more similar among species that share a more recent common ancestor, which historically was used to reconstruct phylogenetic trees, although direct comparison of genetic sequences is a more common method today.\\nEvolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but also other fields including agriculture, medicine, and computer science.\\n\\nHeredity\\nEvolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism\\'s genome (genetic material) is called its genotype.\\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. Some of these traits come from the interaction of its genotype with the environment while others are neutral. Some observable characteristics are not inherited. For example, suntanned skin comes from the interaction between a person\\'s genotype and sunlight; thus, suntans are not passed on to people\\'s children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\\nHeritable characteristics are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called a chromosome. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in a quantitative or epistatic manner.\\n\\nSources of variation\\nEvolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species. However, discoveries in the field of evolutionary developmental biology have demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.\\nAn individual organism\\'s phenotype results from both its genotype and the influence of the environment it has lived in. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.\\n\\nMutation\\nMutations are changes in the DNA sequence of a cell\\'s genome and are the ultimate source of genetic variation in all organisms. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect.\\nAbout half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit. Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.\\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.\\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termed de novo gene birth.\\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling). When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.\\nOne example of mutation is wild boar piglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in the melanocortin 1 receptor (MC1R) disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring.\\n\\nSex and recombination\\nIn asexual organisms, genes are inherited together, or linked, as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents\\' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.\\n\\nThe two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and many invertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment. Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial.\\n\\nGene flow\\nGene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.\\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast Saccharomyces cerevisiae and the adzuki bean weevil Callosobruchus chinensis has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.\\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.\\n\\nEpigenetics\\nSome heritable changes cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.\\n\\nEvolutionary forces\\nFrom a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms, for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias.\\n\\nNatural selection\\nEvolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:\\n\\nVariation exists within populations of organisms with respect to morphology, physiology and behaviour (phenotypic variation).\\nDifferent traits confer different rates of survival and reproduction (differential fitness).\\nThese traits can be passed from generation to generation (heritability of fitness).\\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Consequences of selection include nonrandom mating and genetic hitchhiking.\\nThe central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism\\'s ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism\\'s genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.\\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be selected for. Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are selected against.\\nImportantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form. However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.\\n\\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.\\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system....\" Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation.\\n\\nGenetic drift\\nGenetic drift is the random fluctuation of allele frequencies within a population from one generation to the next. When selective forces are absent or relatively weak, allele frequencies are equally likely to drift upward or downward in each successive generation because the alleles are subject to sampling error. This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.\\nAccording to the neutral theory of molecular evolution most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift. This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature. A better-supported version of this model is the nearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft. Another concept is constructive neutral evolution (CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting, and it has been applied in areas ranging from the origins of the spliceosome to the complex interdependence of microbial communities.\\nThe time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.\\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.\\n\\nMutation bias\\nMutation bias is usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea of developmental bias. J. B. S. Haldane and Ronald Fisher argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution, until the molecular era prompted renewed interest in neutral evolution.\\nNoboru Sueoka and Ernst Freese proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biased E. coli mutator strain in 1967, along with the proposal of the neutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.\\nFor instance, mutation biases are frequently invoked in models of codon usage. Such models also include effects of selection, following the mutation-selection-drift model, which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores. Different insertion vs. deletion biases in different taxa can lead to the evolution of different genome sizes. The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.\\nHowever, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals and (2) bacterial genomes frequently have AT-biased mutation.\\nContemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work showed that the original \"pressures\" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmental biases in the introduction of variation (arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.\\nSeveral studies report that the mutations implicated in adaptation reflect common mutation biases though others dispute this interpretation.\\n\\nGenetic hitchhiking\\nRecombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.\\n\\nSexual selection\\nA special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.\\n\\nNatural outcomes\\nEvolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction, whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation. Macroevolution is the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.\\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically, however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing, and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world\\'s biomass despite their small size and constitute the vast majority of Earth\\'s biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to evolutionary research since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.\\n\\nAdaptation\\nAdaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism\\'s survival. For example, the adaptation of horses\\' teeth to the grinding of grass. By using the term adaptation for the evolutionary process and adaptive trait for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:\\n\\nAdaptation is the evolutionary process whereby an organism becomes better able to live in its habitat or habitats.\\nAdaptedness is the state of being adapted: the degree to which an organism is able to live and reproduce in a given set of habitats.\\nAn adaptive trait is an aspect of the developmental pattern of the organism which enables or enhances the probability of that organism surviving and reproducing.\\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria Escherichia coli evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, Flavobacterium evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium Sphingobium evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms\\' evolvability).\\n\\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.\\nDuring evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.\\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard Holaspis guentheri, which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms\\' eyes.\\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.\\n\\nCoevolution\\nInteractions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.\\n\\nCooperation\\nNot all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.\\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal\\'s germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.\\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative\\'s offspring. This activity is selected for because if the helping individual contains alleles which promote the helping activity, it is likely that its kin will also contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.\\n\\nSpeciation\\nSpeciation is the process where a species diverges into two or more descendant species.\\nThere are multiple ways to define the concept of \"species\". The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\" Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction; this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.\\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the grey tree frog being a particularly well-studied example.\\nSpeciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.\\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.\\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species\\' habitat. One example is the grass Anthoxanthum odoratum, which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.\\n\\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.\\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent\\'s chromosomes are represented by a pair already. An example of such a speciation event is when the plant species Arabidopsis thaliana and Arabidopsis arenosa crossbred to give the new species Arabidopsis suecica. This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.\\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.\\n\\nExtinction\\nExtinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity\\'s expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future. Despite the estimated extinction of more than 99% of all species that ever lived on Earth, about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.\\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.\\n\\nApplications\\nConcepts and models used in evolutionary biology, such as natural selection, have many applications.\\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.\\nUnderstanding the changes that have occurred during an organism\\'s evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.\\nEvolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as to pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.\\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.\\n\\nEvolutionary history of life\\nOrigin of life\\nThe Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. Commenting on the Australian findings, Stephen Blair Hedges wrote: \"If life arose relatively quickly on Earth, then it could be common in the universe.\"  In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\\nMore than 99% of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth\\'s current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80% not yet described.\\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.\\n\\nCommon descent\\nAll organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.\\n\\nDue to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species. To solve this problem and others, some authors prefer to use the \"Coral of life\" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.\\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms\\' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.\\n\\nEvolution of life\\nProkaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.\\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until around 1.7 billion years ago, when multicellular organisms began to appear, with differentiated cells performing specialised functions. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.\\nApproximately 538.8 million years ago, a remarkable amount of biological diversity appeared over a span of around 10 million years in what is called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.\\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, Homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.\\n\\nHistory of evolutionary thought\\nClassical antiquity\\nThe proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork De rerum natura (lit.\\u2009\\'On the Nature of Things\\').\\n\\nMiddle Ages\\nIn contrast to these materialistic views, Aristotelianism had considered all natural things as actualisations of fixed natural possibilities, known as forms. This became part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.\\nA number of Arab Muslim scholars wrote about evolution, most notably Ibn Khaldun, who wrote the book Muqaddimah in 1377, in which he asserted that humans developed from \"the world of the monkeys\", in a process by which \"species become more numerous\".\\n\\nPre-Darwinian\\nThe \"New Science\" of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species\", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.\\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck\\'s \"transmutation\" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray\\'s ideas of benevolent design had been developed by William Paley into the Natural Theology or Evidences of the Existence and Attributes of the Deity (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.\\n\\nDarwinian revolution\\nThe crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin and Alfred Wallace in terms of variable populations. Darwin used the expression descent with modification rather than evolution. Partly influenced by An Essay on the Principle of Population (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin\\'s publication of his \"abstract\" as On the Origin of Species explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin\\'s concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin\\'s ideas to humans, using palaeontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.\\nOthniel C. Marsh, America\\'s first palaeontologist, was the first to provide solid fossil evidence to support Darwin\\'s theory of evolution by unearthing the ancestors of the modern horse. In 1877, Marsh delivered a very influential speech before the annual meeting of the American Association for the Advancement of Science, providing a demonstrative argument for evolution. For the first time, Marsh traced the evolution of vertebrates from fish all the way through humans. Sparing no detail, he listed a wealth of fossil examples of past life forms. The significance of this speech was immediately recognised by the scientific community, and it was printed in its entirety in several scientific journals.\\n\\nIn 1880, Marsh caught the attention of the scientific world with the publication of Odontornithes: a Monograph on Extinct Birds of North America, which included his discoveries of birds with teeth. These skeletons helped bridge the gap between dinosaurs and birds, and provided invaluable support for Darwin\\'s theory of evolution. Darwin wrote to Marsh saying, \"Your work on these old birds & on the many fossil animals of N. America has afforded the best support to the theory of evolution, which has appeared within the last 20 years\" (since Darwin\\'s publication of Origin of Species).\\n\\nPangenesis and heredity\\nThe mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel\\'s laws of inheritance eventually supplanted most of Darwin\\'s pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin\\'s pangenesis theory to Weismann\\'s germ/soma cell distinction and proposed that Darwin\\'s pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell\\'s structure. De Vries was also one of the researchers who made Mendel\\'s work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin\\'s theory, genetic mutations, and Mendelian inheritance was thus reconciled.\\n\\nThe \\'modern synthesis\\'\\nIn the 1920s and 1930s, the modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, through fossil transitions in palaeontology.\\n\\nFurther syntheses\\nSince then, further syntheses have extended evolution\\'s explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of the biological hierarchy from genes to populations.\\nThe publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved understanding of the relationship between genotype and phenotype. Advances were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution\", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.\\nOne extension, known as evolutionary developmental biology and informally called \"evo-devo\", emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development). Since the beginning of the 21st century, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability.\\n\\nSocial and cultural responses\\nIn the 19th century, particularly after the publication of On the Origin of Species in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.\\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of Vestiges of the Natural History of Creation in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 Epperson v. Arkansas decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 Kitzmiller v. Dover Area School District case. The debate over Darwin\\'s ideas did not generate significant controversy in China.\\n\\nSee also\\nDevolution (biology) – Notion that species can revert to primitive forms\\nChronospecies\\n\\nReferences\\nBibliography\\nFurther reading\\nExternal links\\n \\n\\nGeneral information\\n\"Evolution\" on In Our Time at the BBC\\n\"Evolution Resources from the National Academies\". Washington, D.C.: National Academy of Sciences. Retrieved 30 May 2011.\\n\"Understanding Evolution: your one-stop resource for information on evolution\". Berkeley, California: University of California, Berkeley. Retrieved 30 May 2011.\\n\"Evolution of Evolution – 150 Years of Darwin\\'s \\'On the Origin of Species\\'\". Arlington County, Virginia: National Science Foundation. Archived from the original on 19 May 2011. Retrieved 30 May 2011.\\n\"Human Evolution Timeline Interactive\". Smithsonian Institution, National Museum of Natural History. 28 January 2010. Retrieved 14 July 2018. Adobe Flash required.\\n\"History of Evolution in the United States\". Salon. Retrieved 2021-08-24.\\nVideo (1980; Cosmos animation; 8:01): \"Evolution\" – Carl Sagan on YouTube\\nExperiments\\nLenski, Richard E. \"Experimental Evolution\". East Lansing, Michigan: Michigan State University. Retrieved 31 July 2013.\\nChastain, Erick; Livnat, Adi; Papadimitriou, Christos; Vazirani, Umesh (22 July 2014). \"Algorithms, games, and evolution\". PNAS. 111 (29): 10620–10623. Bibcode:2014PNAS..11110620C. doi:10.1073/pnas.1406556111. ISSN 0027-8424. PMC 4115542. PMID 24979793.\\nOnline lectures\\n\"Evolution Matters Lecture Series\". Harvard Online Learning. Cambridge, Massachusetts: Harvard University. Archived from the original on 18 December 2017. Retrieved 15 July 2018.\\nStearns, Stephen C. \"EEB 122: Principles of Evolution, Ecology and Behavior\". Open Yale Courses. New Haven, Connecticut: Yale University. Archived from the original on 1 December 2017. Retrieved 14 July 2018.', 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.\\n\\nHistory\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\\nThe earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\\nModern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\\n\\nRelationships to other fields\\nArtificial intelligence\\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.\\n\\nData compression\\nData mining\\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\\nMachine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).\\n\\nGeneralization\\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\\n\\nStatistics\\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.\\n\\nStatistical physics\\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.\\n\\nTheory\\nA core objective of a learner is to generalise from its experience. Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error.\\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.\\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\\n\\nApproaches\\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\\n\\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that\\'s analogous to rewards, which it tries to maximise.\\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\\n\\nSupervised learning\\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\\nTypes of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person\\'s height based on factors like age and genetics or forecasting future temperatures based on historical data.\\nSimilarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\\n\\nUnsupervised learning\\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.\\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\\nA special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.\\n\\nSemi-supervised learning\\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\\n\\nReinforcement learning\\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\\n\\nDimensionality reduction\\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation.\\n\\nOther types\\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.\\n\\nSelf-learning\\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \\n\\nin situation s perform action a\\nreceive a consequence situation s\\'\\ncompute emotion of being in the consequence situation v(s\\')\\nupdate crossbar memory  w\\'(a,s) = w(a,s) + v(s\\')\\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.\\n\\nFeature learning\\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.\\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\\n\\nSparse dictionary learning\\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\\n\\nAnomaly detection\\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\\n\\nRobot learning\\nRobot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).\\n\\nAssociation rules\\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \\n  \\n    \\n      \\n        {\\n        \\n          o\\n          n\\n          i\\n          o\\n          n\\n          s\\n          ,\\n          p\\n          o\\n          t\\n          a\\n          t\\n          o\\n          e\\n          s\\n        \\n        }\\n        ⇒\\n        {\\n        \\n          b\\n          u\\n          r\\n          g\\n          e\\n          r\\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{\\\\mathrm {onions,potatoes} \\\\}\\\\Rightarrow \\\\{\\\\mathrm {burger} \\\\}}\\n  \\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\\n\\nModels\\nA machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model\\'s internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\\n\\nArtificial neural networks\\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\\n\\nDecision trees\\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item\\'s target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\\n\\nRandom forest regression\\nRandom forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting.  To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application.\\n\\nSupport-vector machines\\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\\n\\nRegression analysis\\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\\nMultivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.\\n\\nBayesian networks\\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\\n\\nGaussian processes\\nA Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\\nGaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.\\n\\nGenetic algorithms\\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\\n\\nBelief functions\\nThe theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster\\'s rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner\\'s decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\\n\\nRule-based models\\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns \\'rules\\' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.\\n\\nTraining models\\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\\n\\nFederated learning\\nFederated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users\\' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users\\' mobile phones without having to send individual searches back to Google.\\n\\nApplications\\nThere are many applications for machine learning, including:\\n\\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers\\' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone\\'s performance and thermal behaviour based on the user\\'s interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.\\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. \\nMachine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.\\n\\nLimitations\\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\\nThe \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual\\'s life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft\\'s Bing Chat chatbot has been reported to produce hostile and offensive response against its users.\\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\\n\\nExplainability\\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\\n\\nOverfitting\\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.\\n\\nOther limitations and vulnerabilities\\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.\\nResearchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.\\n\\nModel assessments\\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.\\n\\nEthics\\nBias\\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK\\'s Commission for Racial Equality found that St. George\\'s Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica\\'s predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\\nWhile responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning\\'s vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\\nLanguage models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.\\nIn an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm\\'s insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems.\\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here\\'s nothing artificial about AI. It\\'s inspired by people, it\\'s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\\n\\nFinancial incentives\\nThere are concerns among health care professionals that these systems might not be designed in the public\\'s interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm\\'s proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\\n\\nHardware\\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\\n\\nTensor Processing Units (TPUs)\\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google\\'s DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\\n\\nNeuromorphic computing\\nNeuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.\\n\\nPhysical neural networks\\nA physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.\\n\\nEmbedded machine learning\\nEmbedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.\\n\\nSoftware\\nSoftware suites containing a variety of machine learning algorithms include the following:\\n\\nFree and open-source software\\nProprietary software with free and open-source editions\\nKNIME\\nRapidMiner\\n\\nProprietary software\\nJournals\\nJournal of Machine Learning Research\\nMachine Learning\\nNature Machine Intelligence\\nNeural Computation\\nIEEE Transactions on Pattern Analysis and Machine Intelligence\\n\\nConferences\\nAAAI Conference on Artificial Intelligence\\nAssociation for Computational Linguistics (ACL)\\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\\nInternational Conference on Machine Learning (ICML)\\nInternational Conference on Learning Representations (ICLR)\\nInternational Conference on Intelligent Robots and Systems (IROS)\\nConference on Knowledge Discovery and Data Mining (KDD)\\nConference on Neural Information Processing Systems (NeurIPS)\\n\\nSee also\\nAutomated machine learning – Process of automating the application of machine learning\\nBig data – Extremely large or complex datasets\\nDeep learning — branch of ML concerned with artificial neural networks\\nDifferentiable programming – Programming paradigm\\nList of datasets for machine-learning research\\nM-theory (learning framework)\\nMachine unlearning\\nSolomonoff\\'s theory of inductive inference – Mathematical theory\\n\\nReferences\\nSources\\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\\n\\nFurther reading\\nExternal links\\nInternational Machine Learning Society\\nmloss is an academic database of open-source machine learning software.', 'Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\nGoals\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\nReasoning and problem-solving\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\nKnowledge representation\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\\n\\nPlanning and decision-making\\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\\nIn some problems, the agent\\'s preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\\n\\nLearning\\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\\n\\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\\n\\nNatural language processing\\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\\nEarly work, based on Noam Chomsky\\'s generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\\n\\nPerception\\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\\nThe field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.\\n\\nSocial intelligence\\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.\\n\\nGeneral intelligence\\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\\n\\nTechniques\\nAI research uses a wide variety of techniques to accomplish the goals above.\\n\\nSearch and optimization\\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\\n\\nState space search\\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\\n\\nLocal search\\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\nLogic\\nFormal logic is used for reasoning and knowledge representation.\\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\\n\\nProbabilistic methods for uncertain reasoning\\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\\n\\nClassifiers and statistical learning methods\\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\\nNeural networks are also used as classifiers.\\n\\nArtificial neural networks\\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\\nIn feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.\\n\\nDeep learning\\nDeep learning uses several layers of neurons between the network\\'s inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\\n\\nGPT\\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.\\nCurrent models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\\n\\nHardware and software\\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models\\' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore\\'s law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang\\'s law, named after Nvidia co-founder and CEO Jensen Huang.\\n\\nApplications\\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple\\'s FaceID or Microsoft\\'s DeepFace and Google\\'s FaceNet) and image labeling (used by Facebook, Apple\\'s Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO).\\n\\nHealth and medicine\\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson\\'s disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson\\'s disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\\n\\nGames\\nGame playing programs have been used since the 1950s to demonstrate and test AI\\'s most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind\\'s AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world\\'s best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\\n\\nMathematics\\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.\\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.   \\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\\nTopological deep learning integrates various topological approaches.\\n\\nFinance\\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I\\'m not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\\n\\nMilitary\\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.\\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.\\n\\nGenerative AI\\nAgents\\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\\n\\nSexuality\\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.\\n\\nOther industry-specific tasks\\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\\nAI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.\\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\\n\\nEthics\\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\\n\\nRisks and harm\\nPrivacy and copyright\\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI\\'s ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of \\'what they know\\' to the question of \\'what they\\'re doing with it\\'.\"\\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\\n\\nDominance by tech giants\\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\\n\\nPower needs and environmental impacts\\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers\\' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.\\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon\\'s spinoff of Constellation.\\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon\\'s data center. \\nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\\nIn 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.\\n\\nMisinformation\\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.\\nIn the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\\nAI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.\\n\\nAlgorithmic bias and fairness\\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\\nOn June 28, 2015, Google Photos\\'s new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn\\'t work.\"\\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\\n\\nLack of transparency\\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\\nPeople who have been harmed by an algorithm\\'s decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union\\'s General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model\\'s outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\\n\\nBad actors and weaponized AI\\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations\\' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\\n\\nTechnological unemployment\\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we\\'re in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\\n\\nExistential risk\\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can\\'t fetch the coffee if you\\'re dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity\\'s morality and values so that it is \"fundamentally on our side\".\\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it\\'s a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers\\' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\\n\\nEthical machines and alignment\\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\\nThe field of machine ethics is also called computational morality,\\nand was founded at an AAAI symposium in 2005.\\nOther approaches include Wendell Wallach\\'s \"artificial moral agents\" and Stuart J. Russell\\'s three principles for developing provably beneficial machines.\\n\\nOpen source\\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\\n\\nFrameworks\\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\\n\\nRespect the dignity of individual people\\nConnect with other people sincerely, openly, and inclusively\\nCare for the wellbeing of everyone\\nProtect social values, justice, and the public interest\\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE\\'s Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.\\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\\nThe UK AI Safety Institute released in 2024 a testing toolset called \\'Inspect\\' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\\n\\nRegulation\\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\\n\\nHistory\\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing\\'s theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing\\'s influential 1950 paper \\'Computing Machinery and Intelligence\\', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert\\'s book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan\\'s fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\\nUp to this point, most of AI\\'s funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\\nFor many specific tasks, other methods were abandoned.\\nDeep learning\\'s success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning\\'s success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\\n\\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game\\'s rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI\\'s breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.\\n\\nPhilosophy\\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\\n\\nDefining artificial intelligence\\nAlan Turing wrote in 1950 \"I propose to consider the question \\'can machines think\\'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\\n\\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making \\'machines that fly so exactly like pigeons that they can fool other pigeons.\\'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\\nThere has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.\\n\\nEvaluating approaches to AI\\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\\n\\nSymbolic AI and its limits\\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec\\'s paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\\n\\nNeat vs. scruffy\\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\\n\\nSoft vs. hard computing\\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\\n\\nNarrow vs. general AI\\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field\\'s long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\\n\\nMachine consciousness, sentience, and mind\\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\nConsciousness\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett\\'s consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\\n\\nComputationalism and functionalism\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.\\n\\nAI welfare and rights\\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.\\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\\n\\nFuture\\nSuperintelligence and the singularity\\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\\n\\nTranshumanism\\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\\n\\nIn fiction\\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\\nA common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov\\'s laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\nSee also\\nArtificial consciousness – Field in cognitive science\\nArtificial intelligence and elections – Use and impact of AI on political elections\\nArtificial intelligence content detection – Software to detect AI-generated content\\nAssociation for the Advancement of Artificial Intelligence (AAAI)\\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\\nBusiness process automation – Automation of business processes\\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\\nDigital immortality – Hypothetical concept of storing a personality in digital form\\nEmergent algorithm – Algorithm exhibiting emergent behavior\\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\\nIntelligence amplification – Use of information technology to augment human intelligence\\nIntelligent agent – Software agent which acts autonomously\\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\\nList of artificial intelligence journals\\nList of artificial intelligence projects\\nMind uploading – Hypothetical process of digitally emulating a brain\\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\\nRobotic process automation – Form of business process automation technology\\nThe Last Day – 1967 Welsh science fiction novel\\nWetware computer – Computer composed of organic material\\nDARWIN EU - A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real-world evidence (RWE) to support the evaluation and supervision of medicines across the EU.\\n\\nExplanatory notes\\nReferences\\nAI textbooks\\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\\n\\nRussell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.\\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.\\nThe four most widely used AI textbooks in 2008:\\n\\nOther textbooks:\\n\\nErtel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.\\nCiaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.\\n\\nHistory of AI\\nOther sources\\nFurther reading\\nExternal links\\n\\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.', 'Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). \\nAlgorithms and data structures are central to computer science.\\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\\n\\nHistory\\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage\\'s impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage\\'s Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage\\'s dream come true\".\\n\\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan\\'s West Side was IBM\\'s first laboratory devoted to pure science. The lab is the forerunner of IBM\\'s Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world\\'s first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\\n\\nEtymology and scope\\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline. This effort, and those of others such as numerical analyst George Forsythe, were successful, and universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\\nA folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.\\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.\\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\\n\\nPhilosophy\\nEpistemology of computer science\\nDespite the word science in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.\\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.\\n\\nParadigms of computer science\\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning\\'s working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\\n\\nFields\\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.\\nCSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\\n\\nTheoretical computer science\\nTheoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\\n\\nTheory of computation\\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\\n\\nInformation and coding theory\\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\\n\\nData structures and algorithms\\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\\n\\nProgramming language theory and formal methods\\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\\n\\nApplied computer science\\nComputer graphics and visualization\\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\\n\\nImage and sound processing\\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.\\n\\nComputational science, finance and engineering\\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, societies and social situations (notably war games) along with their habitats, and interactions among biological cells. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\\n\\nHuman–computer interaction\\nHuman–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.\\n\\nSoftware engineering\\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\\n\\nArtificial intelligence\\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing\\'s question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\\n\\nComputer systems\\nComputer architecture and microarchitecture\\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM\\'s main research center in 1959.\\n\\nConcurrent, parallel and distributed computing\\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\\n\\nComputer networks\\nThis branch of computer science aims studies the construction and behavior of computer networks. It addresses their performance, resilience, security, scalability, and cost-effectiveness, along with the variety of services they can provide.\\n\\nComputer security and cryptography\\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\\n\\nDatabases and data mining\\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\\n\\nDiscoveries\\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\\n\\nGottfried Wilhelm Leibniz\\'s, George Boole\\'s, Alan Turing\\'s, Claude Shannon\\'s, and Samuel Morse\\'s insight: there are only two objects that a computer has to deal with in order to represent \"anything\".\\nAll the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).\\n\\nAlan Turing\\'s insight: there are only five actions that a computer has to perform in order to do \"anything\".\\nEvery algorithm can be expressed in a language for a computer consisting of only five basic instructions:\\nmove left one location;\\nmove right one location;\\nread symbol at current location;\\nprint 0 at current location;\\nprint 1 at current location.\\n\\nCorrado Böhm and Giuseppe Jacopini\\'s insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".\\nOnly three rules are needed to combine any set of basic instructions into more complex ones:\\nsequence: first do this, then do that;\\n selection: IF such-and-such is the case, THEN do this, ELSE do that;\\nrepetition: WHILE such-and-such is the case, DO this.\\nThe three rules of Boehm\\'s and Jacopini\\'s insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\\n\\nProgramming paradigms\\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\\n\\nFunctional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\\nImperative programming, a programming paradigm that uses statements that change a program\\'s state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\\nObject-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object\\'s procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.\\nService-oriented programming, a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and mission critical software programs.\\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\\n\\nResearch\\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\\n\\nSee also\\nNotes\\nReferences\\nFurther reading\\nExternal links\\n\\nDBLP Computer Science Bibliography\\nAssociation for Computing Machinery\\nInstitute of Electrical and Electronics Engineers']\n"
     ]
    }
   ],
   "source": [
    "article_titles = [\n",
    "    \"Galaxy\", \"Black Hole\", \"Supernova\",\n",
    "    \"DNA\", \"Photosynthesis\", \"Evolution\",\n",
    "    \"Machine learning\", \"Artificial Intelligence\", \"Computer Science\"\n",
    "]\n",
    "\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProject/1.0', 'en')\n",
    "\n",
    "documents = []\n",
    "\n",
    "for title in article_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "    else:\n",
    "        print(\"Such Document Doesn't Exist\")\n",
    "    \n",
    "print(\"\\nFinal Document List:\", documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc867041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['galaxy system star stellar remnant interstellar gas dust dark matter bound together gravity word derived greek galaxias literally milky reference milky way galaxy contains solar system galaxy averaging estimated million star range size dwarf less thousand star largest galaxy known supergiant one hundred trillion star orbiting galaxy centre mass mass typical galaxy form dark matter per cent mass visible form star nebula supermassive black hole common feature centre galaxy galaxy categorised according visual morphology elliptical spiral irregular milky way example spiral galaxy estimated billion trillion galaxy observable universe galaxy parsec diameter approximately light year separated distance order million parsec megaparsecs comparison milky way diameter least parsec ly separated andromeda galaxy nearest large neighbour parsec million ly space galaxy filled tenuous gas intergalactic medium average density less one atom per cubic metre galaxy gravitationally organised group cluster superclusters milky way part local group dominates along andromeda galaxy group part virgo supercluster largest scale association generally arranged sheet filament surrounded immense void local group virgo supercluster contained much larger cosmic structure named laniakea etymology word galaxy borrowed via french medieval latin greek term milky way galaxas kklos milky circle named appearance milky band light sky astronomical literature capitalised word galaxy often used refer milky way galaxy distinguish galaxy universe galaxy initially discovered telescopically known spiral nebula th thcentury astronomer considered either unresolved star cluster extragalactic nebula true composition nature remained mystery observation using larger telescope nearby bright galaxy like andromeda galaxy began resolving huge conglomeration star based simply apparent faintness sheer population star true distance object placed well beyond milky way reason popularly called island universe harlow shapley began advocate term galaxy using universe nebula object influential edwin hubble stuck nebula nomenclature fully change hubble death nomenclature million galaxy catalogued wellestablished name andromeda galaxy magellanic cloud whirlpool galaxy sombrero galaxy astronomer work number certain catalogue messier catalogue ngc new general catalogue ic index catalogue cgcg catalogue galaxy cluster galaxy mcg morphological catalogue galaxy ugc uppsala general catalogue galaxy pgc catalogue principal galaxy also known leda wellknown galaxy appear one catalogue time different number example messier spiral galaxy number catalogue messier also designation ngc ugc cgcg mcg pgc leda among others million fainter galaxy known identifier sky survey sloan digital sky survey observation history milky way greek philosopher democritus bce proposed bright band night sky known milky way might consist distant star aristotle bce however believed milky way caused ignition fiery exhalation star large numerous close together ignition take place upper part atmosphere region world continuous heavenly motion neoplatonist philosopher olympiodorus younger c ce critical view arguing milky way sublunary situated earth moon appear different different time place earth parallax view milky way celestial according mohani mohamed arabian astronomer ibn alhaytham made first attempt observing measuring milky way parallax thus determined milky way parallax must remote earth belonging atmosphere persian astronomer albiruni proposed milky way galaxy collection countless fragment nature nebulous star andalusian astronomer avempace proposed composed many star almost touched one another appeared continuous image due effect refraction sublunary material citing observation conjunction jupiter mar evidence occurring two object near th century syrianborn ibn qayyim aljawziyya proposed milky way galaxy myriad tiny star packed together sphere fixed star actual proof milky way consisting many star came italian astronomer galileo galilei used telescope study discovered composed huge number faint star english astronomer thomas wright original theory new hypothesis universe correctly speculated might rotating body huge number star held together gravitational force akin solar system much larger scale resulting disk star could seen band sky perspective inside treatise immanuel kant elaborated wright idea milky way structure first project describe shape milky way position sun undertaken william herschel counting number star different region sky produced diagram shape galaxy solar system close center using refined approach kapteyn arrived picture small diameter kiloparsecs ellipsoid galaxy sun close center different method harlow shapley based cataloguing globular cluster led radically different picture flat disk diameter approximately kiloparsecs sun far centre analysis failed take account absorption light interstellar dust present galactic plane robert julius trumpler quantified effect studying open cluster present picture milky way galaxy emerged distinction nebula galaxy outside milky way visible dark night unaided eye including andromeda galaxy large magellanic cloud small magellanic cloud triangulum galaxy th century persian astronomer abd alrahman alsufi made earliest recorded identification andromeda galaxy describing small cloud probably mentioned large magellanic cloud book fixed star referring al bakr southern arab since declination south visible lived well known european magellan voyage th century andromeda galaxy later independently noted simon marius philosopher emanuel swedenborg principia speculated might galaxy outside formed galactic cluster minuscule part universe extended far beyond could seen swedenborg view remarkably close presentday view cosmos pierre louis maupertuis conjectured nebulalike object collection star unique property including glow exceeding light star produced repeated johannes heveliuss view bright spot massive flattened due rotation thomas wright correctly speculated milky way flattened disk star nebula visible night sky might separate milky way toward end th century charles messier compiled catalog containing brightest celestial object nebulous appearance subsequently william herschel assembled catalog nebula lord rosse examined nebula catalogued herschel observed spiral structure messier object known whirlpool galaxy vesto slipher made spectrographic study brightest spiral nebula determine composition slipher discovered spiral nebula high doppler shift indicating moving rate exceeding velocity star measured found majority nebula moving away u heber doust curtis observed nova andromedae within great andromeda nebula andromeda galaxy messier object known searching photographic record found nova curtis noticed nova average magnitude fainter occurred within galaxy result able come distance estimate parsec became proponent socalled island universe hypothesis hold spiral nebula actually independent galaxy debate took place harlow shapley heber curtis great debate concerning nature milky way spiral nebula dimension universe support claim great andromeda nebula external galaxy curtis noted appearance dark lane resembling dust cloud milky way well significant doppler shift estonian astronomer ernst pik gave distance determination supported theory andromeda nebula indeed distant extragalactic object using new inch mount wilson telescope edwin hubble able resolve outer part spiral nebula collection individual star identified cepheid variable thus allowing estimate distance nebula far distant part milky way hubble produced classification galactic morphology used day multiwavelength observation advance astronomy always driven technology century success optical astronomy recent decade seen major progress region electromagnetic spectrum dust present interstellar medium opaque visual light transparent farinfrared used observe interior region giant molecular cloud galactic core great detail infrared also used observe distant redshifted galaxy formed much earlier water vapor carbon dioxide absorb number useful portion infrared spectrum highaltitude spacebased telescope used infrared astronomy first nonvisual study galaxy particularly active galaxy made using radio frequency earth atmosphere nearly transparent radio mhz ghz ionosphere block signal range large radio interferometer used map active jet emitted active nucleus ultraviolet xray telescope observe highly energetic galactic phenomenon ultraviolet flare sometimes observed star distant galaxy torn apart tidal force nearby black hole distribution hot gas galactic cluster mapped xrays existence supermassive black hole core galaxy confirmed xray astronomy modern research hendrik van de hulst predicted microwave radiation wavelength cm would detectable interstellar atomic hydrogen gas observed radiation affected dust absorption doppler shift used map motion gas galaxy observation led hypothesis rotating bar structure center galaxy improved radio telescope hydrogen gas could also traced galaxy vera rubin uncovered discrepancy observed galactic rotation speed predicted visible mass star gas today galaxy rotation problem thought explained presence large quantity unseen dark matter beginning hubble space telescope yielded improved observation among thing data helped establish missing dark matter galaxy could consist solely inherently faint small star hubble deep field extremely long exposure relatively empty part sky provided evidence billion galaxy observable universe improved technology detecting spectrum invisible human radio telescope infrared camera xray telescope allows detection galaxy detected hubble particularly survey zone avoidance region sky blocked visiblelight wavelength milky way revealed number new galaxy study published astrophysical journal led christopher conselice university nottingham analyzed many source data estimate observable universe z contained least two trillion galaxy factor directly observed hubble image however later observation new horizon space probe outside zodiacal light observed less cosmic optical light conselice still suggesting direct observation missing galaxy type morphology galaxy come three main type ellipticals spiral irregular slightly extensive description galaxy type based appearance given hubble sequence since hubble sequence entirely based upon visual morphological type shape may miss certain important characteristic galaxy star formation rate starburst galaxy activity core active galaxy many galaxy thought contain supermassive black hole center includes milky way whose core region called galactic center ellipticals hubble classification system rate elliptical galaxy basis ellipticity ranging e nearly spherical e highly elongated galaxy ellipsoidal profile giving elliptical appearance regardless viewing angle appearance show little structure typically relatively little interstellar matter consequently galaxy also low portion open cluster reduced rate new star formation instead dominated generally older evolved star orbiting common center gravity random direction star contain low abundance heavy element star formation cease initial burst sense similarity much smaller globular cluster typecd galaxy largest galaxy typecd galaxy first described paper thomas matthew others subtype general class galaxy giant elliptical galaxy except much larger popularly known supergiant elliptical galaxy constitute largest luminous galaxy known galaxy feature central elliptical nucleus extensive faint halo star extending megaparsec scale profile surface brightness function radius distance core fall slowly smaller counterpart formation cd galaxy remains active area research leading model result merger smaller galaxy environment dense cluster even outside cluster random overdensities process mechanism drive formation fossil group fossil cluster large relatively isolated supergiant elliptical resides middle cluster surrounded extensive cloud xrays residue galactic collision another older model posit phenomenon cooling flow heated gas cluster collapse towards center cool forming star process phenomenon observed cluster perseus recently phoenix cluster shell galaxy shell galaxy type elliptical galaxy star halo arranged concentric shell onetenth elliptical galaxy shelllike structure never observed spiral galaxy structure thought develop larger galaxy absorbs smaller companion galaxythat two galaxy center approach start oscillate around center point oscillation creates gravitational ripple forming shell star similar ripple spreading water example galaxy ngc shell spiral spiral galaxy resemble spiraling pinwheel though star visible material contained galaxy lie mostly plane majority mass spiral galaxy exists roughly spherical halo dark matter extends beyond visible component demonstrated universal rotation curve concept spiral galaxy consist rotating disk star interstellar medium along central bulge generally older star extending outward bulge relatively bright arm hubble classification scheme spiral galaxy listed type followed letter b c indicates degree tightness spiral arm size central bulge sa galaxy tightly wound poorly defined arm possesses relatively large core region extreme sc galaxy open welldefined arm small core region galaxy poorly defined arm sometimes referred flocculent spiral galaxy contrast grand design spiral galaxy prominent welldefined spiral arm speed galaxy rotates thought correlate flatness disc spiral galaxy thick bulge others thin dense spiral galaxy spiral arm shape approximate logarithmic spiral pattern theoretically shown result disturbance uniformly rotating mass star like star spiral arm rotate around center constant angular velocity spiral arm thought area highdensity matter density wave star move arm space velocity stellar system modified gravitational force higher density velocity return normal star depart side arm effect akin wave slowdown moving along highway full moving car arm visible high density facilitates star formation therefore harbor many bright young star barred spiral galaxy majority spiral galaxy including milky way galaxy linear barshaped band star extends outward either side core merges spiral arm structure hubble classification scheme designated sb followed lowercase letter b c indicates form spiral arm manner categorization normal spiral galaxy bar thought temporary structure occur result density wave radiating outward core else due tidal interaction another galaxy many barred spiral galaxy active possibly result gas channeled core along arm galaxy milky way large diskshaped barredspiral galaxy kiloparsecs diameter kiloparsec thick contains two hundred billion star total mass six hundred billion time mass sun superluminous spiral recently researcher described galaxy called superluminous spiral large upward diameter lightyears compared milky way lightyear diameter mass billion solar mass generate significant amount ultraviolet midinfrared light thought increased star formation rate around time faster milky way morphology peculiar galaxy galactic formation develop unusual property due tidal interaction galaxy ring galaxy ringlike structure star interstellar medium surrounding bare core ring galaxy thought occur smaller galaxy pass core spiral galaxy event may affected andromeda galaxy display multiringlike structure viewed infrared radiation lenticular galaxy intermediate form property elliptical spiral galaxy categorized hubble type possess illdefined spiral arm elliptical halo star barred lenticular galaxy receive hubble classification sb irregular galaxy galaxy readily classified elliptical spiral morphology irri galaxy structure align cleanly hubble classification scheme irrii galaxy possess structure resembles hubble classification may disrupted nearby example dwarf irregular galaxy include magellanic cloud dark ultra diffuse galaxy extremelylowluminosity galaxy may size milky way visible star count one percent milky way multiple mechanism producing type galaxy proposed possible different dark galaxy formed different mean one candidate explanation low luminosity galaxy lost starforming gas early stage resulting old stellar population dwarf despite prominence large elliptical spiral galaxy galaxy dwarf galaxy relatively small compared galactic formation one hundredth size milky way billion star blue compact dwarf galaxy contains large cluster young hot massive star ultracompact dwarf galaxy discovered parsec across many dwarf galaxy may orbit single larger galaxy milky way least dozen satellite estimated yet discovered information dwarf galaxy come observation local group containing two spiral galaxy milky way andromeda many dwarf galaxy dwarf galaxy classified either irregular dwarf ellipticaldwarf spheroidal galaxy study milky way neighbor found dwarf galaxy central mass approximately million solar mass regardless whether thousand million star suggests galaxy largely formed dark matter minimum size may indicate form warm dark matter incapable gravitational coalescence smaller scale variant interacting interaction galaxy relatively frequent play important role galactic evolution near miss galaxy result warping distortion due tidal interaction may cause exchange gas dust collision occur two galaxy pas directly sufficient relative momentum merge star interacting galaxy usually collide gas dust within two form interacts sometimes triggering star formation collision severely distort galaxy shape forming bar ring taillike structure extreme interaction galactic merger galaxy relative momentum insufficient allow pas instead gradually merge form single larger galaxy merger result significant change galaxy original morphology one galaxy much massive result known cannibalism massive larger galaxy remains relatively undisturbed smaller one torn apart milky way galaxy currently process cannibalizing sagittarius dwarf elliptical galaxy canis major dwarf galaxy starburst star created within galaxy reserve cold gas form giant molecular cloud galaxy observed form star exceptional rate known starburst continue would consume reserve gas time span less galaxy lifespan hence starburst activity usually last ten million year relatively brief period galaxy history starburst galaxy common universe early history still contribute estimated total star production starburst galaxy characterized dusty concentration gas appearance newly formed star including massive star ionize surrounding cloud create h ii region star produce supernova explosion creating expanding remnant interact powerfully surrounding gas outburst trigger chain reaction starbuilding spread throughout gaseous region available gas nearly consumed dispersed activity end starbursts often associated merging interacting galaxy prototype example starburstforming interaction experienced close encounter larger irregular galaxy often exhibit spaced knot starburst activity radio galaxy radio galaxy galaxy giant region radio emission extending well beyond visible structure energetic radio lobe powered jet active galactic nucleus radio galaxy classified according fanaroffriley classification fr class lower radio luminosity exhibit structure elongated fr ii class higher radio luminosity correlation radio luminosity structure suggests source two type galaxy may differ radio galaxy also classified giant radio galaxy grgs whose radio emission extend scale megaparsecs million lightyears alcyoneus fr ii class lowexcitation radio galaxy largest observed radio emission lobed structure spanning megaparsecs ly comparison another similarly sized giant radio galaxy c lobe million lightyears across however noted radio emission always considered part main galaxy giant radio galaxy special class object characterized presence radio lobe generated relativistic jet powered central galaxy supermassive black hole giant radio galaxy different ordinary radio galaxy extend much larger scale reaching upwards several megaparsecs across far larger diameter host galaxy normal radio galaxy source supermassive black hole monster neutron star instead source synchrotron radiation relativistic electron accelerated supernova source comparatively short lived making radio spectrum normal radio galaxy especially good way study star formation active galaxy observable galaxy classified active contain active galactic nucleus agn significant portion galaxy total energy output emitted active nucleus instead star dust interstellar medium multiple classification naming scheme agns lower range luminosity called seyfert galaxy luminosity much greater host galaxy known quasistellar object quasar model agns suggest significant fraction light shifted farinfrared frequency optical uv emission nucleus absorbed remitted dust gas surrounding standard model active galactic nucleus based accretion disc form around supermassive black hole smbh galaxy core region radiation active galactic nucleus result gravitational energy matter fall toward black hole disc agns luminosity depends smbhs mass rate matter fall onto galaxy diametrically opposed pair energetic jet ejects particle galaxy core velocity close speed light mechanism producing jet well understood seyfert galaxy seyfert galaxy one two largest group active galaxy along quasar quasarlike nucleus luminous distant bright source electromagnetic radiation high surface brightness unlike quasar host galaxy clearly detectable seen telescope seyfert galaxy appears like ordinary galaxy bright star superimposed atop core seyfert galaxy divided two principal subtypes based frequency observed spectrum quasar quasar energetic distant member active galactic nucleus extremely luminous first identified high redshift source electromagnetic energy including radio wave visible light appeared similar star extended source similar galaxy luminosity time milky way nearest known quasar markarian million lightyears earth others discovered far away uhz roughly billion lightyears distant quasar noteworthy providing first demonstration phenomenon gravity act lens light agns blazars believed active galaxy relativistic jet pointed direction earth radio galaxy emits radio frequency relativistic jet unified model type active galaxy explains difference based observer position possibly related active galactic nucleus well starburst region lowionization nuclear emissionline region liner emission linertype galaxy dominated weakly ionized element excitation source weakly ionized line include postagb star agn shock approximately onethird nearby galaxy classified containing liner nucleus luminous infrared galaxy luminous infrared galaxy lirgs galaxy luminositiesthe measurement electromagnetic power outputabove l solar luminosity case energy come large number young star heat surrounding dust reradiates energy infrared luminosity high enough lirg requires star formation rate least yr ultraluminous infrared galaxy ulirgs least ten time luminous still form star rate yr many lirgs also emit radiation agn infrared galaxy emit energy infrared wavelength combined peak emission typically wavelength micron lirgs believed created strong interaction merger spiral galaxy uncommon local universe lirgs ulirgs prevalent universe younger physical diameter galaxy definite boundary nature characterized gradually decreasing stellar density function increasing distance center making measurement true extent difficult nevertheless astronomer past decade made several criterion defining size galaxy angular diameter early time edwin hubble attempt characterize diameter galaxy earliest effort based observed angle subtended galaxy estimated distance leading angular diameter also called metric diameter isophotal diameter isophotal diameter introduced conventional way measuring galaxy size based apparent surface brightness isophotes curve diagram picture galaxy adjoins point equal brightness useful defining extent galaxy apparent brightness flux galaxy measured unit magnitude per square arcsecond magarcsec sometimes expressed mag arcsec defines brightness depth isophote illustrate unit work typical galaxy brightness flux magarcsec central region brightness equivalent light th magnitude hypothetical point object like star spread evenly one square arcsecond area sky isophotal diameter typically defined region enclosing light magarcsec blue bband referred standard effective radius halflight variation halflight radius also known effective radius measure based galaxy overall brightness flux radius upon half total brightness flux galaxy emitted first proposed grard de vaucouleurs choice using arbitrary proved useful work r fish established luminosity concentration law relates brightness elliptical galaxy respective jos luis srsic defined massradius relation galaxy defining necessary overall brightness flux galaxy captured method employed bershady suggesting measure twice size brightness flux arbitrarily chosen radius defined local flux divided overall average flux equal using halflight radius allows rough estimate galaxy size particularly helpful determining morphology variation method exist particular esouppsala catalogue galaxy value total blue light light detected bband specific filter used calculate galaxy diameter petrosian magnitude first described vahe petrosian modified version method used sloan digital sky survey sdss method employ mathematical model galaxy whose radius determined azimuthally horizontal averaged profile brightness flux particular sdss employed petrosian magnitude rband nm red part visible spectrum ensure brightness flux galaxy would captured much possible counteracting effect background noise galaxy whose brightness profile exponential expected capture brightness flux galaxy follow profile follows de vaucouleurss law petrosian magnitude advantage redshift distance independent allowing measurement galaxy apparent size since petrosian radius defined term galaxy overall luminous flux critique earlier version method issued infrared processing analysis center method causing magnitude error upwards value using isophotal diameter use petrosian magnitude also disadvantage missing light outside petrosian aperture defined relative galaxy overall brightness profile especially elliptical galaxy higher signaltonoise ratio higher distance redshift correction method issued graham et al based assumption galaxy follow srsics law nearinfrared method method used mass adaptation previously used method isophotal measurement since mass operates near infrared advantage able recognize dimmer cooler older star different form approach compared method normally use bfilter detail method used mass described thoroughly document jarrett et al survey measuring several parameter standard aperture ellipse area detection defined infrared isophote k band roughly wavelength magarcsec gathering overall luminous flux galaxy employed least four method first circular aperture extending arcsecond center isophote magarcsec total aperture defined radial light distribution cover supposed extent galaxy kron aperture defined time firstmoment radius integration flux total aperture largerscale structure deepsky survey show galaxy often found group cluster solitary galaxy significantly interacted galaxy comparable mass past billion year relatively scarce galaxy surveyed isolated sense however may interacted even merged galaxy past may still orbited smaller satellite galaxy largest scale universe continually expanding resulting average increase separation individual galaxy see hubble law association galaxy overcome expansion local scale mutual gravitational attraction association formed early clump dark matter pulled respective galaxy together nearby group later merged form largerscale cluster ongoing merging process well influx infalling gas heat intergalactic gas cluster high temperature megakelvins cluster mass form dark matter consisting heated gas remaining percent form galaxy galaxy gravitationally bound number galaxy form fractallike hierarchical distribution clustered structure smallest association termed group group galaxy common type galactic cluster formation contain majority galaxy well baryonic mass universe remain gravitationally bound group member galaxy must sufficiently low velocity prevent escaping see virial theorem insufficient kinetic energy however group may evolve smaller number galaxy merger cluster galaxy consist hundred thousand galaxy bound together gravity cluster galaxy often dominated single giant elliptical galaxy known brightest cluster galaxy time tidally destroys satellite galaxy add mass superclusters contain ten thousand galaxy found cluster group sometimes individually supercluster scale galaxy arranged sheet filament surrounding vast empty void scale universe appears direction isotropic homogeneous though notion challenged recent year numerous finding largescale structure appear exceeding scale herculescorona borealis great wall currently largest structure universe found far billion lightyears three gigaparsecs length milky way galaxy member association named local group relatively small group galaxy diameter approximately one megaparsec milky way andromeda galaxy two brightest galaxy within group many member galaxy dwarf companion two local group part cloudlike structure within virgo supercluster large extended structure group cluster galaxy centered virgo cluster turn virgo supercluster portion laniakea supercluster magnetic field galaxy magnetic field galaxy magnetic field influence dynamic multiple way including affecting formation spiral arm transporting angular momentum gas cloud latter effect particularly important necessary factor gravitational collapse cloud thus star formation typical average equipartition strength spiral galaxy g microgauss nt nanotesla comparison earth magnetic field average strength g gauss microtesla radiofaint galaxy like milky way neighbor weaker field g gasrich galaxy high starformation rate like ngc g average prominent spiral arm field strength g region cold gas dust also concentrated strongest total equipartition field g found starburst galaxiesfor example antenna nuclear starburst region center ngc barred galaxy formation evolution formation current model formation galaxy early universe based cdm model year big bang atom hydrogen helium began form event called recombination nearly hydrogen neutral nonionized readily absorbed light star yet formed result period called dark age density fluctuation anisotropic irregularity primordial matter larger structure began appear result mass baryonic matter started condense within cold dark matter halo primordial structure allowed gas condense protogalaxies large scale gas cloud precursor first galaxy gas fall gravity dark matter halo pressure temperature rise condense gas must radiate energy process slow early universe dominated hydrogen atom molecule inefficient radiator compared heavier element clump gas aggregate forming rotating disk temperature pressure continue increase place within disk reach high enough density form star protogalaxies began form contract first halo star called population iii star appeared within composed primordial gas almost entirely hydrogen helium emission first star heat remaining gas helping trigger additional star formation ultraviolet light emission first generation star reionized surrounding neutral hydrogen expanding sphere eventually reaching entire universe event called reionization massive star collapse violent supernova explosion releasing heavy element metal interstellar medium metal content incorporated population ii star theoretical model early galaxy formation verified informed large number variety sophisticated astronomical observation photometric observation generally need spectroscopic confirmation due large number mechanism introduce systematic error example high redshift z photometric observation james webb space telescope jwst later corrected closer z nevertheless confirmed observation jwst observatory accumulating allowing systematic comparison early galaxy prediction theory evidence individual population iii star early galaxy even challenging even seemingly confirmed spectroscopic evidence may turn origin example astronomer reported heii emission evidence population iii star cosmos redshift galaxy redshift value subsequent observation found metallic emission line oiii inconsistent earlygalaxy star evolution star begin form emit radiation case explode process galaxy formation becomes complex involving interaction force gravity radiation thermal energy many detail still poorly understood within billion year galaxy formation key structure begin appear globular cluster central supermassive black hole galactic bulge metalpoor population ii star form creation supermassive black hole appears play key role actively regulating growth galaxy limiting total amount additional matter added early epoch galaxy undergo major burst star formation following two billion year accumulated matter settle galactic disc galaxy continue absorb infalling material highvelocity cloud dwarf galaxy throughout life matter mostly hydrogen helium cycle stellar birth death slowly increase abundance heavy element eventually allowing formation planet star formation rate galaxy depend upon local environment isolated void galaxy highest rate per stellar mass field galaxy associated spiral galaxy lower rate galaxy dense cluster lowest rate evolution galaxy significantly affected interaction collision merger galaxy common early epoch majority galaxy peculiar morphology given distance star great majority stellar system colliding galaxy unaffected however gravitational stripping interstellar gas dust make spiral arm produce long train star known tidal tail example formation seen ngc antenna galaxy milky way galaxy nearby andromeda galaxy moving toward km anddepending upon lateral movementsthe two might collide five six billion year although milky way never collided galaxy large andromeda collided merged galaxy past cosmological simulation indicate billion year ago merged particularly large galaxy labeled kraken largescale interaction rare time pass merger two system equal size become less common bright galaxy remained fundamentally unchanged last billion year net rate star formation probably also peaked ten billion year ago future trend spiral galaxy like milky way produce new generation star long dense molecular cloud interstellar hydrogen spiral arm elliptical galaxy largely devoid gas form new star supply starforming material finite star converted available supply hydrogen heavier element new star formation come end current era star formation expected continue one hundred billion year stellar age wind ten trillion one hundred trillion year year smallest longestlived star visible universe tiny red dwarf begin fade end stellar age galaxy composed compact object brown dwarf white dwarf cooling cold black dwarf neutron star black hole eventually result gravitational relaxation star either fall central supermassive black hole flung intergalactic space result collision gallery see also note reference bibliography external link nasaipac extragalactic database ned ned redshiftindependent distance galaxy time bbc atlas universe galaxy information amateur observation galaxy zoo citizen science galaxy classification project flight universe sloan digital sky survey animated video berkeley lab', 'black hole massive compact astronomical object dense gravity prevents anything escaping even light albert einstein theory general relativity predicts sufficiently compact mass form black hole boundary escape called event horizon black hole great effect fate circumstance object crossing locally detectable feature according general relativity many way black hole act like ideal black body reflects light quantum field theory curved spacetime predicts event horizon emit hawking radiation spectrum black body temperature inversely proportional mass temperature order billionth kelvin stellar black hole making essentially impossible observe directly object whose gravitational field strong light escape first considered th century john michell pierresimon laplace karl schwarzschild found first modern solution general relativity would characterise black hole due influential research schwarzschild metric named david finkelstein first published interpretation black hole region space nothing escape black hole long considered mathematical curiosity theoretical work showed generic prediction general relativity first black hole known cygnus x identified several researcher independently black hole typically form massive star collapse end life cycle black hole formed grow absorbing mass surroundings supermassive black hole million solar mass may form absorbing star merging black hole via direct collapse gas cloud consensus supermassive black hole exist centre galaxy presence black hole inferred interaction matter electromagnetic radiation visible light matter falling toward black hole form accretion disk infalling plasma heated friction emitting light extreme case creates quasar brightest object universe star passing close supermassive black hole shredded streamer shine brightly swallowed star orbiting black hole orbit used determine black hole mass location observation used exclude possible alternative neutron star way astronomer identified numerous stellar black hole candidate binary system established radio source known sagittarius core milky way galaxy contains supermassive black hole million solar mass history idea body big even light could escape briefly proposed english astronomical pioneer clergyman john michell independently french scientist pierresimon laplace scholar proposed large star rather modern model star extraordinary density michells idea short part letter published calculated star density time radius sun would let emitted light escape surface escape velocity would exceed speed light michell correctly noted supermassive nonradiating body might detectable gravitational effect nearby visible body laplace mentioned star could invisible sufficiently large speculating origin solar system book exposition du systme du monde franz xaver von zach asked laplace mathematical analysis laplace provided published journal edited von zach scholar time initially excited proposal giant invisible dark star might hiding plain view enthusiasm dampened wavelike nature light became apparent early nineteenth century since light understood wave rather particle unclear influence gravity would escaping light wave general relativity albert einstein developed theory general relativity earlier shown gravity influence light motion month later karl schwarzschild found solution einstein field equation describes gravitational field point mass spherical mass month schwarzschild johannes droste student hendrik lorentz independently gave solution point mass wrote extensively property solution peculiar behaviour called schwarzschild radius became singular meaning term einstein equation became infinite nature surface quite understood time arthur eddington showed singularity disappeared change coordinate george lematre realised meant singularity schwarzschild radius nonphysical coordinate singularity arthur eddington commented possibility star mass compressed schwarzschild radius book noting einstein theory allows u rule overly large density visible star like betelgeuse star million km radius could possibly high density sun firstly force gravitation would great light would unable escape ray falling back star like stone earth secondly red shift spectral line would great spectrum would shifted existence thirdly mass would produce much curvature spacetime metric space would close around star leaving u outside ie nowhere subrahmanyan chandrasekhar calculated using special relativity nonrotating body electrondegenerate matter certain limiting mass called chandrasekhar limit stable solution argument opposed many contemporary like eddington lev landau argued yet unknown mechanism would stop collapse partly correct white dwarf slightly massive chandrasekhar limit collapse neutron star stable robert oppenheimer george volkoff predicted neutron star another limit tolmanoppenheimervolkoff limit would collapse reason presented chandrasekhar concluded law physic likely intervene stop least star collapsing black hole original calculation based pauli exclusion principle gave subsequent consideration neutronneutron repulsion mediated strong force raised estimate approximately observation neutron star merger gw thought generated black hole shortly afterward refined tov limit estimate oppenheimer coauthor interpreted singularity boundary schwarzschild radius indicating boundary bubble time stopped valid point view external observer infalling observer hypothetical collapsed star called frozen star outside observer would see surface star frozen time instant collapse take schwarzschild radius also einstein attempted prove black hole impossible publication stationary system spherical symmetry consisting many gravitating mass using theory general relativity defend argument month later oppenheimer student hartland snyder provided oppenheimersnyder model paper continued gravitational contraction predicted existence black hole paper made reference einstein recent publication oppenheimer snyder used einstein theory general relativity show condition black hole could develop first time contemporary physic david finkelstein identified schwarzschild surface event horizon perfect unidirectional membrane causal influence cross one direction finkelstein created new reference frame include point view infalling observer finkelsteins solution extended schwarzschild solution future observer falling black hole similar concept already found martin kruskal wheeler understood significance eventually kruskalszekeres coordinate helped physic understand black hole multiple perspective golden age era mids mids golden age black hole research general relativity black hole became mainstream subject research period general black hole solution found roy kerr found exact solution rotating black hole two year later ezra newman found axisymmetric solution black hole rotating electrically charged work werner israel brandon carter david robinson nohair theorem emerged stating stationary black hole solution completely described three parameter kerrnewman metric mass angular momentum electric charge first suspected strange feature black hole solution pathological artefact symmetry condition imposed singularity would appear generic situation view held particular vladimir belinsky isaak khalatnikov evgeny lifshitz tried prove singularity appear generic solution however late roger penrose stephen hawking used global technique prove singularity appear generically work penrose received half nobel prize physic hawking died astronomical observation also made great stride era antony hewish jocelyn bell burnell discovered pulsar shown rapidly rotating neutron star time neutron star like black hole regarded theoretical curiosity discovery pulsar showed physical relevance spurred interest type compact object might formed gravitational collapse based observation greenwich toronto early cygnus x galactic xray source discovered became first astronomical object commonly accepted black hole work james bardeen jacob bekenstein carter hawking early led formulation black hole thermodynamics law describe behaviour black hole close analogy law thermodynamics relating mass energy area entropy surface gravity temperature analogy completed hawking showed quantum field theory implies black hole radiate like black body temperature proportional surface gravity black hole predicting effect known hawking radiation observation february ligo scientific collaboration virgo collaboration announced first direct detection gravitational wave representing first observation black hole merger april first direct image black hole vicinity published following observation made event horizon telescope eht supermassive black hole messier galactic centre gaia mission observation found evidence sunlike star orbiting black hole named gaia bh around lightyears parsec away evidence suggests brown dwarf star orbit gaia bh though couple dozen black hole found far milky way thought hundred million solitary cause emission radiation therefore would detectable gravitational lensing etymology december student reportedly suggested phrase black hole lecture john wheeler wheeler adopted term brevity advertising value wheeler stature field ensured quickly caught leading credit wheeler coining phrase however term used others around time science writer marcia bartusiak trace term black hole physicist robert h dicke early reportedly compared phenomenon black hole calcutta notorious prison people entered never left alive term black hole used print life science news magazine science journalist ann ewing article black hole space dated january report meeting american association advancement science held cleveland ohio property structure escape velocity black hole exceeds speed light formula escape velocity v g r displaystyle vsqrt mgr object radius r spherical mass g gravitational constant velocity speed light c radius r g c displaystyle rsmgc called schwarzschild radius technical definition black hole object whose mass contained radius smaller schwarzschild radius limit derived one solution equation general relativity nohair theorem postulate achieves stable condition formation black hole three independent physical property mass electric charge angular momentum black hole otherwise featureless conjecture true two black hole share value property parameter indistinguishable one another degree conjecture true real black hole law modern physic currently unsolved problem property special visible outside black hole example charged black hole repels like charge like charged object similarly total mass inside sphere containing black hole found using gravitational analogue gauss law adm mass far away black hole likewise angular momentum spin measured far away using frame dragging gravitomagnetic field example lensethirring effect object fall black hole information shape object distribution charge evenly distributed along horizon black hole lost outside observer behaviour horizon situation dissipative system closely analogous conductive stretchy membrane friction electrical resistancethe membrane paradigm different field theory electromagnetism friction resistivity microscopic level timereversible black hole eventually achieves stable state three parameter way avoid losing information initial condition gravitational electric field black hole give little information went information lost includes every quantity cannot measured far away black hole horizon including approximately conserved quantum number total baryon number lepton number behaviour puzzling called black hole information loss paradox physical property simplest static black hole mass neither electric charge angular momentum black hole often referred schwarzschild black hole karl schwarzschild discovered solution according birkhoffs theorem vacuum solution spherically symmetric mean observable difference distance gravitational field black hole spherical object mass popular notion black hole sucking everything surroundings therefore correct near black hole horizon far away external gravitational field identical body mass solution describing general black hole also exist nonrotating charged black hole described reissnernordstrm metric kerr metric describes noncharged rotating black hole general stationary black hole solution known kerrnewman metric describes black hole charge angular momentum mass black hole take positive value charge angular momentum constrained mass total electric charge q total angular momentum j expected satisfy inequality q c j g g displaystyle frac qpi epsilon frac cjgmleq gm black hole mass black hole minimum possible mass satisfying inequality called extremal solution einstein equation violate inequality exist possess event horizon solution socalled naked singularity observed outside hence deemed unphysical cosmic censorship hypothesis rule formation singularity created gravitational collapse realistic matter supported numerical simulation due relatively large strength electromagnetic force black hole forming collapse star expected retain nearly neutral charge star rotation however expected universal feature compact astrophysical object blackhole candidate binary xray source grs appears angular momentum near maximum allowed value uncharged limit j g c displaystyle jleq frac gmc allowing definition dimensionless spin parameter c j g displaystyle leq frac cjgmleq black hole commonly classified according mass independent angular momentum j size black hole determined radius event horizon schwarzschild radius proportional mass r g c k displaystyle rmathrm frac gmcapprox frac mmodot mathrm km r schwarzschild radius mass sun black hole nonzero spin electric charge radius smaller extremal black hole could event horizon close r g c displaystyle rmathrm frac gmc event horizon defining feature black hole appearance event horizona boundary spacetime matter light pas inward towards mass black hole nothing even light escape inside event horizon event horizon referred event occurs within boundary information event cannot reach outside observer making impossible determine whether event occurred predicted general relativity presence mass deforms spacetime way path taken particle bend towards mass event horizon black hole deformation becomes strong path lead away black hole thought experiment distant observer imagine clock near black hole would appear tick slowly farther away black hole effect known gravitational time dilation would also cause object falling black hole appear slow approach event horizon taking infinite amount time reach process object would appear slow viewpoint fixed outside observer light emitted object appear redder dimmer effect known gravitational redshift eventually falling object fade away longer seen typically process happens rapidly object disappearing view within less second hand imaginary indestructible observer falling black hole would notice effect cross event horizon clock appear tick normally cross event horizon finite time without noting singular behaviour general relativity impossible determine location event horizon local observation due einstein equivalence principle topology event horizon black hole equilibrium always spherical nonrotating static black hole geometry event horizon precisely spherical rotating black hole event horizon oblate singularity centre unrealistically simple schwarzschild model black hole gravitational singularity region spacetime curvature becomes infinite nonrotating black hole region take shape single point rotating black hole smeared form ring singularity lie plane rotation case singular region zero volume also shown singular region contains mass black hole solution singular region thus thought infinite density even though schwarzschild model valid singularity observer falling schwarzschild black hole ie nonrotating charged cannot avoid carried singularity cross event horizon prolong experience accelerating away slow descent limit reach singularity crushed infinite density mass added total black hole happens torn apart growing tidal force process sometimes referred spaghettification noodle effect case charged reissnernordstrm rotating kerr black hole possible avoid singularity extending solution far possible reveals hypothetical possibility exiting black hole different spacetime black hole acting wormhole possibility travelling another universe however theoretical since perturbation would destroy possibility also appears possible follow closed timelike curve returning one past around kerr singularity lead problem causality like grandfather paradox expected none peculiar effect would survive proper quantum treatment rotating charged black hole appearance singularity general relativity signal breakdown theory breakdown occurs quantum effect describe action due extremely high density therefore particle interaction date possible combine quantum gravitational effect single theory although exist attempt formulate theory quantum gravity generally expected theory feature singularity photon sphere photon sphere spherical boundary photon move tangent sphere would trapped nonstable circular orbit around black hole nonrotating black hole photon sphere radius time schwarzschild radius orbit would dynamically unstable hence small perturbation particle infalling matter would cause instability would grow time either setting photon outward trajectory causing escape black hole inward spiral would eventually cross event horizon light still escape photon sphere light cross photon sphere inbound trajectory captured black hole hence light reach outside observer photon sphere must emitted object photon sphere event horizon kerr black hole radius photon sphere depends spin parameter detail photon orbit prograde photon rotates sense black hole spin retrograde ergosphere rotating black hole surrounded region spacetime impossible stand still called ergosphere result process known framedragging general relativity predicts rotating mass tend slightly drag along spacetime immediately surrounding object near rotating mass tend start moving direction rotation rotating black hole effect strong near event horizon object would move faster speed light opposite direction stand still ergosphere black hole volume bounded black hole event horizon ergosurface coincides event horizon pole much greater distance around equator object radiation escape normally ergosphere penrose process object emerge ergosphere energy entered extra energy taken rotational energy black hole thereby rotation black hole slows variation penrose process presence strong magnetic field blandfordznajek process considered likely mechanism enormous luminosity relativistic jet quasar active galactic nucleus innermost stable circular orbit isco newtonian gravity test particle stably orbit arbitrary distance central object general relativity however exists innermost stable circular orbit often called isco infinitesimal inward perturbation circular orbit lead spiraling black hole outward perturbation depending energy result spiraling stably orbiting apastron periastron escaping infinity location isco depends spin black hole case schwarzschild black hole spin zero r c r g c displaystyle rrm iscorsfrac gmc decrease increasing black hole spin particle orbiting direction spin plunging region final observable region spacetime around black hole called plunging region area longer possible matter follow circular orbit stop final descent black hole instead rapidly plunge toward black hole close speed light formation evolution given bizarre character black hole long questioned whether object could actually exist nature whether merely pathological solution einstein equation einstein wrongly thought black hole would form held angular momentum collapsing particle would stabilise motion radius led general relativity community dismiss result contrary many year however minority relativists continued contend black hole physical object end persuaded majority researcher field obstacle formation event horizon penrose demonstrated event horizon form general relativity without quantum mechanic requires singularity form within shortly afterwards hawking showed many cosmological solution describe big bang singularity without scalar field exotic matter kerr solution nohair theorem law black hole thermodynamics showed physical property black hole simple comprehensible making respectable subject research conventional black hole formed gravitational collapse heavy object star also theory formed process gravitational collapse gravitational collapse occurs object internal pressure insufficient resist object gravity star usually occurs either star little fuel left maintain temperature stellar nucleosynthesis star would stable receives extra matter way raise core temperature either case star temperature longer high enough prevent collapsing weight collapse may stopped degeneracy pressure star constituent allowing condensation matter exotic denser state result one various type compact star type form depends mass remnant original star left outer layer blown away example type ii supernova mass remnant collapsed object survives explosion substantially less original star remnant exceeding produced star collapse mass remnant exceeds tolmanoppenheimervolkoff limit either original star heavy remnant collected additional mass accretion matter even degeneracy pressure neutron insufficient stop collapse known mechanism except possibly quark degeneracy pressure powerful enough stop implosion object inevitably collapse form black hole gravitational collapse heavy star assumed responsible formation stellar mass black hole star formation early universe may resulted massive star upon collapse would produced black hole black hole could seed supermassive black hole found centre galaxy suggested massive black hole typical mass could formed direct collapse gas cloud young universe massive object proposed seed eventually formed earliest quasar observed already redshift z displaystyle zsim candidate object found observation young universe energy released gravitational collapse emitted quickly outside observer actually see end process even though collapse take finite amount time reference frame infalling matter distant observer would see infalling material slow halt event horizon due gravitational time dilation light collapsing material take longer longer reach observer light emitted event horizon form delayed infinite amount time thus external observer never see formation event horizon instead collapsing material seems become dimmer increasingly redshifted eventually fading away primordial black hole big bang gravitational collapse requires great density current epoch universe high density found star early universe shortly big bang density much greater possibly allowing creation black hole high density alone enough allow black hole formation since uniform mass distribution allow mass bunch order primordial black hole formed dense medium must initial density perturbation could grow gravity different model early universe vary widely prediction scale fluctuation various model predict creation primordial black hole ranging size planck mass p c g displaystyle mpsqrt hbar cg gevc kg hundred thousand solar mass despite early universe extremely dense recollapse black hole big bang since expansion rate greater attraction following inflation theory general relativity predicts gravitational field slows expansion universethe negative pressure overcomes positive energy density produce net repulsive gravitational field model gravitational collapse object relatively constant size star necessarily apply way rapidly expanding space big bang highenergy collision gravitational collapse process could create black hole principle black hole could formed highenergy collision achieve sufficient density event detected either directly indirectly deficiency mass balance particle accelerator experiment suggests must lower limit mass black hole theoretically boundary expected lie around planck mass quantum effect expected invalidate prediction general relativity would put creation black hole firmly reach highenergy process occurring near earth however certain development quantum gravity suggest minimum black hole mass could much lower braneworld scenario example put boundary low tevc would make conceivable micro black hole created highenergy collision occur cosmic ray hit earth atmosphere possibly large hadron collider cern theory speculative creation black hole process deemed unlikely many specialist even micro black hole could formed expected would evaporate second posing threat earth growth black hole formed continue grow absorbing additional matter black hole continually absorb gas interstellar dust surroundings growth process one possible way supermassive black hole may formed although formation supermassive black hole still open field research similar process suggested formation intermediatemass black hole found globular cluster black hole also merge object star even black hole thought important especially early growth supermassive black hole could formed aggregation many smaller object process also proposed origin intermediatemass black hole evaporation hawking predicted black hole entirely black emit small amount thermal radiation temperature cgmkb effect become known hawking radiation applying quantum field theory static black hole background determined black hole emit particle display perfect black body spectrum since hawking publication many others verified result various approach hawking theory black hole radiation correct black hole expected shrink evaporate time lose mass emission photon particle temperature thermal spectrum hawking temperature proportional surface gravity black hole schwarzschild black hole inversely proportional mass hence large black hole emit less radiation small black hole stellar black hole hawking temperature nanokelvins far less k temperature cosmic microwave background radiation stellarmass larger black hole receive mass cosmic microwave background emit hawking radiation thus grow instead shrinking hawking temperature larger k able evaporate black hole would need mass less moon black hole would diameter less tenth millimetre black hole small radiation effect expected become strong black hole mass car would diameter take nanosecond evaporate time would briefly luminosity time sun lowermass black hole expected evaporate even faster example black hole mass tevc would take less second evaporate completely small black hole quantum gravity effect expected play important role could hypothetically make small black hole stable although current development quantum gravity indicate case hawking radiation astrophysical black hole predicted weak would thus exceedingly difficult detect earth possible exception however burst gamma ray emitted last stage evaporation primordial black hole search flash proven unsuccessful provide stringent limit possibility existence low mass primordial black hole nasa fermi gammaray space telescope launched continue search flash black hole evaporate via hawking radiation solar mass black hole evaporate beginning temperature cosmic microwave background drop black hole period year supermassive black hole mass evaporate around year collapse supercluster galaxy supermassive black hole predicted grow perhaps even would evaporate timescale year observational evidence nature black hole emit electromagnetic radiation hypothetical hawking radiation astrophysicist searching black hole must generally rely indirect observation example black hole existence sometimes inferred observing gravitational influence surroundings direct interferometry event horizon telescope eht active program directly observes immediate environment black hole event horizon black hole centre milky way april eht began observing black hole centre messier eight radio observatory six mountain four continent observed galaxy virgo day april provide data yielding image april two year data processing eht released first image black hole center messier galaxy visible black holewhich show black loss light within dark region instead gas edge event horizon displayed orange red define black hole may eht released first image sagittarius supermassive black hole centre milky way galaxy published image displayed ringlike structure shadow seen black hole boundary shadow area less brightness match predicted gravitationally lensed photon orbit image created using technique black hole imaging process sagittarius thousand time smaller less massive significantly complex instability surroundings image sagittarius partially blurred turbulent plasma way galactic centre effect prevents resolution image longer wavelength brightening material bottom half processed eht image thought caused doppler beaming whereby material approaching viewer relativistic speed perceived brighter material moving away case black hole phenomenon implies visible material rotating relativistic speed km mph speed possible centrifugally balance immense gravitational attraction singularity thereby remain orbit event horizon configuration bright material implies eht observed perspective catching black hole accretion disc nearly edgeon whole system rotated clockwise extreme gravitational lensing associated black hole produce illusion perspective see accretion disc reality ring eht image created light emitted far side accretion disc bent around black hole gravity well escaped meaning possible perspective see entire disc even directly behind shadow eht detected magnetic field outside event horizon sagittarius even discerned property field line pas accretion disc complex mixture ordered tangled theoretical study black hole predicted existence magnetic field april image shadow messier black hole related highenergy jet viewed together first time presented detection gravitational wave merging black hole september ligo gravitational wave observatory made firstever successful direct observation gravitational wave signal consistent theoretical prediction gravitational wave produced merger two black hole one solar mass around solar mass observation provides concrete evidence existence black hole date instance gravitational wave signal suggests separation two object merger km roughly four time schwarzschild radius corresponding inferred mass object must therefore extremely compact leaving black hole plausible interpretation importantly signal observed ligo also included start postmerger ringdown signal produced newly formed compact object settle stationary state arguably ringdown direct way observing black hole ligo signal possible extract frequency damping time dominant mode ringdown possible infer mass angular momentum final object match independent prediction numerical simulation merger frequency decay time dominant mode determined geometry photon sphere hence observation mode confirms presence photon sphere however cannot exclude possible exotic alternative black hole compact enough photon sphere observation also provides first observational evidence existence stellarmass black hole binary furthermore first observational evidence stellarmass black hole weighing solar mass since many gravitational wave event observed star orbiting sagittarius proper motion star near centre milky way provide strong observational evidence star orbiting supermassive black hole since astronomer tracked motion star orbiting invisible object coincident radio source sagittarius fitting motion keplerian orbit astronomer able infer object must contained volume radius lightyears cause motion star since one starscalled shas completed full orbit orbital data astronomer able refine calculation mass radius less lightyears object causing orbital motion star upper limit object size still large test whether smaller schwarzschild radius nevertheless observation strongly suggest central object supermassive black hole plausible scenario confining much invisible mass small volume additionally observational evidence object might possess event horizon feature unique black hole accretion matter due conservation angular momentum gas falling gravitational well created massive object typically form disklike structure around object artist impression accompanying representation black hole corona commonly depict black hole flatspace body hiding part disk behind reality gravitational lensing would greatly distort image accretion disk within disk friction would cause angular momentum transported outward allowing matter fall farther inward thus releasing potential energy increasing temperature gas accreting object neutron star black hole gas inner accretion disk orbit high speed proximity compact object resulting friction significant heat inner disk temperature emits vast amount electromagnetic radiation mainly xrays bright xray source may detected telescope process accretion one efficient energyproducing process known rest mass accreted material emitted radiation nuclear fusion rest mass emitted energy many case accretion disk accompanied relativistic jet emitted along pole carry away much energy mechanism creation jet currently well understood part due insufficient data many universe energetic phenomenon attributed accretion matter black hole particular active galactic nucleus quasar believed accretion disk supermassive black hole similarly xray binary generally accepted binary star system one two star compact object accreting matter companion also suggested ultraluminous xray source may accretion disk intermediatemass black hole star observed get torn apart tidal force immediate vicinity supermassive black hole galaxy nucleus known tidal disruption event tde material disrupted star form accretion disk around black hole emits observable electromagnetic radiation november first direct observation quasar accretion disk around supermassive black hole reported xray binary xray binary binary star system emit majority radiation xray part spectrum xray emission generally thought result one star compact object accretes matter another regular star presence ordinary star system provides opportunity studying central object determine might black hole system emits signal directly traced back compact object cannot black hole absence signal however exclude possibility compact object neutron star studying companion star often possible obtain orbital parameter system obtain estimate mass compact object much larger tolmanoppenheimervolkoff limit maximum mass star without collapsing object cannot neutron star generally expected black hole first strong candidate black hole cygnus x discovered way charles thomas bolton louise webster paul murdin doubt remained due uncertainty result companion star much heavier candidate black hole currently better candidate black hole found class xray binary called soft xray transient class system companion star relatively low mass allowing accurate estimate black hole mass system actively emit xrays several month every year period low xray emission called quiescence accretion disk extremely faint allowing detailed observation companion star period one best candidate v cygni quasiperiodic oscillation xray emission accretion disk sometimes flicker certain frequency signal called quasiperiodic oscillation thought caused material moving along inner edge accretion disk innermost stable circular orbit frequency linked mass compact object thus used alternative way determine mass candidate black hole galactic nucleus astronomer use term active galaxy describe galaxy unusual characteristic unusual spectral line emission strong radio emission theoretical observational study shown activity active galactic nucleus agn may explained presence supermassive black hole million time massive stellar one model agn consist central black hole may million billion time massive sun disk interstellar gas dust called accretion disk two jet perpendicular accretion disk although supermassive black hole expected found agn galaxy nucleus carefully studied attempt identify measure actual mass central supermassive black hole candidate notable galaxy supermassive black hole candidate include andromeda galaxy ngc ngc ngc ngc ngc oj apm sombrero galaxy widely accepted centre nearly every galaxy active one contains supermassive black hole close observational correlation mass hole velocity dispersion host galaxy bulge known msigma relation strongly suggests connection formation black hole galaxy microlensing another way black hole nature object may tested observation effect caused strong gravitational field vicinity one effect gravitational lensing deformation spacetime around massive object cause light ray deflected light passing optic lens observation made weak gravitational lensing light ray deflected arcsecond microlensing occurs source unresolved observer see small brightening turn millennium saw first candidate detection black hole way january astronomer reported first confirmed detection microlensing event isolated black hole another possibility observing gravitational lensing black hole would observe star orbiting black hole several candidate observation orbit around sagittarius alternative evidence stellar black hole strongly relies existence upper limit mass neutron star size limit heavily depends assumption made property dense matter new exotic phase matter could push bound phase free quark high density might allow existence dense quark star supersymmetric model predict existence q star extension standard model posit existence preons fundamental building block quark lepton could hypothetically form preon star hypothetical model could potentially explain number observation stellar black hole candidate however shown argument general relativity object maximum mass since average density black hole inside schwarzschild radius inversely proportional square mass supermassive black hole much less dense stellar black hole average density black hole comparable water consequently physic matter forming supermassive black hole much better understood possible alternative explanation supermassive black hole observation much mundane example supermassive black hole could modelled large cluster dark object however alternative typically stable enough explain supermassive black hole candidate evidence existence stellar supermassive black hole implies order black hole form general relativity must fail theory gravity perhaps due onset quantum mechanical correction much anticipated feature theory quantum gravity feature singularity event horizon thus black hole would real artefact example fuzzball model based string theory individual state black hole solution generally event horizon singularity classicalsemiclassical observer statistical average state appears ordinary black hole deduced general relativity theoretical object conjectured match observation astronomical black hole candidate identically nearidentically function via different mechanism include gravastar black star related nestar darkenergy star open question entropy thermodynamics hawking showed general condition total area event horizon collection classical black hole never decrease even collide merge result known second law black hole mechanic remarkably similar second law thermodynamics state total entropy isolated system never decrease classical object absolute zero temperature assumed black hole zero entropy case second law thermodynamics would violated entropyladen matter entering black hole resulting decrease total entropy universe therefore bekenstein proposed black hole entropy proportional horizon area link law thermodynamics strengthened hawking discovery quantum field theory predicts black hole radiates blackbody radiation constant temperature seemingly cause violation second law black hole mechanic since radiation carry away energy black hole causing shrink radiation also carry away entropy proven general assumption sum entropy matter surrounding black hole one quarter area horizon measured planck unit fact always increasing allows formulation first law black hole mechanic analogue first law thermodynamics mass acting energy surface gravity temperature area entropy one puzzling feature entropy black hole scale area rather volume since entropy normally extensive quantity scale linearly volume system odd property led gerard hooft leonard susskind propose holographic principle suggests anything happens volume spacetime described data boundary volume although general relativity used perform semiclassical calculation black hole entropy situation theoretically unsatisfying statistical mechanic entropy understood counting number microscopic configuration system macroscopic quality mass charge pressure etc without satisfactory theory quantum gravity one cannot perform computation black hole progress made various approach quantum gravity andrew strominger cumrun vafa showed counting microstates specific supersymmetric black hole string theory reproduced bekensteinhawking entropy since similar result reported different black hole string theory approach quantum gravity like loop quantum gravity information loss paradox black hole internal parameter information matter went forming black hole lost regardless type matter go black hole appears information concerning total mass charge angular momentum conserved long black hole thought persist forever information loss problematic information thought existing inside black hole inaccessible outside represented event horizon accordance holographic principle however black hole slowly evaporate emitting hawking radiation radiation appear carry additional information matter formed black hole meaning information appears gone forever question whether information truly lost black hole black hole information paradox divided theoretical physic community quantum mechanic loss information corresponds violation property called unitarity argued loss unitarity would also imply violation conservation energy though also disputed recent year evidence building indeed information unitarity preserved full quantum gravitational treatment problem one attempt resolve black hole information paradox known black hole complementarity firewall paradox introduced goal demonstrating black hole complementarity fails solve information paradox according quantum field theory curved spacetime single emission hawking radiation involves two mutually entangled particle outgoing particle escape emitted quantum hawking radiation infalling particle swallowed black hole assume black hole formed finite time past fully evaporate away finite time future emit finite amount information encoded within hawking radiation according research physicist like page leonard susskind eventually time outgoing particle must entangled hawking radiation black hole previously emitted seemingly creates paradox principle called monogamy entanglement requires like quantum system outgoing particle cannot fully entangled two system time yet outgoing particle appears entangled infalling particle independently past hawking radiation order resolve contradiction physicist may eventually forced give one three timetested principle einstein equivalence principle unitarity local quantum field theory one possible solution violates equivalence principle firewall destroys incoming particle event horizon general whichif anyof assumption abandoned remains topic debate science fiction christopher nolans science fiction epic interstellar feature black hole known gargantua central object planetary system distant galaxy humanity accessed system via wormhole outer solar system near saturn see also note reference source carroll sean spacetime geometry addison wesley isbn lecture note book based available free sean carroll website archived march wayback machine hawking w elli g f r large scale structure space time cambridge university press isbn archived original july retrieved may misner charles thorne kip wheeler john gravitation w h freeman company isbn thorne kip black hole time warp norton w w company inc isbn wald robert general relativity university chicago press isbn archived original august retrieved february wheeler j craig cosmic catastrophe nd ed cambridge university press isbn reading popular reading university textbook monograph review paper external link black hole time bbc stanford encyclopedia philosophy singularity black hole erik curiel peter bokulich black hole gravity relentless pull interactive multimedia web site physic astronomy black hole space telescope science institute hubblesite esas black hole visualization archived may wayback machine frequently asked question faq black hole schwarzschild geometry black hole basic nyt april video yearlong study track star orbiting sagittarius movie black hole candidate max planck institute cowen ron april simulation colliding black hole hailed realistic yet nature doinature computer visualisation signal detected ligo two black hole merge one based upon signal gw', 'supernova pl supernova supernova powerful luminous explosion star supernova occurs last evolutionary stage massive star white dwarf triggered runaway nuclear fusion original object called progenitor either collapse neutron star black hole completely destroyed form diffuse nebula peak optical luminosity supernova comparable entire galaxy fading several week month last supernova directly observed milky way kepler supernova appearing long tychos supernova visible naked eye observation recent supernova remnant within milky way coupled study supernova galaxy suggest powerful stellar explosion occur galaxy approximately three time per century average supernova milky way would almost certainly observable modern astronomical telescope recent nakedeye supernova sn explosion blue supergiant star large magellanic cloud satellite galaxy milky way theoretical study indicate supernova triggered one two basic mechanism sudden reignition nuclear fusion white dwarf sudden gravitational collapse massive star core reignition white dwarf object temperature raised enough trigger runaway nuclear fusion completely disrupting star possible cause accumulation material binary companion accretion stellar merger case massive star sudden implosion core massive star undergo sudden collapse unable produce sufficient energy fusion counteract star gravity must happen star begin fusing iron may happen earlier stage metal fusion supernova expel several solar mass material speed several percent speed light drive expanding shock wave surrounding interstellar medium sweeping expanding shell gas dust observed supernova remnant supernova major source element interstellar medium oxygen rubidium expanding shock wave supernova trigger formation new star supernova major source cosmic ray might also produce gravitational wave etymology word supernova plural form supernova supernova often abbreviated sn sne derived latin word nova meaning new refers appears temporary new bright star adding prefix super distinguishes supernova ordinary nova far less luminous word supernova coined walter baade fritz zwicky began using astrophysics lecture first use journal article came following year publication knut lundmark may coined independently observation history compared star entire history visual appearance supernova brief sometimes spanning several month chance observing one naked eye roughly lifetime tiny fraction billion star typical galaxy capacity become supernova ability restricted high mass rare kind binary star system least one white dwarf early discovery earliest record possible supernova known hb likely viewed unknown prehistoric people indian subcontinent recorded rock carving burzahama region kashmir dated bc later sn documented chinese astronomer ad brightest recorded supernova sn observed ad constellation lupus event described observer china japan iraq egypt europe widely observed supernova sn produced crab nebula supernova sn sn latest milky way supernova observed naked eye notable influence development astronomy europe used argue aristotelian idea universe beyond moon planet static unchanging johannes kepler began observing sn peak october continued make estimate brightness faded naked eye view year later second supernova observed generation tycho brahe observed sn cassiopeia evidence youngest known supernova galaxy g occurred late th century considerably recently cassiopeia around neither noted time case g high extinction dust along plane galactic disk could dimmed event sufficiently go unnoticed situation cassiopeia less clear infrared light echo detected showing region especially high extinction telescope finding development astronomical telescope observation discovery fainter distant supernova became possible first observation sn andromeda galaxy second supernova sn b discovered ngc decade later early work originally believed simply new category nova performed variously called upperclass nova hauptnovae giant nova name supernova thought coined walter baade zwicky lecture caltech used supernova journal paper published knut lundmark paper baade zwicky hyphen longer used modern name use american astronomer rudolph minkowski fritz zwicky developed modern supernova classification scheme beginning astronomer found maximum intensity supernova could used standard candle hence indicator astronomical distance distant supernova observed appeared dimmer expected support view expansion universe accelerating technique developed reconstructing supernova event written record observed date cassiopeia supernova event determined light echo nebula age supernova remnant rx j estimated temperature measurement gamma ray emission radioactive decay titanium luminous supernova ever recorded asassnlh distance gigalightyears first detected june peaked billion l twice bolometric luminosity known supernova nature supernova debated several alternative explanation tidal disruption star black hole suggested sn f recorded three hour supernova event october intermediate palomar transient factory among earliest supernova caught detonation earliest spectrum obtained beginning six hour actual explosion star located spiral galaxy named ngc million lightyears away constellation pegasus supernova sn gkg detected amateur astronomer victor buso rosario argentina september first time initial shock breakout optical supernova observed progenitor star identified hubble space telescope image collapse astronomer alex filippenko noted observation star first moment begin exploding provide information cannot directly obtained way discovery program supernova relatively rare event within galaxy occurring three time century milky way obtaining good sample supernova study requires regular monitoring many galaxy today amateur professional astronomer finding two thousand every year near maximum brightness others old astronomical photograph plate supernova galaxy cannot predicted meaningful accuracy normally discovered already progress use supernova standard candle measuring distance observation peak luminosity required therefore important discover well reach maximum amateur astronomer greatly outnumber professional astronomer played important role finding supernova typically looking closer galaxy optical telescope comparing earlier photograph toward end th century astronomer increasingly turned computercontrolled telescope ccds hunting supernova system popular amateur also professional installation katzman automatic imaging telescope supernova early warning system snews project us network neutrino detector give early warning supernova milky way galaxy neutrino subatomic particle produced great quantity supernova significantly absorbed interstellar gas dust galactic disk supernova search fall two class focused relatively nearby event looking farther away expansion universe distance remote object known emission spectrum estimated measuring doppler shift redshift average moredistant object recede greater velocity nearby higher redshift thus search split high redshift low redshift boundary falling around redshift range z z dimensionless measure spectrum frequency shift high redshift search supernova usually involve observation supernova light curve useful standard calibrated candle generate hubble diagram make cosmological prediction supernova spectroscopy used study physic environment supernova practical low high redshift low redshift observation also anchor lowdistance end hubble curve plot distance versus redshift visible galaxy survey programme rapidly increase number detected supernova collated collection observation light decay curve astrometry presupernova observation spectroscopy assembled pantheon data set assembled detailed supernova data set expanded light curve supernova taken different survey increase year naming convention supernova discovery reported international astronomical union central bureau astronomical telegram sends circular name assigns supernova name formed prefix sn followed year discovery suffixed one twoletter designation first supernova year designated capital letter z next pair lowercase letter used aa ab hence example sn c designates third supernova reported year last supernova sn nc th since professional amateur astronomer finding several hundred supernova year historical supernova known simply year occurred sn sn sn sn called tychos nova sn kepler star since additional letter notation used even one supernova discovered year example sn sn etc last happened sn sn supernova standard prefix twoletter designation rarely needed since needed every year since increasing number discovery regularly led additional use threeletter designation zz come aaa aab aac example last supernova retained asiago supernova catalogue terminated december bear designation sn jzp classification astronomer classify supernova according light curve absorption line different chemical element appear spectrum supernova spectrum contains line hydrogen known balmer series visual portion spectrum classified type ii otherwise type two type subdivision according presence line element shape light curve graph supernova apparent magnitude function time type type supernova subdivided basis spectrum type ia showing strong ionised silicon absorption line type supernova without strong line classified type ib ic type ib showing strong neutral helium line type ic lacking historically light curve type supernova seen broadly similar much make useful distinction variation light curve studied classification continues made spectral ground rather lightcurve shape small number type ia supernova exhibit unusual feature nonstandard luminosity broadened light curve typically categorised referring earliest example showing similar feature example subluminous sn ha often referred sn cxlike class iacx small proportion type ic supernova show highly broadened blended emission line taken indicate high expansion velocity ejecta classified type icbl icbl calciumrich supernova rare type fast supernova unusually strong calcium line spectrum model suggest occur material accreted heliumrich companion rather hydrogenrich star helium line spectrum resemble type ib supernova thought different progenitor type ii supernova type ii also subdivided based spectrum type ii supernova show broad emission line indicate expansion velocity many thousand kilometre per second sn gl relatively narrow feature spectrum called type iin n stand narrow supernova sn k sn j appear change type show line hydrogen early time period week month become dominated line helium term type iib used describe combination feature normally associated type ii type ib type ii supernova normal spectrum dominated broad hydrogen line remain life decline classified basis light curve common type show distinctive plateau light curve shortly peak brightness visual luminosity stay relatively constant several month decline resume called type iip referring plateau less common type iil supernova lack distinct plateau l signifies linear although light curve actually straight line supernova fit normal classification designated peculiar pec type iii iv v zwicky defined additional supernova type based example cleanly fit parameter type type ii supernova sn ngc prototype member type iii supernova class noted broad light curve maximum broad hydrogen balmer line slow develop spectrum sn f ngc prototype member type iv class light curve similar type iip supernova hydrogen absorption line weak hydrogen emission line type v class coined sn v ngc unusual faint supernova supernova impostor slow rise brightness maximum lasting many month unusual emission spectrum similarity sn v eta carina great outburst noted supernova also suggested possible type iv type v supernova type would treated peculiar type ii supernova iipec many example discovered although still debated whether sn v true supernova following lbv outburst impostor current model supernova type code summarised table taxonomic type number based light observed supernova necessarily cause example type ia supernova produced runaway fusion ignited degenerate white dwarf progenitor spectrally similar type ibc produced massive stripped progenitor star core collapse thermal runaway white dwarf star may accumulate sufficient material stellar companion raise core temperature enough ignite carbon fusion point undergoes runaway nuclear fusion completely disrupting three avenue detonation theorised happen stable accretion material companion collision two white dwarf accretion cause ignition shell ignites core dominant mechanism type ia supernova produced remains unclear despite uncertainty type ia supernova produced type ia supernova uniform property useful standard candle intergalactic distance calibration required compensate gradual change property different frequency abnormal luminosity supernova high redshift small variation brightness identified light curve shape spectrum normal type ia several mean supernova type form share common underlying mechanism carbonoxygen white dwarf accreted enough matter reach chandrasekhar limit solar mass nonrotating star would longer able support bulk mass electron degeneracy pressure would begin collapse however current view limit normally attained increasing temperature density inside core ignite carbon fusion star approach limit within collapse initiated contrast core primarily composed oxygen neon magnesium collapsing white dwarf typically form neutron star case fraction star mass ejected collapse within second collapse process substantial fraction matter white dwarf undergoes nuclear fusion releasing enough energy j unbind star supernova outwardly expanding shock wave generated matter reaching velocity order km roughly speed light also significant increase luminosity reaching absolute magnitude billion time brighter sun little variation model formation category supernova close binary star system larger two star first evolve main sequence expands form red giant two star share common envelope causing mutual orbit shrink giant star shed envelope losing mass longer continue nuclear fusion point becomes white dwarf star composed primarily carbon oxygen eventually secondary star also evolves main sequence form red giant matter giant accreted white dwarf causing latter increase mass exact detail initiation heavy element produced catastrophic event remain unclear type ia supernova produce characteristic light curvethe graph luminosity function timeafter event luminosity generated radioactive decay nickel cobalt iron peak luminosity light curve extremely consistent across normal type ia supernova maximum absolute magnitude typical type ia supernova arise consistent type progenitor star gradual mass acquisition explode acquire consistent typical mass giving rise similar supernova condition behaviour allows used secondary standard candle measure distance host galaxy second model formation type ia supernova involves merger two white dwarf star combined mass momentarily exceeding chandrasekhar limit sometimes referred doubledegenerate model star degenerate white dwarf due possible combination mass chemical composition pair much variation type event many case may supernova case less luminous light curve normal sn type ia nonstandard type ia abnormally bright type ia supernova occur white dwarf already mass higher chandrasekhar limit possibly enhanced asymmetry ejected material less normal kinetic energy superchandrasekharmass scenario occur example extra mass supported differential rotation formal subclassification nonstandard type ia supernova proposed group subluminous supernova occur helium accretes onto white dwarf classified type iax type supernova may always completely destroy white dwarf progenitor could leave behind zombie star one specific type supernova originates exploding white dwarf like type ia contains hydrogen line spectrum possibly white dwarf surrounded envelope hydrogenrich circumstellar material supernova dubbed type iaiin type ian type iia type iian quadruple star hd belonging open cluster ic vela constellation predicted become nonstandard type ia supernova core collapse massive star undergo core collapse nuclear fusion becomes unable sustain core gravity passing threshold cause type supernova except type ia collapse may cause violent expulsion outer layer star resulting supernova however release gravitational potential energy insufficient star may instead collapse black hole neutron star little radiated energy core collapse caused several different mechanism exceeding chandrasekhar limit electron capture pairinstability photodisintegration massive star develops iron core larger chandrasekhar mass longer able support electron degeneracy pressure collapse neutron star black hole electron capture magnesium degenerate onemg core solar mass progenitor star remove support cause gravitational collapse followed explosive oxygen fusion similar result electronpositron pair production large posthelium burning core remove thermodynamic support cause initial collapse followed runaway fusion resulting pairinstability supernova sufficiently large hot stellar core may generate gammarays energetic enough initiate photodisintegration directly cause complete collapse core table list known reason core collapse massive star type star occur associated supernova type remnant produced metallicity proportion element hydrogen helium compared sun initial mass mass star prior supernova event given multiple sun mass although mass time supernova may much lower type iin supernova listed table produced various type core collapse different progenitor star possibly even type ia white dwarf ignition although seems iron core collapse luminous supergiant hypergiants including lbvs narrow spectral line named occur supernova expanding small dense cloud circumstellar material appears significant proportion supposed type iin supernova supernova impostor massive eruption lbvlike star similar great eruption eta carina event material previously ejected star creates narrow absorption line cause shock wave interaction newly ejected material detailed process stellar core longer supported gravity collapse velocity reaching km c resulting rapid increase temperature density follows depends mass structure collapsing core lowmass degenerate core forming neutron star highermass degenerate core mostly collapsing completely black hole nondegenerate core undergoing runaway fusion initial collapse degenerate core accelerated beta decay photodisintegration electron capture cause burst electron neutrino density increase neutrino emission cut become trapped core inner core eventually reach typically km diameter density comparable atomic nucleus neutron degeneracy pressure try halt collapse core mass solar mass neutron degeneracy insufficient stop collapse black hole form directly supernova lower mass core collapse stopped newly formed neutron core initial temperature billion kelvin time temperature sun core temperature neutrinoantineutrino pair flavour efficiently formed thermal emission thermal neutrino several time abundant electroncapture neutrino joule approximately star rest mass converted tensecond burst neutrino main output event suddenly halted core collapse rebound produce shock wave stall outer core within millisecond energy lost dissociation heavy element process clearly understood necessary allow outer layer core reabsorb around joule foe neutrino pulse producing visible brightness although theory could power explosion material outer envelope fall back onto neutron star core beyond sufficient fallback form black hole fallback reduce kinetic energy created mass expelled radioactive material situation may also generate relativistic jet result gammaray burst exceptionally luminous supernova collapse massive nondegenerate core ignite fusion core collapse initiated pair instability photon turning electronpositron pair thereby reducing radiation pressure oxygen fusion begin collapse may halted core mass collapse halt star remains intact collapse occur larger core formed core around fusion oxygen heavier element energetic entire star disrupted causing supernova upper end mass range supernova unusually luminous extremely longlived due many solar mass ejected ni even larger core mass core temperature becomes high enough allow photodisintegration core collapse completely black hole type ii star initial mass less never develop core large enough collapse eventually lose atmosphere become white dwarf star least possibly much evolve complex fashion progressively burning heavier element hotter temperature core star becomes layered like onion burning easily fused element occurring larger shell although popularly described onion iron core least massive supernova progenitor oxygenneonmagnesium core superagb star may form majority core collapse supernova although less luminous less commonly observed massive progenitor core collapse occurs supergiant phase star still hydrogen envelope result type ii supernova rate mass loss luminous star depends metallicity luminosity extremely luminous star near solar metallicity lose hydrogen reach core collapse form supernova type ii low metallicity star reach core collapse hydrogen envelope sufficiently massive star collapse directly black hole without producing visible supernova star initial mass time sun little less high metallicity result type iip supernova commonly observed type moderate high metallicity star near upper end mass range lost hydrogen core collapse occurs result type iil supernova low metallicity star around reach core collapse pair instability still hydrogen atmosphere oxygen core result supernova type ii characteristic large mass ejected ni high luminosity type ib ic supernova like type ii massive star undergo core collapse unlike progenitor type ii supernova star become type ib type ic supernova lost outer hydrogen envelope due strong stellar wind else interaction companion star known wolfrayet star occur moderate high metallicity continuum driven wind cause sufficiently high massloss rate observation type ibc supernova match observed expected occurrence wolfrayet star alternate explanation type core collapse supernova involve star stripped hydrogen binary interaction binary model provide better match observed supernova proviso suitable binary helium star ever observed type ib supernova common result wolfrayet star type wc still helium atmosphere narrow range mass star evolve reaching core collapse become wo star little helium remaining progenitor type ic supernova percent type ic supernova associated gammaray burst grb though also believed hydrogenstripped type ib ic supernova could produce grb depending circumstance geometry mechanism producing type grb jet produced magnetic field rapidly spinning magnetar formed collapsing core star jet would also transfer energy expanding outer shell producing superluminous supernova ultrastripped supernova occur exploding star stripped almost way metal core via mass transfer close binary result little material ejected exploding star c extreme case ultrastripped supernova occur naked metal core barely chandrasekhar mass limit sn ek might first observational example ultrastripped supernova giving rise relatively dim fast decaying light curve nature ultrastripped supernova iron corecollapse electron capture supernova depending mass collapsing core ultrastripped supernova believed associated second supernova explosion binary system producing example tight double neutron star system team astronomer led researcher weizmann institute science reported first supernova explosion showing direct evidence wolfrayet progenitor star sn hgp type icn supernova also first element neon detected electroncapture supernova third type supernova predicted kenichi nomoto university tokyo called electroncapture supernova would arise star transitional range solar mass white dwarf formation iron corecollapse supernova degenerate onemg core imploded core ran nuclear fuel causing gravity compress electron star core atomic nucleus leading supernova explosion leaving behind neutron star june paper journal nature astronomy reported supernova sn zd galaxy ngc million lightyears earth appeared first observation electroncapture supernova supernova explosion created crab nebula galaxy thought best candidate electroncapture supernova paper make likely correct failed supernova core collapse massive star may result visible supernova happens initial core collapse cannot reversed mechanism produce explosion usually core massive event difficult detect large survey detected possible candidate red supergiant nbh ngc underwent modest outburst march fading view faint infrared source remains star location light curve ejecta gas would dim quickly without energy input keep hot source energywhich maintain optical supernova glow monthswas first puzzle considered rotational energy central pulsar source although energy initially power type supernova delivered promptly light curve dominated subsequent radioactive heating rapidly expanding ejecta intensely radioactive nature ejecta gas first calculated sound nucleosynthesis ground late since demonstrated correct supernova sn direct observation gammaray line unambiguously identified major radioactive nucleus known direct observation much light curve graph luminosity function time occurrence type ii supernova sn explained predicted radioactive decay although luminous emission consists optical photon radioactive power absorbed ejected gas keep remnant hot enough radiate light radioactive decay ni daughter co fe produce gammaray photon primarily energy kev kev absorbed dominate heating thus luminosity ejecta intermediate time several week late time several month energy peak light curve sna provided decay ni co halflife day energy later light curve particular fit closely day halflife co decaying fe later measurement space gammaray telescope small fraction co co gamma ray escaped sn remnant without absorption confirmed earlier prediction two radioactive nucleus power source latetime decay phase visual light curve different supernova type depend radioactive heating vary shape amplitude underlying mechanism way visible radiation produced epoch observation transparency ejected material light curve significantly different wavelength example ultraviolet wavelength early extremely luminous peak lasting hour corresponding breakout shock launched initial event breakout hardly detectable optically light curve type ia mostly uniform consistent maximum absolute magnitude relatively steep decline luminosity optical energy output driven radioactive decay ejected nickel halflife day decay radioactive cobalt halflife day radioisotope excite surrounding material incandescence modern study cosmology rely ni radioactivity providing energy optical brightness supernova type ia standard candle cosmology whose diagnostic kev kev gamma ray first detected initial phase light curve decline steeply effective size photosphere decrease trapped electromagnetic radiation depleted light curve continues decline b band may show small shoulder visual day hint secondary maximum occurs infrared certain ionised heavy element recombine produce infrared radiation ejecta become transparent visual light curve continues decline rate slightly greater decay rate radioactive cobalt longer halflife control later curve ejected material becomes diffuse less able convert high energy radiation visual radiation several month light curve change decline rate positron emission remaining cobalt becomes dominant although portion light curve littlestudied type ib ic light curve similar type ia although lower average peak luminosity visual light output due radioactive decay converted visual radiation much lower mass created nickel peak luminosity varies considerably even occasional type ibc supernova order magnitude less luminous norm luminous type ic supernova referred hypernovae tend broadened light curve addition increased peak luminosity source extra energy thought relativistic jet driven formation rotating black hole also produce gammaray burst light curve type ii supernova characterised much slower decline type order magnitude per day excluding plateau phase visual light output dominated kinetic energy rather radioactive decay several month due primarily existence hydrogen ejecta atmosphere supergiant progenitor star initial destruction hydrogen becomes heated ionised majority type ii supernova show prolonged plateau light curve hydrogen recombines emitting visible light becoming transparent followed declining light curve driven radioactive decay although slower type supernova due efficiency conversion light hydrogen type iil plateau absent progenitor relatively little hydrogen left atmosphere sufficient appear spectrum insufficient produce noticeable plateau light output type iib supernova hydrogen atmosphere progenitor depleted thought due tidal stripping companion star light curve closer type supernova hydrogen even disappears spectrum several week type iin supernova characterised additional narrow spectral line produced dense shell circumstellar material light curve generally broad extended occasionally also extremely luminous referred superluminous supernova light curve produced highly efficient conversion kinetic energy ejecta electromagnetic radiation interaction dense shell material occurs material sufficiently dense compact indicating produced progenitor star shortly supernova occurs large number supernova catalogued classified provide distance candle test model average characteristic vary somewhat distance type host galaxy broadly specified supernova type note asymmetry longstanding puzzle surrounding type ii supernova remaining compact object receives large velocity away epicentre pulsar thus neutron star observed high peculiar velocity black hole presumably well although far harder observe isolation initial impetus substantial propelling object solar mass velocity km greater indicates expansion asymmetry mechanism momentum transferred compact object remains puzzle proposed explanation kick include convection collapsing star asymmetric ejection matter neutron star formation asymmetrical neutrino emission one possible explanation asymmetry largescale convection core convection create radial variation density giving rise variation amount energy absorbed neutrino outflow however analysis mechanism predicts modest momentum transfer another possible explanation accretion gas onto central neutron star create disk drive highly directional jet propelling matter high velocity star driving transverse shock completely disrupt star jet might play crucial role resulting supernova similar model used explaining long gammaray burst dominant mechanism may depend upon mass progenitor star initial asymmetry also confirmed type ia supernova observation result may mean initial luminosity type supernova depends viewing angle however expansion becomes symmetrical passage time early asymmetry detectable measuring polarisation emitted light energy output although supernova primarily known luminous event electromagnetic radiation release almost minor sideeffect particularly case core collapse supernova emitted electromagnetic radiation tiny fraction total energy released event fundamental difference balance energy production different type supernova type ia white dwarf detonation energy directed heavy element synthesis kinetic energy ejecta core collapse supernova vast majority energy directed neutrino emission apparently power observed destruction neutrino escape star first minute following start collapse standard type ia supernova derive energy runaway nuclear fusion carbonoxygen white dwarf detail energetics still fully understood result ejection entire mass original star high kinetic energy around half solar mass mass ni generated silicon burning ni radioactive decay co beta plus decay half life six day gamma ray co decay beta plus positron path halflife day stable fe two process responsible electromagnetic radiation type ia supernova combination changing transparency ejected material produce rapidly declining light curve core collapse supernova average visually fainter type ia supernova total energy released far higher outlined following table core collapse supernova fallback onto black hole drive relativistic jet may produce brief energetic directional burst gamma ray also transfer substantial energy ejected material one scenario producing highluminosity supernova thought cause type ic hypernovae longduration gammaray burst relativistic jet brief fail penetrate stellar envelope lowluminosity gammaray burst may produced supernova may subluminous supernova occurs inside small dense cloud circumstellar material produce shock wave efficiently convert high fraction kinetic energy electromagnetic radiation even though initial energy entirely normal resulting supernova high luminosity extended duration since rely exponential radioactive decay type event may cause type iin hypernovae although pairinstability supernova core collapse supernova spectrum light curve similar type iip nature core collapse like giant type ia runaway fusion carbon oxygen silicon total energy released highestmass event comparable core collapse supernova neutrino production thought low hence kinetic electromagnetic energy released high core star much larger white dwarf amount radioactive nickel heavy element ejected core order magnitude higher consequently high visual luminosity progenitor supernova classification type closely tied type progenitor star time collapse occurrence type supernova depends star metallicity since affect strength stellar wind thereby rate star loses mass type ia supernova produced white dwarf star binary star system occur galaxy type core collapse supernova found galaxy undergoing current recent star formation since result shortlived massive star commonly found type sc spiral also arm spiral galaxy irregular galaxy especially starburst galaxy type ib ic supernova hypothesised produced core collapse massive star lost outer layer hydrogen helium either via strong stellar wind mass transfer companion normally occur region new star formation extremely rare elliptical galaxy progenitor type iin supernova also high rate mass loss period prior explosion type ic supernova observed occur region metalrich higher starformation rate average host galaxy table show progenitor main type core collapse supernova approximate proportion observed local neighbourhood number difficulty reconciling modelled observed stellar evolution leading core collapse supernova red supergiant progenitor vast majority core collapse supernova observed relatively low mass luminosity l respectively progenitor type ii supernova detected must considerably fainter presumably less massive discrepancy referred red supergiant problem first described stephen smartt also coined term performing volumelimited search supernova smartt et al found lower upper mass limit type iip supernova form respectively former consistent expected upper mass limit white dwarf progenitor form latter consistent massive star population local group upper limit red supergiant produce visible supernova explosion calculated thought higher mass red supergiant explode supernova instead evolve back towards hotter temperature several progenitor type iib supernova confirmed k g supergiant plus one supergiant yellow hypergiants lbvs proposed progenitor type iib supernova almost type iib supernova near enough observe shown progenitor blue supergiant form unexpectedly high proportion confirmed supernova progenitor partly due high luminosity easy detection single wolfrayet progenitor yet clearly identified model difficulty showing blue supergiant lose enough mass reach supernova without progressing different evolutionary stage one study shown possible route lowluminosity postred supergiant luminous blue variable collapse likely type iin supernova several example hot luminous progenitor type iin supernova detected sn gy sn jl apparently massive luminous star distant sn ip highly luminous progenitor likely lbv peculiar supernova whose exact nature disputed progenitor type ibc supernova observed constraint possible luminosity often lower known wc star wo star extremely rare visually relatively faint difficult say whether progenitor missing yet observed luminous progenitor securely identified despite numerous supernova observed near enough progenitor would clearly imaged population modelling show observed type ibc supernova could reproduced mixture single massive star strippedenvelope star interacting binary system continued lack unambiguous detection progenitor normal type ib ic supernova may due massive star collapsing directly black hole without supernova outburst supernova produced lowermass lowluminosity helium star binary system small number would rapidly rotating massive star likely corresponding highly energetic type icbl event associated longduration gammaray burst external impact supernova event generate heavier element scattered throughout surrounding interstellar medium expanding shock wave supernova trigger star formation galactic cosmic ray generated supernova explosion source heavy element supernova major source element interstellar medium oxygen rubidium though theoretical abundance element produced seen spectrum varies significantly depending various supernova type type ia supernova produce mainly silicon ironpeak element metal nickel iron core collapse supernova eject much smaller quantity ironpeak element type ia supernova larger mass light alpha element oxygen neon element heavier zinc latter especially true electron capture supernova bulk material ejected type ii supernova hydrogen helium heavy element produced nuclear fusion nucleus silicon photodisintegration rearrangement quasiequilibrium silicon burning nucleus ar ni rapid capture neutron rprocess supernova collapse element heavier iron rprocess produce highly unstable nucleus rich neutron rapidly beta decay stable form supernova rprocess reaction responsible half isotope element beyond iron although neutron star merger may main astrophysical source many element modern universe old asymptotic giant branch agb star dominant source dust oxide carbon sprocess element however early universe agb star formed supernova may main source dust role stellar evolution remnant many supernova consist compact object rapidly expanding shock wave material cloud material sweep surrounding interstellar medium free expansion phase last two century wave gradually undergoes period adiabatic expansion slowly cool mix surrounding interstellar medium period year big bang produced hydrogen helium trace lithium heavier element synthesised star supernova collision neutron star thus indirectly due supernova supernova tend enrich surrounding interstellar medium element hydrogen helium usually astronomer refer metal ejected element ultimately enrich molecular cloud site star formation thus stellar generation slightly different composition going almost pure mixture hydrogen helium metalrich composition supernova dominant mechanism distributing heavier element formed star period nuclear fusion different abundance element material form star important influence star life may influence possibility planet orbiting giant planet form around star higher metallicity kinetic energy expanding supernova remnant trigger star formation compressing nearby dense molecular cloud space increase turbulent pressure also prevent star formation cloud unable lose excess energy evidence daughter product shortlived radioactive isotope show nearby supernova helped determine composition solar system billion year ago may even triggered formation system fast radio burst frbs intense transient pulse radio wave typically last millisecond many explanation event proposed magnetars produced corecollapse supernova leading candidate cosmic ray supernova remnant thought accelerate large fraction galactic primary cosmic ray direct evidence cosmic ray production found small number remnant gamma ray piondecay detected supernova remnant ic w produced accelerated proton remnant impact interstellar material gravitational wave supernova potentially strong galactic source gravitational wave none far detected gravitational wave event far detected merger black hole neutron star probable remnant supernova like neutrino emission gravitational wave produced corecollapse supernova expected arrive without delay affect light consequently may provide information corecollapse process unavailable mean gravitationalwave signal predicted supernova model short duration lasting less second thus difficult detect using arrival neutrino signal may provide trigger identify time window seek gravitational wave helping distinguish latter background noise effect earth nearearth supernova supernova close enough earth noticeable effect biosphere depending upon type energy supernova could far lightyears away theorised trace past supernova might detectable earth form metal isotope signature rock stratum iron enrichment later reported deepsea rock pacific ocean elevated level nitrate ion found antarctic ice coincided supernova gamma ray supernova could boosted atmospheric level nitrogen oxide became trapped ice historically nearby supernova may influenced biodiversity life planet geological record suggest nearby supernova event led increase cosmic ray turn produced cooler climate greater temperature difference pole equator created stronger wind increased ocean mixing resulted transport nutrient shallow water along continental shelf led greater biodiversity type ia supernova thought potentially dangerous occur close enough earth supernova arise dim common white dwarf star binary system likely supernova affect earth occur unpredictably star system well studied closestknown candidate ik pegasi hr lightyears away observation suggest could long billion year white dwarf accrete critical mass required become type ia supernova according estimate type ii supernova would closer parsec lightyears destroy half earth ozone layer candidate closer lightyears milky way candidate next supernova milky way likely detectable even occurs far side galaxy likely produced collapse unremarkable red supergiant probable already catalogued infrared survey mass smaller chance next core collapse supernova produced different type massive star yellow hypergiant luminous blue variable wolfrayet chance next supernova type ia produced white dwarf calculated third core collapse supernova observable wherever occurs less likely progenitor ever observed even known exactly type ia progenitor system look like difficult detect beyond parsec total supernova rate milky way estimated per century although one actually observed several century statistically common variety corecollapse supernova type iip progenitor type red supergiant difficult identify supergiant final stage heavy element fusion core million year left mostmassive red supergiant shed atmosphere evolve wolfrayet star core collapse wolfrayet star end life wolfrayet phase within million year difficult identify closest core collapse one class expected thousand year exploding wo wolfrayet star known exhausted core helium eight known four milky way number close wellknown star identified possible core collapse supernova candidate highmass blue star spica rigel deneb red supergiant betelgeuse antares vv cephei yellow hypergiant rho cassiopeiae luminous blue variable eta carina already produced supernova impostor component blue supergiant wolfrayet star regor gamma velorum system mimosa acrux hadar beta centauri three bright star system southern constellation crux centaurus respectively contain blue star sufficient mass explode supernova others gained notoriety possible although likely progenitor gammaray burst example wr identification candidate type ia supernova much speculative binary accreting white dwarf might produce supernova although exact mechanism timescale still debated system faint difficult identify nova recurrent nova system conveniently advertise one example u scorpii see also kilonova neutron star merger list supernova list supernova remnant list supernova candidate quarknova hypothetical violent explosion resulting conversion neutron star quark star superluminous supernova supernova least ten time luminous standard supernova supernova fiction timeline white dwarf neutron star supernova chronological list development knowledge record collapsar star undergone gravitational collapse hypernova supernova ejects large mass unusually high velocity reference reading athem w alsabti paul murdin ed handbook supernova springer cham bibcodehsnbooka doi isbn branch wheeler j c supernova explosion springer p isbn iliadis christian nuclear physic star nd ed weinheim wileyvch doi isbn takahashi k sato k burrow thompson supernova neutrino neutrino oscillation mass progenitor star physical review arxivhepph bibcodephrvdkt doiphysrevd scid woosley e janka ht physic corecollapse supernova nature physic arxivastroph bibcodenatphw citeseerx doinphys scid external link tsvetkov yu pavlyuk n n bartunov pskovskii p sternberg astronomical institute supernova catalogue sternberg astronomical institute moscow university retrieved november searchable catalogue open supernova catalog github october openaccess catalog supernova light curve spectrum list supernova iau designation iau central bureau astronomical telegram retrieved october', 'deoxyribonucleic acid dna polymer composed two polynucleotide chain coil around form double helix polymer carry genetic instruction development functioning growth reproduction known organism many virus dna ribonucleic acid rna nucleic acid alongside protein lipid complex carbohydrate polysaccharide nucleic acid one four major type macromolecule essential known form life two dna strand known polynucleotides composed simpler monomeric unit called nucleotide nucleotide composed one four nitrogencontaining nucleobases cytosine c guanine g adenine thymine sugar called deoxyribose phosphate group nucleotide joined one another chain covalent bond known phosphodiester linkage sugar one nucleotide phosphate next resulting alternating sugarphosphate backbone nitrogenous base two separate polynucleotide strand bound together according base pairing rule c g hydrogen bond make doublestranded dna complementary nitrogenous base divided two group singleringed pyrimidine doubleringed purine dna pyrimidine thymine cytosine purine adenine guanine strand doublestranded dna store biological information information replicated two strand separate large part dna human noncoding meaning section serve pattern protein sequence two strand dna run opposite direction thus antiparallel attached sugar one four type nucleobases base sequence four nucleobases along backbone encodes genetic information rna strand created using dna strand template process called transcription dna base exchanged corresponding base except case thymine rna substitute uracil u genetic code rna strand specify sequence amino acid within protein process called translation within eukaryotic cell dna organized long structure called chromosome typical cell division chromosome duplicated process dna replication providing complete set chromosome daughter cell eukaryotic organism animal plant fungi protist store dna inside cell nucleus nuclear dna mitochondrion mitochondrial dna chloroplast chloroplast dna contrast prokaryote bacteria archaea store dna cytoplasm circular chromosome within eukaryotic chromosome chromatin protein histone compact organize dna compacting structure guide interaction dna protein helping control part dna transcribed property dna long polymer made repeating unit called nucleotide structure dna dynamic along length capable coiling tight loop shape specie composed two helical chain bound hydrogen bond chain coiled around axis pitch ngstrms nm pair chain radius nm according another study measured different solution dna chain measured nm wide one nucleotide unit measured nm long buoyant density dna gcm dna usually exist single strand instead pair strand held tightly together two long strand coil around shape double helix nucleotide contains segment backbone molecule hold chain together nucleobase interacts dna strand helix nucleobase linked sugar called nucleoside base linked sugar one phosphate group called nucleotide biopolymer comprising multiple linked nucleotide dna called polynucleotide backbone dna strand made alternating phosphate sugar group sugar dna deoxyribose pentose fivecarbon sugar sugar joined phosphate group form phosphodiester bond third fifth carbon atom adjacent sugar ring known end three prime end end five prime end carbon prime symbol used distinguish carbon atom base deoxyribose form glycosidic bond therefore dna strand normally one end phosphate group attached carbon ribose phosphoryl another end free hydroxyl group attached carbon ribose hydroxyl orientation carbon along sugarphosphate backbone confers directionality sometimes called polarity dna strand nucleic acid double helix direction nucleotide one strand opposite direction strand strand antiparallel asymmetric end dna strand said directionality five prime end three prime end end terminal phosphate group end terminal hydroxyl group one major difference dna rna sugar deoxyribose dna replaced related pentose sugar ribose rna dna double helix stabilized primarily two force hydrogen bond nucleotide basestacking interaction among aromatic nucleobases four base found dna adenine cytosine c guanine g thymine four base attached sugarphosphate form complete nucleotide shown adenosine monophosphate adenine pair thymine guanine pair cytosine forming gc base pair nucleobase classification nucleobases classified two type purine g fused five sixmembered heterocyclic compound pyrimidine sixmembered ring c fifth pyrimidine nucleobase uracil u usually take place thymine rna differs thymine lacking methyl group ring addition rna dna many artificial nucleic acid analogue created study property nucleic acid use biotechnology noncanonical base modified base occur dna first recognized methylcytosine found genome mycobacterium tuberculosis reason presence noncanonical base bacterial virus bacteriophage avoid restriction enzyme present bacteria enzyme system act least part molecular immune system protecting bacteria infection virus modification base cytosine adenine common modified dna base play vital role epigenetic control gene expression plant animal number noncanonical base known occur dna modification canonical base plus uracil modified adenine ncarbamoylmethyladenine nmethyadenine modified guanine deazaguanine methylguanine modified cytosine nmethylcytosine carboxylcytosine formylcytosine glycosylhydroxymethylcytosine hydroxycytosine methylcytosine modified thymidine glutamythymidine putrescinylthymine uracil modification base j uracil dihydroxypentauracil hydroxymethyldeoxyuracil others deoxyarchaeosine diaminopurine aminoadenine groove twin helical strand form dna backbone another double helix may found tracing space groove strand void adjacent base pair may provide binding site strand symmetrically located respect groove unequally sized major groove ngstrms nm wide minor groove nm width due larger width major groove edge base accessible major groove minor groove result protein transcription factor bind specific sequence doublestranded dna usually make contact side base exposed major groove situation varies unusual conformation dna within cell see major minor groove always named reflect difference width would seen dna twisted back ordinary b form base pairing dna double helix type nucleobase one strand bond one type nucleobase strand called complementary base pairing purine form hydrogen bond pyrimidine adenine bonding thymine two hydrogen bond cytosine bonding guanine three hydrogen bond arrangement two nucleotide binding together across double helix sixcarbon ring sixcarbon ring called watsoncrick base pair dna high gccontent stable dna low gccontent hoogsteen base pair hydrogen bonding carbon ring carbon ring rare variation basepairing hydrogen bond covalent broken rejoined relatively easily two strand dna double helix thus pulled apart like zipper either mechanical force high temperature result base pair complementarity information doublestranded sequence dna helix duplicated strand vital dna replication reversible specific interaction complementary base pair critical function dna organism ssdna v dsdna dna molecule actually two polymer strand bound together helical fashion noncovalent bond doublestranded dsdna structure maintained largely intrastrand base stacking interaction strongest gc stack two strand come aparta process known meltingto form two singlestranded dna ssdna molecule melting occurs high temperature low salt high ph low ph also melt dna since dna unstable due acid depurination low ph rarely used stability dsdna form depends gccontent gc basepairs also sequence since stacking sequence specific also length longer molecule stable stability measured various way common way melting temperature also called tm value temperature doublestrand molecule converted singlestrand molecule melting temperature dependent ionic strength concentration dna result percentage gc base pair overall length dna double helix determines strength association two strand dna long dna helix high gccontent strongly interacting strand short helix high content weakly interacting strand biology part dna double helix need separate easily tataat pribnow box promoter tend high content making strand easier pull apart laboratory strength interaction measured finding melting temperature tm necessary break half hydrogen bond base pair dna double helix melt strand separate exist solution two entirely independent molecule singlestranded dna molecule single common shape conformation stable others amount human total female diploid nuclear genome per cell extends gigabase pair gbp cm long weighs picograms pg male value gbp cm pg dna polymer contain hundred million nucleotide chromosome chromosome largest human chromosome approximately million base pair would mm long straightened eukaryote addition nuclear dna also mitochondrial dna mtdna encodes certain protein used mitochondrion mtdna usually relatively small comparison nuclear dna example human mitochondrial dna form closed circular molecule contains dna base pair molecule normally containing full set mitochondrial gene human mitochondrion contains average approximately mtdna molecule human cell contains approximately mitochondrion giving total number mtdna molecule per human cell approximately however amount mitochondrion per cell also varies cell type egg cell contain mitochondrion corresponding copy mitochondrial genome constituting dna cell sense antisense dna sequence called sense sequence messenger rna copy translated protein sequence opposite strand called antisense sequence sense antisense sequence exist different part strand dna ie strand contain sense antisense sequence prokaryote eukaryote antisense rna sequence produced function rna entirely clear one proposal antisense rna involved regulating gene expression rnarna base pairing dna sequence prokaryote eukaryote plasmid virus blur distinction sense antisense strand overlapping gene case dna sequence double duty encoding one protein read along one strand second protein read opposite direction along strand bacteria overlap may involved regulation gene transcription virus overlapping gene increase amount information encoded within small viral genome supercoiling dna twisted like rope process called dna supercoiling dna relaxed state strand usually circle axis double helix every base pair dna twisted strand become tightly loosely wound dna twisted direction helix positive supercoiling base held tightly together twisted opposite direction negative supercoiling base come apart easily nature dna slight negative supercoiling introduced enzyme called topoisomerases enzyme also needed relieve twisting stress introduced dna strand process transcription dna replication alternative dna structure dna exists many possible conformation include adna bdna zdna form although bdna zdna directly observed functional organism conformation dna adopts depends hydration level dna sequence amount direction supercoiling chemical modification base type concentration metal ion presence polyamines solution first published report adna xray diffraction patternsand also bdnaused analysis based patterson function provided limited amount structural information oriented fiber dna alternative analysis proposed wilkins et al vivo bdna xray diffractionscattering pattern highly hydrated dna fiber term square bessel function journal james watson francis crick presented molecular modeling analysis dna xray diffraction pattern suggest structure double helix although bdna form common condition found cell welldefined conformation family related dna conformation occur high hydration level present cell corresponding xray diffraction scattering pattern characteristic molecular paracrystals significant degree disorder compared bdna adna form wider righthanded spiral shallow wide minor groove narrower deeper major groove form occurs nonphysiological condition partly dehydrated sample dna cell may produced hybrid pairing dna rna strand enzymedna complex segment dna base chemically modified methylation may undergo larger change conformation adopt z form strand turn helical axis lefthanded spiral opposite common b form unusual structure recognized specific zdna binding protein may involved regulation transcription alternative dna chemistry many year exobiologists proposed existence shadow biosphere postulated microbial biosphere earth us radically different biochemical molecular process currently known life one proposal existence lifeforms use arsenic instead phosphorus dna report possibility bacterium gfaj announced though research disputed evidence suggests bacterium actively prevents incorporation arsenic dna backbone biomolecules quadruplex structure end linear chromosome specialized region dna called telomere main function region allow cell replicate chromosome end using enzyme telomerase enzyme normally replicate dna cannot copy extreme end chromosome specialized chromosome cap also help protect dna end stop dna repair system cell treating damage corrected human cell telomere usually length singlestranded dna containing several thousand repeat simple ttaggg sequence guaninerich sequence may stabilize chromosome end forming structure stacked set fourbase unit rather usual base pair found dna molecule four guanine base known guanine tetrad form flat plate flat fourbase unit stack top form stable gquadruplex structure structure stabilized hydrogen bonding edge base chelation metal ion centre fourbase unit structure also formed central set four base coming either single strand folded around base several different parallel strand contributing one base central structure addition stacked structure telomere also form large loop structure called telomere loop tloops singlestranded dna curl around long circle stabilized telomerebinding protein end tloop singlestranded telomere dna held onto region doublestranded dna telomere strand disrupting doublehelical dna base pairing one two strand triplestranded structure called displacement loop dloop branched dna dna fraying occurs noncomplementary region exist end otherwise complementary doublestrand dna however branched dna occur third strand dna introduced contains adjoining region able hybridize frayed region preexisting doublestrand although simplest example branched dna involves three strand dna complex involving additional strand multiple branch also possible branched dna used nanotechnology construct geometric shape see section us technology artificial base several artificial nucleobases synthesized successfully incorporated eightbase dna analogue named hachimoji dna dubbed b p z artificial base capable bonding predictable way sb pz maintain double helix structure dna transcribed rna existence could seen indication nothing special four natural nucleobases evolved earth hand dna tightly related rna act transcript dna also performs molecular machine many task cell purpose fold structure shown allow create possible structure least four base required corresponding rna higher number also possible would natural principle least effort acidity phosphate group dna give similar acidic property phosphoric acid considered strong acid fully ionized normal cellular ph releasing proton leave behind negative charge phosphate group negative charge protect dna breakdown hydrolysis repelling nucleophiles could hydrolyze macroscopic appearance pure dna extracted cell form white stringy clump chemical modification altered dna packaging base modification dna packaging expression gene influenced dna packaged chromosome structure called chromatin base modification involved packaging region low gene expression usually containing high level methylation cytosine base dna packaging influence gene expression also occur covalent modification histone protein core around dna wrapped chromatin structure else remodeling carried chromatin remodeling complex see chromatin remodeling crosstalk dna methylation histone modification coordinately affect chromatin gene expression one example cytosine methylation produce methylcytosine important xinactivation chromosome average level methylation varies organismsthe worm caenorhabditis elegans lack cytosine methylation vertebrate higher level dna containing methylcytosine despite importance methylcytosine deaminate leave thymine base methylated cytosine particularly prone mutation base modification include adenine methylation bacteria presence hydroxymethylcytosine brain glycosylation uracil produce jbase kinetoplastids damage dna damaged many sort mutagen change dna sequence mutagen include oxidizing agent alkylating agent also highenergy electromagnetic radiation ultraviolet light xrays type dna damage produced depends type mutagen example uv light damage dna producing thymine dimer crosslinks pyrimidine base hand oxidant free radical hydrogen peroxide produce multiple form damage including base modification particularly guanosine doublestrand break typical human cell contains base suffered oxidative damage oxidative lesion dangerous doublestrand break difficult repair produce point mutation insertion deletion dna sequence chromosomal translocation mutation cause cancer inherent limit dna repair mechanism human lived long enough would eventually develop cancer dna damage naturally occurring due normal cellular process produce reactive oxygen specie hydrolytic activity cellular water etc also occur frequently although damage repaired cell dna damage may remain despite action repair process remaining dna damage accumulate age mammalian postmitotic tissue accumulation appears important underlying cause aging many mutagen fit space two adjacent base pair called intercalation intercalators aromatic planar molecule example include ethidium bromide acridines daunomycin doxorubicin intercalator fit base pair base must separate distorting dna strand unwinding double helix inhibits transcription dna replication causing toxicity mutation result dna intercalators may carcinogen case thalidomide teratogen others benzoapyrene diol epoxide aflatoxin form dna adduct induce error replication nevertheless due ability inhibit dna transcription replication similar toxin also used chemotherapy inhibit rapidly growing cancer cell biological function dna usually occurs linear chromosome eukaryote circular chromosome prokaryote set chromosome cell make genome human genome approximately billion base pair dna arranged chromosome information carried dna held sequence piece dna called gene transmission genetic information gene achieved via complementary base pairing example transcription cell us information gene dna sequence copied complementary rna sequence attraction dna correct rna nucleotide usually rna copy used make matching protein sequence process called translation depends interaction rna nucleotide alternative fashion cell may copy genetic information process called dna replication detail function covered article focus interaction dna molecule mediate function genome gene genome genomic dna tightly orderly packed process called dna condensation fit small available volume cell eukaryote dna located cell nucleus small amount mitochondrion chloroplast prokaryote dna held within irregularly shaped body cytoplasm called nucleoid genetic information genome held within gene complete set information organism called genotype gene unit heredity region dna influence particular characteristic organism gene contain open reading frame transcribed regulatory sequence promoter enhancer control transcription open reading frame many specie small fraction total sequence genome encodes protein example human genome consists proteincoding exon human dna consisting noncoding repetitive sequence reason presence much noncoding dna eukaryotic genome extraordinary difference genome size cvalue among specie represent longstanding puzzle known cvalue enigma however dna sequence code protein may still encode functional noncoding rna molecule involved regulation gene expression noncoding dna sequence play structural role chromosome telomere centromere typically contain gene important function stability chromosome abundant form noncoding dna human pseudogenes copy gene disabled mutation sequence usually molecular fossil although occasionally serve raw genetic material creation new gene process gene duplication divergence transcription translation gene sequence dna contains genetic information influence phenotype organism within gene sequence base along dna strand defines messenger rna sequence defines one protein sequence relationship nucleotide sequence gene aminoacid sequence protein determined rule translation known collectively genetic code genetic code consists threeletter word called codon formed sequence three nucleotide eg act cag ttt transcription codon gene copied messenger rna rna polymerase rna copy decoded ribosome read rna sequence basepairing messenger rna transfer rna carry amino acid since base letter combination possible codon combination encode twenty standard amino acid giving amino acid one possible codon also three stop nonsense codon signifying end coding region tag taa tga codon uag uaa uga mrna replication cell division essential organism grow cell divide must replicate dna genome two daughter cell genetic information parent doublestranded structure dna provides simple mechanism dna replication two strand separated strand complementary dna sequence recreated enzyme called dna polymerase enzyme make complementary strand finding correct base complementary base pairing bonding onto original strand dna polymerase extend dna strand direction different mechanism used copy antiparallel strand double helix way base old strand dictate base appears new strand cell end perfect copy dna extracellular nucleic acid naked extracellular dna edna released cell death nearly ubiquitous environment concentration soil may high gl concentration natural aquatic environment may high gl various possible function proposed edna may involved horizontal gene transfer may provide nutrient may act buffer recruit titrate ion antibiotic extracellular dna act functional extracellular matrix component biofilms several bacterial specie may act recognition factor regulate attachment dispersal specific cell type biofilm may contribute biofilm formation may contribute biofilms physical strength resistance biological stress cellfree fetal dna found blood mother sequenced determine great deal information developing fetus name environmental dna edna seen increased use natural science survey tool ecology monitoring movement presence specie water air land assessing area biodiversity neutrophil extracellular trap neutrophil extracellular trap net network extracellular fiber primarily composed dna allow neutrophil type white blood cell kill extracellular pathogen minimizing damage host cell interaction protein function dna depend interaction protein protein interaction nonspecific protein bind specifically single dna sequence enzyme also bind dna polymerase copy dna base sequence transcription dna replication particularly important dnabinding protein structural protein bind dna wellunderstood example nonspecific dnaprotein interaction within chromosome dna held complex structural protein protein organize dna compact structure called chromatin eukaryote structure involves dna binding complex small basic protein called histone prokaryote multiple type protein involved histone form diskshaped complex called nucleosome contains two complete turn doublestranded dna wrapped around surface nonspecific interaction formed basic residue histone making ionic bond acidic sugarphosphate backbone dna thus largely independent base sequence chemical modification basic amino acid residue include methylation phosphorylation acetylation chemical change alter strength interaction dna histone making dna less accessible transcription factor changing rate transcription nonspecific dnabinding protein chromatin include highmobility group protein bind bent distorted dna protein important bending array nucleosomes arranging larger structure make chromosome distinct group dnabinding protein dnabinding protein specifically bind singlestranded dna human replication protein bestunderstood member family used process double helix separated including dna replication recombination dna repair binding protein seem stabilize singlestranded dna protect forming stemloops degraded nuclease contrast protein evolved bind particular dna sequence intensively studied various transcription factor protein regulate transcription transcription factor bind one particular set dna sequence activates inhibits transcription gene sequence close promoter transcription factor two way firstly bind rna polymerase responsible transcription either directly mediator protein locates polymerase promoter allows begin transcription alternatively transcription factor bind enzyme modify histone promoter change accessibility dna template polymerase dna target occur throughout organism genome change activity one type transcription factor affect thousand gene consequently protein often target signal transduction process control response environmental change cellular differentiation development specificity transcription factor interaction dna come protein making multiple contact edge dna base allowing read dna sequence baseinteractions made major groove base accessible dnamodifying enzyme nuclease ligases nuclease enzyme cut dna strand catalyzing hydrolysis phosphodiester bond nuclease hydrolyse nucleotide end dna strand called exonuclease endonuclease cut within strand frequently used nuclease molecular biology restriction endonuclease cut dna specific sequence instance ecorv enzyme shown left recognizes base sequence gatatc make cut horizontal line nature enzyme protect bacteria phage infection digesting phage dna enters bacterial cell acting part restriction modification system technology sequencespecific nuclease used molecular cloning dna fingerprinting enzyme called dna ligases rejoin cut broken dna strand ligases particularly important lagging strand dna replication join short segment dna produced replication fork complete copy dna template also used dna repair genetic recombination topoisomerases helicases topoisomerases enzyme nuclease ligase activity protein change amount supercoiling dna enzyme work cutting dna helix allowing one section rotate thereby reducing level supercoiling enzyme seal dna break type enzyme capable cutting one dna helix passing second strand dna break rejoining helix topoisomerases required many process involving dna dna replication transcription helicases protein type molecular motor use chemical energy nucleoside triphosphates predominantly adenosine triphosphate atp break hydrogen bond base unwind dna double helix single strand enzyme essential process enzyme need access dna base polymerase polymerase enzyme synthesize polynucleotide chain nucleoside triphosphates sequence product created based existing polynucleotide chainswhich called template enzyme function repeatedly adding nucleotide hydroxyl group end growing polynucleotide chain consequence polymerase work direction active site enzyme incoming nucleoside triphosphate basepairs template allows polymerase accurately synthesize complementary strand template polymerase classified according type template use dna replication dnadependent dna polymerase make copy dna polynucleotide chain preserve biological information essential sequence base copy precisely complementary sequence base template strand many dna polymerase proofreading activity polymerase recognizes occasional mistake synthesis reaction lack base pairing mismatched nucleotide mismatch detected exonuclease activity activated incorrect base removed organism dna polymerase function large complex called replisome contains multiple accessory subunit dna clamp helicases rnadependent dna polymerase specialized class polymerase copy sequence rna strand dna include reverse transcriptase viral enzyme involved infection cell retrovirus telomerase required replication telomere example hiv reverse transcriptase enzyme aid virus replication telomerase unusual polymerase contains rna template part structure synthesizes telomere end chromosome telomere prevent fusion end neighboring chromosome protect chromosome end damage transcription carried dnadependent rna polymerase copy sequence dna strand rna begin transcribing gene rna polymerase bind sequence dna called promoter separate dna strand copy gene sequence messenger rna transcript reach region dna called terminator halt detaches dna human dnadependent dna polymerase rna polymerase ii enzyme transcribes gene human genome operates part large protein complex multiple regulatory accessory subunit genetic recombination dna helix usually interact segment dna human cell different chromosome even occupy separate area nucleus called chromosome territory physical separation different chromosome important ability dna function stable repository information one time chromosome interact chromosomal crossover occurs sexual reproduction genetic recombination occurs chromosomal crossover two dna helix break swap section rejoin recombination allows chromosome exchange genetic information produce new combination gene increase efficiency natural selection important rapid evolution new protein genetic recombination also involved dna repair particularly cell response doublestrand break common form chromosomal crossover homologous recombination two chromosome involved share similar sequence nonhomologous recombination damaging cell produce chromosomal translocation genetic abnormality recombination reaction catalyzed enzyme known recombinases rad first step recombination doublestranded break caused either endonuclease damage dna series step catalyzed part recombinase lead joining two helix least one holliday junction segment single strand helix annealed complementary strand helix holliday junction tetrahedral junction structure moved along pair chromosome swapping one strand another recombination reaction halted cleavage junction religation released dna strand like polarity exchange dna recombination two type cleavage eastwest cleavage northsouth cleavage northsouth cleavage nick strand dna eastwest cleavage one strand dna intact formation holliday junction recombination make possible genetic diversity gene exchange chromosome expression wildtype viral genome evolution dna contains genetic information allows form life function grow reproduce however unclear long billionyear history life dna performed function proposed earliest form life may used rna genetic material rna may acted central part early cell metabolism transmit genetic information carry catalysis part ribozymes ancient rna world nucleic acid would used catalysis genetics may influenced evolution current genetic code based four nucleotide base would occur since number different base organism tradeoff small number base increasing replication accuracy large number base increasing catalytic efficiency ribozymes however direct evidence ancient genetic system recovery dna fossil impossible dna survives environment less one million year slowly degrades short fragment solution claim older dna made notably report isolation viable bacterium salt crystal million year old claim controversial building block dna adenine guanine related organic molecule may formed extraterrestrially outer space complex dna rna organic compound life including uracil cytosine thymine also formed laboratory condition mimicking found outer space using starting chemical pyrimidine found meteorite pyrimidine like polycyclic aromatic hydrocarbon pahs carbonrich chemical found universe may formed red giant interstellar cosmic dust gas cloud ancient dna recovered ancient organism timescale genome evolution directly observed including extinct organism million year old woolly mammoth us technology genetic engineering method developed purify dna organism phenolchloroform extraction manipulate laboratory restriction digest polymerase chain reaction modern biology biochemistry make intensive use technique recombinant dna technology recombinant dna manmade dna sequence assembled dna sequence transformed organism form plasmid appropriate format using viral vector genetically modified organism produced used produce product recombinant protein used medical research grown agriculture dna profiling forensic scientist use dna blood semen skin saliva hair found crime scene identify matching dna individual perpetrator process formally termed dna profiling also called dna fingerprinting dna profiling length variable section repetitive dna short tandem repeat minisatellites compared people method usually extremely reliable technique identifying matching dna however identification complicated scene contaminated dna several people dna profiling developed british geneticist sir alec jeffreys first used forensic science convict colin pitchfork enderby murder case development forensic science ability obtain genetic matching minute sample blood skin saliva hair led reexamining many case evidence uncovered scientifically impossible time original examination combined removal double jeopardy law place allow case reopened prior trial failed produce sufficient evidence convince jury people charged serious crime may required provide sample dna matching purpose obvious defense dna match obtained forensically claim crosscontamination evidence occurred resulted meticulous strict handling procedure new case serious crime dna profiling also used successfully positively identify victim mass casualty incident body body part serious accident individual victim mass war graf via matching family member dna profiling also used dna paternity testing determine someone biological parent grandparent child probability parentage typically alleged parent biologically related child normal dna sequencing method happen birth new method test paternity mother still pregnant dna enzyme catalytic dna deoxyribozymes also called dnazymes catalytic dna first discovered mostly single stranded dna sequence isolated large pool random dna sequence combinatorial approach called vitro selection systematic evolution ligand exponential enrichment selex dnazymes catalyze variety chemical reaction including rnadna cleavage rnadna ligation amino acid phosphorylationdephosphorylation carboncarbon bond formation etc dnazymes enhance catalytic rate chemical reaction fold uncatalyzed reaction extensively studied class dnazymes rnacleaving type used detect different metal ion designing therapeutic agent several metalspecific dnazymes reported including gr dnazyme leadspecific ca dnazymes copperspecific e dnazyme uranylspecific naa dnazyme sodiumspecific naa dnazyme reported fold selective sodium metal ion used make realtime sodium sensor cell bioinformatics bioinformatics involves development technique store data mine search manipulate biological data including dna nucleic acid sequence data led widely applied advance computer science especially string searching algorithm machine learning database theory string searching matching algorithm find occurrence sequence letter inside larger sequence letter developed search specific sequence nucleotide dna sequence may aligned dna sequence identify homologous sequence locate specific mutation make distinct technique especially multiple sequence alignment used studying phylogenetic relationship protein function data set representing entire genome worth dna sequence produced human genome project difficult use without annotation identify location gene regulatory element chromosome region dna sequence characteristic pattern associated protein rnacoding gene identified gene finding algorithm allow researcher predict presence particular gene product possible function organism even isolated experimentally entire genome may also compared shed light evolutionary history particular organism permit examination complex evolutionary event dna nanotechnology dna nanotechnology us unique molecular recognition property dna nucleic acid create selfassembling branched dna complex useful property dna thus used structural material rather carrier biological information led creation twodimensional periodic lattice tilebased using dna origami method threedimensional structure shape polyhedron nanomechanical device algorithmic selfassembly also demonstrated dna structure used template arrangement molecule gold nanoparticles streptavidin protein dna nucleic acid basis aptamers synthetic oligonucleotide ligand specific target molecule used range biotechnology biomedical application history anthropology dna collect mutation time inherited contains historical information comparing dna sequence geneticist infer evolutionary history organism phylogeny field phylogenetics powerful tool evolutionary biology dna sequence within specie compared population geneticist learn history particular population used study ranging ecological genetics anthropology information storage dna storage device information enormous potential since much higher storage density compared electronic device however high cost slow read write time memory latency insufficient reliability prevented practical use history dna first isolated swiss physician friedrich miescher discovered microscopic substance pu discarded surgical bandage resided nucleus cell called nuclein albrecht kossel isolated nonprotein component nuclein nucleic acid later isolated five primary nucleobases phoebus levene identified base sugar phosphate nucleotide unit rna named yeast nucleic acid levene identified deoxyribose sugar thymus nucleic acid dna levene suggested dna consisted string four nucleotide unit linked together phosphate group tetranucleotide hypothesis levene thought chain short base repeated fixed order nikolai koltsov proposed inherited trait would inherited via giant hereditary molecule made two mirror strand would replicate semiconservative fashion using strand template frederick griffith experiment discovered trait smooth form pneumococcus could transferred rough form bacteria mixing killed smooth bacteria live rough form system provided first clear suggestion dna carry genetic information studying virgin sea urchin egg jean brachet suggested dna found cell nucleus rna present exclusively cytoplasm time yeast nucleic acid rna thought occur plant thymus nucleic acid dna animal latter thought tetramer function buffering cellular ph william astbury produced first xray diffraction pattern showed dna regular structure oswald avery along coworkers colin macleod maclyn mccarty identified dna transforming principle supporting griffith suggestion averymacleodmccarty experiment erwin chargaff developed published observation known chargaffs rule stating dna specie organism amount guanine equal cytosine amount adenine equal thymine alec todd collaborator university cambridge determined biochemical method backbone dna structured via successive linking carbon atom sugar phosphate would help corroborate watson crick later xray structural work todd would later awarded nobel prize chemistry discovery related dna late francis crick started working james watson cavendish laboratory within university cambridge dna role heredity confirmed alfred hershey martha chase hersheychase experiment showed dna genetic material enterobacteria phage may raymond gosling graduate student working supervision rosalind franklin took xray diffraction image labeled photo high hydration level dna photo given watson crick maurice wilkins critical obtaining correct structure dna franklin told crick watson backbone outside linus pauling watson crick erroneous model chain inside base pointing outwards franklin identification space group dna crystal proved correct february linus pauling robert corey proposed model nucleic acid containing three intertwined chain phosphate near axis base outside watson crick completed model accepted first correct model double helix dna february crick interrupted patron lunchtime eagle pub cambridge england announce watson discovered secret life april issue journal nature published series five article giving watson crick doublehelix structure dna evidence supporting structure reported letter titled molecular structure nucleic acid structure deoxyribose nucleic acid said escaped notice specific pairing postulated immediately suggests possible copying mechanism genetic material letter followed letter franklin gosling first publication xray diffraction data original analysis method followed letter wilkins two colleague contained analysis vivo bdna xray pattern supported presence vivo watson crick structure april scientist based new evidence concluded rosalind franklin contributor equal player discovery process dna rather otherwise may presented subsequently time discovery franklin death watson crick wilkins jointly received nobel prize physiology medicine nobel prize awarded living recipient debate continues receive credit discovery influential presentation crick laid central dogma molecular biology foretold relationship dna rna protein articulated adaptor hypothesis final confirmation replication mechanism implied doublehelical structure followed meselsonstahl experiment work crick coworkers showed genetic code based nonoverlapping triplet base called codon allowing har gobind khorana robert w holley marshall warren nirenberg decipher genetic code finding represent birth molecular biology dna analysis first used criminal investigation police uk requested alec jeffreys university leicester prove disprove involvement particular case suspect claimed innocence matter although suspect already confessed committing recent rapemurder denying involvement similar crime committed three year earlier yet detail two case alike police concluded crime committed person however charge suspect dropped jeffreys dna testing exonerated suspect earlier murder one hed confessed dna profiling led positive identification another suspect colin pitchfork found guilty rapemurders see also reference reading external link dna binding site prediction protein dna double helix game official nobel prize web site dna electron microscope dolan dna learning center double helix year dna nature proteopedia dna proteopedia formsofdna encode thread explorer encode home page nature double helix national centre biotechnology education genetic education module teacher dna beginning study guide pdb molecule month dna clue chemistry heredity found new york time june first american newspaper coverage discovery dna structure dna beginning another dna learning center site dna gene heredity mendel human genome project register francis crick personal paper mandeville special collection library university california san diego sevenpage handwritten letter crick sent yearold son michael describing structure dna see crick medal go hammer nature april', 'photosynthesis fohtsinthsis system biological process photopigmentbearing autotrophic organism plant algae cyanobacteria convert light energy typically sunlight chemical energy necessary fuel metabolism term photosynthesis usually refers oxygenic photosynthesis process release oxygen byproduct water splitting photosynthetic organism store converted chemical energy within bond intracellular organic compound complex compound containing carbon typically carbohydrate like sugar mainly glucose fructose sucrose starch phytoglycogen cellulose needing use stored energy organism cell metabolize organic compound cellular respiration photosynthesis play critical role producing maintaining oxygen content earth atmosphere supply biological energy necessary complex life earth organism also perform anoxygenic photosynthesis produce oxygen bacteria eg purple bacteria us bacteriochlorophyll split hydrogen sulfide reductant instead water releasing sulfur instead oxygen dominant form photosynthesis euxinic canfield ocean boring billion archaea halobacterium also perform type noncarbonfixing anoxygenic photosynthesis simpler photopigment retinal microbial rhodopsin derivative used absorb green light produce proton hydron gradient across cell membrane subsequent ion movement power transmembrane proton pump directly synthesize adenosine triphosphate atp energy currency cell archaeal photosynthesis might earliest form photosynthesis evolved earth far back paleoarchean preceding cyanobacteria see purple earth hypothesis detail may differ specie process always begin light energy absorbed reaction center protein contain photosynthetic pigment chromophore plant pigment chlorophyll porphyrin derivative absorbs red blue spectrum light thus reflecting green held inside chloroplast abundant leaf cell cyanobacteria embedded plasma membrane lightdependent reaction energy used strip electron suitable substance water producing oxygen gas hydrogen freed splitting water used creation two important molecule participate energetic process reduced nicotinamide adenine dinucleotide phosphate nadph atp plant algae cyanobacteria sugar synthesized subsequent sequence lightindependent reaction called calvin cycle process atmospheric carbon dioxide incorporated already existing organic compound ribulose bisphosphate rubp using atp nadph produced lightdependent reaction resulting compound reduced removed form carbohydrate glucose bacteria different mechanism like reverse krebs cycle used achieve end first photosynthetic organism probably evolved early evolutionary history life using reducing agent hydrogen hydrogen sulfide rather water source electron cyanobacteria appeared later excess oxygen produced contributed directly oxygenation earth rendered evolution complex life possible average rate energy captured global photosynthesis approximately terawatts eight time total power consumption human civilization photosynthetic organism also convert around billion ton pg petagrams billion metric ton carbon biomass per year photosynthesis discovered jan ingenhousz showed plant need light soil water overview photosynthetic organism photoautotrophs mean able synthesize food directly carbon dioxide water using energy light however organism use carbon dioxide source carbon atom carry photosynthesis photoheterotrophs use organic compound rather carbon dioxide source carbon plant algae cyanobacteria photosynthesis release oxygen oxygenic photosynthesis far common type photosynthesis used living organism shadeloving plant sciophytes produce low level oxygen photosynthesis use instead releasing atmosphere although difference oxygenic photosynthesis plant algae cyanobacteria overall process quite similar organism also many variety anoxygenic photosynthesis used mostly bacteria consume carbon dioxide release oxygen produce elemental sulfur instead molecular oxygen carbon dioxide converted sugar process called carbon fixation photosynthesis capture energy sunlight convert carbon dioxide carbohydrate carbon fixation endothermic redox reaction general outline photosynthesis opposite cellular respiration photosynthesis process reduction carbon dioxide carbohydrate cellular respiration oxidation carbohydrate nutrient carbon dioxide nutrient used cellular respiration include carbohydrate amino acid fatty acid nutrient oxidized produce carbon dioxide water release chemical energy drive organism metabolism photosynthesis cellular respiration distinct process take place different sequence chemical reaction different cellular compartment cellular respiration mitochondrion general equation photosynthesis first proposed cornelis van niel cocarbondioxide haelectron donor photonslight energy chocarbohydrate aoxidizedelectrondonor howater since water used electron donor oxygenic photosynthesis equation process cocarbondioxide howater photonslight energy chocarbohydrate ooxygen howater equation emphasizes water reactant lightdependent reaction product lightindependent reaction canceling n water molecule side give net equation cocarbondioxide ho water photonslight energy chocarbohydrate oxygen process substitute compound arsenite water electronsupply role example microbe use sunlight oxidize arsenite arsenate equation reaction cocarbondioxide asoarsenite photonslight energy asoarsenate cocarbonmonoxideused build compound subsequent reaction photosynthesis occurs two stage first stage lightdependent reaction light reaction capture energy light use make hydrogen carrier nadph energystorage molecule atp second stage lightindependent reaction use product capture reduce carbon dioxide organism use oxygenic photosynthesis use visible light lightdependent reaction although least three use shortwave infrared specifically farred radiation organism employ even radical variant photosynthesis archaea use simpler method employ pigment similar used vision animal bacteriorhodopsin change configuration response sunlight acting proton pump produce proton gradient directly converted chemical energy process involve carbon dioxide fixation release oxygen seems evolved separately common type photosynthesis photosynthetic membrane organelle photosynthetic bacteria protein gather light photosynthesis embedded cell membrane simplest form involves membrane surrounding cell however membrane may tightly folded cylindrical sheet called thylakoids bunched round vesicle called intracytoplasmic membrane structure fill interior cell giving membrane large surface area therefore increasing amount light bacteria absorb plant algae photosynthesis take place organelle called chloroplast typical plant cell contains chloroplast chloroplast enclosed membrane membrane composed phospholipid inner membrane phospholipid outer membrane intermembrane space enclosed membrane aqueous fluid called stroma embedded within stroma stack thylakoids grana site photosynthesis thylakoids appear flattened disk thylakoid enclosed thylakoid membrane within enclosed volume lumen thylakoid space embedded thylakoid membrane integral peripheral membrane protein complex photosynthetic system plant absorb light primarily using pigment chlorophyll green part light spectrum absorbed reflected reason plant green color besides chlorophyll plant also use pigment carotene xanthophyll algae also use chlorophyll various pigment present phycocyanin carotene xanthophyll green algae phycoerythrin red algae rhodophytes fucoxanthin brown algae diatom resulting wide variety color pigment embedded plant algae complex called antenna protein protein pigment arranged work together combination protein also called lightharvesting complex although cell green part plant chloroplast majority found specially adapted structure called leaf certain specie adapted condition strong sunlight aridity many euphorbia cactus specie main photosynthetic organ stem cell interior tissue leaf called mesophyll contain chloroplast every square millimeter leaf surface leaf coated waterresistant waxy cuticle protects leaf excessive evaporation water decrease absorption ultraviolet blue light minimize heating transparent epidermis layer allows light pas palisade mesophyll cell photosynthesis take place lightdependent reaction lightdependent reaction one molecule pigment chlorophyll absorbs one photon loses one electron electron taken modified form chlorophyll called pheophytin pass electron quinone molecule starting flow electron electron transport chain lead ultimate reduction nadp nadph addition creates proton gradient energy gradient across chloroplast membrane used atp synthase synthesis atp chlorophyll molecule ultimately regains electron lost water molecule split process called photolysis release oxygen overall equation lightdependent reaction condition noncyclic electron flow green plant wavelength light support photosynthesis photosynthetic action spectrum depends type accessory pigment present example green plant action spectrum resembles absorption spectrum chlorophyll carotenoid absorption peak violetblue red light red algae action spectrum bluegreen light allows algae use blue end spectrum grow deeper water filter longer wavelength red light used aboveground green plant nonabsorbed part light spectrum give photosynthetic organism color eg green plant red algae purple bacteria least effective photosynthesis respective organism z scheme plant lightdependent reaction occur thylakoid membrane chloroplast drive synthesis atp nadph lightdependent reaction two form cyclic noncyclic noncyclic reaction photon captured lightharvesting antenna complex photosystem ii chlorophyll accessory pigment see diagram zscheme absorption photon antenna complex loosens electron process called photoinduced charge separation antenna system core chlorophyll molecule photosystem ii reaction center loosened electron taken primary electronacceptor molecule pheophytin electron shuttled electron transport chain socalled zscheme shown diagram chemiosmotic potential generated pumping proton cation h across membrane thylakoid space atp synthase enzyme us chemiosmotic potential make atp photophosphorylation whereas nadph product terminal redox reaction zscheme electron enters chlorophyll molecule photosystem excited light absorbed photosystem electron passed along chain electron acceptor transfer energy energy delivered electron acceptor used move hydrogen ion across thylakoid membrane lumen electron eventually used reduce coenzyme nadp h nadph function lightindependent reaction point path electron end cyclic reaction similar noncyclic differs generates atp reduced nadp nadph created cyclic reaction take place photosystem electron displaced photosystem electron passed electron acceptor molecule return photosystem emitted hence name cyclic reaction water photolysis linear electron transport photosystem leave reaction center photosystem oxidized elevating another electron first require rereduction reaction center excited electron lost reaction center p photosystem replaced transfer plastocyanin whose electron come electron transport photosystem ii photosystem ii first step zscheme requires external source electron reduce oxidized chlorophyll reaction center source electron photosynthesis green plant cyanobacteria water two water molecule oxidized energy four successive chargeseparation reaction photosystem ii yield molecule diatomic oxygen four hydrogen ion electron yielded transferred redoxactive tyrosine residue oxidized energy p reset ability p absorb another photon release another photodissociated electron oxidation water catalyzed photosystem ii redoxactive structure contains four manganese ion calcium ion oxygenevolving complex bind two water molecule contains four oxidizing equivalent used drive wateroxidizing reaction koks sstate diagram hydrogen ion released thylakoid lumen therefore contribute transmembrane chemiosmotic potential lead atp synthesis oxygen waste product lightdependent reaction majority organism earth use oxygen energy cellular respiration including photosynthetic organism lightindependent reaction calvin cycle lightindependent dark reaction enzyme rubisco capture co atmosphere process called calvin cycle us newly formed nadph release threecarbon sugar later combined form sucrose starch overall equation lightindependent reaction green plant carbon fixation produce threecarbon sugar intermediate converted final carbohydrate product simple carbon sugar photosynthesis produce used form organic compound building material cellulose precursor lipid amino acid biosynthesis fuel cellular respiration latter occurs plant also animal carbon energy plant passed food chain fixation reduction carbon dioxide process carbon dioxide combine fivecarbon sugar ribulose bisphosphate yield two molecule threecarbon compound glycerate phosphate also known phosphoglycerate glycerate phosphate presence atp nadph produced lightdependent stage reduced glyceraldehyde phosphate product also referred phosphoglyceraldehyde pgal generically triose phosphate five six molecule glyceraldehyde phosphate produced used regenerate ribulose bisphosphate process continue triose phosphate thus recycled often condense form hexose phosphate ultimately yield sucrose starch cellulose well glucose fructose sugar produced carbon metabolism yield carbon skeleton used metabolic reaction like production amino acid lipid carbon concentrating mechanism land hot dry condition plant close stoma prevent water loss condition co decrease oxygen gas produced light reaction photosynthesis increase causing increase photorespiration oxygenase activity ribulosebisphosphate carboxylaseoxygenase rubisco decrease carbon fixation plant evolved mechanism increase co concentration leaf condition plant use c carbon fixation process chemically fix carbon dioxide cell mesophyll adding threecarbon molecule phosphoenolpyruvate pep reaction catalyzed enzyme called pep carboxylase creating fourcarbon organic acid oxaloacetic acid oxaloacetic acid malate synthesized process translocated specialized bundle sheath cell enzyme rubisco calvin cycle enzyme located co released decarboxylation fourcarbon acid fixed rubisco activity threecarbon phosphoglyceric acid physical separation rubisco oxygengenerating light reaction reduces photorespiration increase co fixation thus photosynthetic capacity leaf c plant produce sugar c plant condition high light temperature many important crop plant c plant including maize sorghum sugarcane millet plant use pepcarboxylase carbon fixation called c plant primary carboxylation reaction catalyzed rubisco produce threecarbon phosphoglyceric acid directly calvinbenson cycle plant use c carbon fixation compared use c carbon fixation however evolution c sixty plant lineage make striking example convergent evolution c photosynthesis involves carbonconcentration selective breakdown photorespiratory glycine evolutionary precursor c useful carbonconcentrating mechanism right xerophyte cactus succulent also use pep carboxylase capture carbon dioxide process called crassulacean acid metabolism cam contrast c metabolism spatially separate co fixation pep calvin cycle cam temporally separate two process cam plant different leaf anatomy c plant fix co night stoma open cam plant store co mostly form malic acid via carboxylation phosphoenolpyruvate oxaloacetate reduced malate decarboxylation malate day release co inside leaf thus allowing carbon fixation phosphoglycerate rubisco cam used specie plant calciumoxalateaccumulating plant amaranthus hybridus colobanthus quitensis show variation photosynthesis calcium oxalate crystal function dynamic carbon pool supplying carbon dioxide co photosynthetic cell stoma partially totally closed process named alarm photosynthesis stress condition eg water deficit oxalate released calcium oxalate crystal converted co oxalate oxidase enzyme produced co support calvin cycle reaction reactive hydrogen peroxide ho byproduct oxalate oxidase reaction neutralized catalase alarm photosynthesis represents photosynthetic variant added wellknown c cam pathway however alarm photosynthesis contrast pathway operates biochemical pump collect carbon organ interior soil atmosphere water cyanobacteria possess carboxysomes increase concentration co around rubisco increase rate photosynthesis enzyme carbonic anhydrase located within carboxysome release co dissolved hydrocarbonate ion hco co diffuse rubisco concentrated within carboxysome quickly sponge hco ion made co outside cell another carbonic anhydrase actively pumped cell membrane protein cannot cross membrane charged within cytosol turn back co slowly without help carbonic anhydrase cause hco ion accumulate within cell diffuse carboxysomes pyrenoids algae hornwort also act concentrate co around rubisco order kinetics overall process photosynthesis take place four stage efficiency plant usually convert light chemical energy photosynthetic efficiency absorbed light unconverted dissipated primarily heat small fraction reemitted chlorophyll fluorescence longer redder wavelength fact allows measurement light reaction photosynthesis using chlorophyll fluorometers actual plant photosynthetic efficiency varies frequency light converted light intensity temperature proportion carbon dioxide atmosphere vary comparison solar panel convert light electric energy efficiency approximately massproduced panel laboratory device scientist studying photosynthesis hope developing plant increased yield efficiency light dark reaction measured relationship two complex example light reaction creates atp nadph energy molecule c plant use carbon fixation photorespiration electron may also flow electron sink reason uncommon author differentiate work done nonphotorespiratory condition photorespiratory condition chlorophyll fluorescence photosystem ii measure light reaction infrared gas analyzer measure dark reaction integrated chlorophyll fluorometer gas exchange system investigate light dark reaction researcher use two separate system together infrared gas analyzer moisture sensor sensitive enough measure photosynthetic assimilation co ho using reliable method co commonly measured molsms part per million volume per million ho commonly measured mmolsms mbars measuring co assimilation ho leaf temperature barometric pressure leaf area photosynthetically active radiation par becomes possible estimate carbon assimilation e transpiration g stomatal conductance ci intracellular co however common use chlorophyll fluorescence plant stress measurement appropriate commonly used parameter fvfm yii ffm measured second allowing investigation larger plant population gas exchange system offer control co level ambient allow common practice measurement aci curve different co level characterize plant photosynthetic response integrated chlorophyll fluorometer gas exchange system allow precise measure photosynthetic response mechanism standard gas exchange photosynthesis system measure ci substomatal co level addition integrated chlorophyll fluorescence measurement allows precise measurement cc estimation co concentration site carboxylation chloroplast replace ci co concentration chloroplast becomes possible estimate measurement mesophyll conductance gm using integrated system photosynthesis measurement system designed directly measure amount light leaf absorbs analysis chlorophyll fluorescence p pabsorbance gas exchange measurement reveal detailed information eg photosystems quantum efficiency co assimilation rate instrument even wavelength dependency photosynthetic efficiency analyzed phenomenon known quantum walk increase efficiency energy transport light significantly photosynthetic cell alga bacterium plant lightsensitive molecule called chromophore arranged antennashaped structure called photocomplex photon absorbed chromophore converted quasiparticle referred exciton jump chromophore chromophore towards reaction center photocomplex collection molecule trap energy chemical form accessible cell metabolism excitons wave property enable cover wider area try several possible path simultaneously allowing instantaneously choose efficient route highest probability arriving destination minimum possible time quantum walking take place temperature far higher quantum phenomenon usually occur possible short distance obstacle form destructive interference cause particle lose wave property instant regains freed locked position classic hop movement electron towards photo center therefore covered series conventional hop quantum walk evolution fossil thought filamentous photosynthetic organism dated billion year old recent study also suggest photosynthesis may begun billion year ago though first direct evidence photosynthesis come thylakoid membrane preserved billionyearold chert oxygenic photosynthesis main source oxygen earth atmosphere earliest appearance sometimes referred oxygen catastrophe geological evidence suggests oxygenic photosynthesis cyanobacteria became important paleoproterozoic era around two billion year ago modern photosynthesis plant photosynthetic prokaryote oxygenic using water electron donor oxidized molecular oxygen photosynthetic reaction center symbiosis origin chloroplast several group animal formed symbiotic relationship photosynthetic algae common coral sponge sea anemone scientist presume due particularly simple body plan large surface area animal compared volume addition marine mollusk elysia viridis elysia chlorotica also maintain symbiotic relationship chloroplast capture algae diet store body see kleptoplasty allows mollusk survive solely photosynthesis several month time gene plant cell nucleus even transferred slug chloroplast supplied protein need survive even closer form symbiosis may explain origin chloroplast chloroplast many similarity photosynthetic bacteria including circular chromosome prokaryotictype ribosome similar protein photosynthetic reaction center endosymbiotic theory suggests photosynthetic bacteria acquired endocytosis early eukaryotic cell form first plant cell therefore chloroplast may photosynthetic bacteria adapted life inside plant cell like mitochondrion chloroplast possess dna separate nuclear dna plant host cell gene chloroplast dna resemble found cyanobacteria dna chloroplast code redox protein found photosynthetic reaction center corr hypothesis proposes colocation gene gene product required redox regulation gene expression account persistence dna bioenergetic organelle photosynthetic eukaryotic lineage symbiotic kleptoplastic organism excluded glaucophytes red green algaeclade archaeplastida uni multicellular cryptophytesclade cryptista unicellular haptophytesclade haptista unicellular dinoflagellate chromerids superphylum myzozoa pseudoblepharisma phylum ciliophoraclade alveolata unicellular ochrophytesclade stramenopila uni multicellular chlorarachniophytes three specie paulinella phylum cercozoaclade rhizaria unicellular euglenidsclade excavata unicellular except euglenid found within excavata belong diaphoretickes archaeplastida photosynthetic paulinella got plastid surrounded two membrane primary endosymbiosis two separate event engulfing cyanobacterium plastid group either red green algal origin referred red lineage green lineage known exception ciliate pseudoblepharisma tenue addition plastid originated green algae also purple sulfur bacterium symbiont dinoflagellate euglenid plastid surrounded three membrane remaining line four nucleomorph remnant original algal nucleus located inner outer membrane plastid present cryptophyte red alga chlorarachniophytes green alga dinoflagellate lost photosynthetic ability later regained new endosymbiotic event different algae able perform photosynthesis many eukaryotic group mixotrophs practice heterotrophy various degree photosynthetic prokaryotic lineage early photosynthetic system green purple sulfur green purple nonsulfur bacteria thought anoxygenic used various molecule water electron donor green purple sulfur bacteria thought used hydrogen sulfur electron donor green nonsulfur bacteria used various amino organic acid electron donor purple nonsulfur bacteria used variety nonspecific organic molecule use molecule consistent geological evidence earth early atmosphere highly reducing time possible exception heimdallarchaeota photosynthesis found archaea haloarchaea photoheterotrophic absorb energy sun harvest carbon atmosphere therefore photosynthetic instead chlorophyll use rhodopsin convert lightenergy ion gradient cannot mediate electron transfer reaction bacteria eight photosynthetic lineage currently known cyanobacteria prokaryote performing oxygenic photosynthesis prokaryote contain two type photosystems type rci also known fe type type ii rcii also known quinone type seven remaining prokaryote anoxygenic photosynthesis use version either type type ii chlorobi green sulfur bacteria type heliobacteria type chloracidobacterium type proteobacteria purple sulfur bacteria purple nonsulfur bacteria type ii see purple bacteria chloroflexota green nonsulfur bacteria type ii gemmatimonadota type ii eremiobacterota type ii cyanobacteria evolution photosynthesis biochemical capacity use water source electron photosynthesis evolved common ancestor extant cyanobacteria formerly called bluegreen algae geological record indicates transforming event took place early earth history least million year ago speculated much earlier earth atmosphere contained almost oxygen estimated development photosynthesis believed first photosynthetic cyanobacteria generate oxygen available evidence geobiological study archean sedimentary rock indicates life existed question oxygenic photosynthesis evolved still unanswered clear paleontological window cyanobacterial evolution opened revealing alreadydiverse biota cyanobacteria cyanobacteria remained principal primary producer oxygen throughout proterozoic eon part redox structure ocean favored photoautotrophs capable nitrogen fixation green algae joined cyanobacteria major primary producer oxygen continental shelf near end proterozoic mesozoic radiation dinoflagellate coccolithophorids diatom primary production oxygen marine shelf water take modern form cyanobacteria remain critical marine ecosystem primary producer oxygen oceanic gyre agent biological nitrogen fixation modified form plastid marine algae experimental history discovery although step photosynthesis still completely understood overall photosynthetic equation known since th century jan van helmont began research process midth century carefully measured mass soil plant using mass plant grew noticing soil mass changed little hypothesized mass growing plant must come water substance added potted plant hypothesis partially accurate much gained mass come carbon dioxide well water however signaling point idea bulk plant biomass come input photosynthesis soil joseph priestley chemist minister discovered isolated volume air inverted jar burned candle gave co candle would burn quickly much ran wax discovered mouse could similarly injure air showed plant could restore air candle mouse injured jan ingenhousz repeated priestley experiment discovered influence sunlight plant could cause revive mouse matter hour jean senebier swiss pastor botanist naturalist demonstrated green plant consume carbon dioxide release oxygen influence light soon afterward nicolasthodore de saussure showed increase mass plant grows could due uptake co also incorporation water thus basic reaction organism use photosynthesis produce food glucose outlined refinement cornelis van niel made key discovery explaining chemistry photosynthesis studying purple sulfur bacteria green bacteria first demonstrate photosynthesis lightdependent redox reaction hydrogen reduces donates atom electron proton carbon dioxide robert emerson discovered two light reaction testing plant productivity using different wavelength light red alone light reaction suppressed blue red combined output much substantial thus two photosystems one absorbing nm wavelength nm former known psii latter psi psi contains chlorophyll psii contains primarily chlorophyll available chlorophyll b among pigment include phycobilin red blue pigment red blue algae respectively fucoxanthol brown algae diatom process productive absorption quantum equal psii psi assuring input energy antenna complex divided psi psii system turn power photochemistry robert hill thought complex reaction consisted intermediate cytochrome b plastoquinone another cytochrome f step carbohydrategenerating mechanism linked plastoquinone require energy reduce cytochrome f experiment prove oxygen developed photosynthesis green plant came water performed hill showed isolated chloroplast give oxygen presence unnatural reducing agent like iron oxalate ferricyanide benzoquinone exposure light hill reaction ho light chloroplast ah electron acceptor therefore light electron acceptor reduced oxygen evolved samuel ruben martin kamen used radioactive isotope determine oxygen liberated photosynthesis came water melvin calvin andrew benson along james bassham elucidated path carbon assimilation photosynthetic carbon reduction cycle plant carbon reduction cycle known calvin cycle many scientist refer calvinbenson bensoncalvin even calvinbensonbassham cbb cycle nobel prizewinning scientist rudolph marcus later able discover function significance electron transport chain otto heinrich warburg dean burk discovered iquantum photosynthesis reaction split co activated respiration first experimental evidence existence photophosphorylation vivo presented otto kandler using intact chlorella cell interpreting finding lightdependent atp formation daniel arnon et al discovered photophosphorylation vitro isolated chloroplast help p louis n duysens jan amesz discovered chlorophyll absorb one light oxidize cytochrome f chlorophyll pigment absorb another light reduce oxidized cytochrome stating two light reaction series development concept american botanist charles reid barnes proposed two term photosyntax photosynthesis biological process synthesis complex carbon compound carbonic acid presence chlorophyll influence light term photosynthesis derived greek ph gleam snthesis arranging together another word designated photosyntax sntaxis configuration time term photosynthesis came common usage later discovery anoxygenic photosynthetic bacteria photophosphorylation necessitated redefinition term c c photosynthesis research late university california berkeley detail photosynthetic carbon metabolism sorted chemist melvin calvin andrew benson james bassham score student researcher utilizing carbon isotope paper chromatography technique pathway co fixation algae chlorella fraction second light resulted three carbon molecule called phosphoglyceric acid pga original groundbreaking work nobel prize chemistry awarded melvin calvin parallel plant physiologist studied leaf gas exchange using new method infrared gas analysis leaf chamber net photosynthetic rate ranged mol coms conclusion terrestrial plant photosynthetic capacity light saturated less sunlight later cornell university field grown maize reported much greater leaf photosynthetic rate mol coms saturated near full sunlight higher rate maize almost double observed specie wheat soybean indicating large difference photosynthesis exist among higher plant university arizona detailed gas exchange research specie monocot dicot uncovered first time difference leaf anatomy crucial factor differentiating photosynthetic capacity among specie tropical grass including maize sorghum sugarcane bermuda grass dicot amaranthus leaf photosynthetic rate around mol coms leaf two type green cell ie outer layer mesophyll cell surrounding tightly packed cholorophyllous vascular bundle sheath cell type anatomy termed kranz anatomy th century botanist gottlieb haberlandt studying leaf anatomy sugarcane plant specie greatest photosynthetic rate kranz anatomy showed apparent photorespiration low co compensation point high optimum temperature high stomatal resistance lower mesophyll resistance gas diffusion rate never saturated full sun light research arizona designated citation classic specie later termed c plant first stable compound co fixation light four carbon malate aspartate specie lack kranz anatomy termed c type cotton sunflower first stable carbon compound threecarbon pga ppm co measuring air c c plant similar leaf photosynthetic rate around mol coms indicating suppression photorespiration c plant factor four main factor influencing photosynthesis several corollary factor four main light irradiance wavelength water absorption carbon dioxide concentration temperature total photosynthesis limited range environmental factor include amount light available amount leaf area plant capture light shading plant major limitation photosynthesis rate carbon dioxide supplied chloroplast support photosynthesis availability water availability suitable temperature carrying photosynthesis light intensity irradiance wavelength temperature process photosynthesis provides main input free energy biosphere one four main way radiation important plant life radiation climate within plant community extremely variable time space early th century frederick blackman gabrielle matthaei investigated effect light intensity irradiance temperature rate carbon assimilation constant temperature rate carbon assimilation varies irradiance increasing irradiance increase reaching plateau higher irradiance low irradiance increasing temperature little influence rate carbon assimilation constant high irradiance rate carbon assimilation increase temperature increased two experiment illustrate several important point first known general photochemical reaction affected temperature however experiment clearly show temperature affect rate carbon assimilation must two set reaction full process carbon assimilation lightdependent photochemical temperatureindependent stage lightindependent temperaturedependent stage second blackmans experiment illustrate concept limiting factor another limiting factor wavelength light cyanobacteria reside several meter underwater cannot receive correct wavelength required cause photoinduced charge separation conventional photosynthetic pigment combat problem cyanobacteria lightharvesting complex called phycobilisome complex made series protein different pigment surround reaction center carbon dioxide level photorespiration carbon dioxide concentration rise rate sugar made lightindependent reaction increase limited factor rubisco enzyme capture carbon dioxide lightindependent reaction binding affinity carbon dioxide oxygen concentration carbon dioxide high rubisco fix carbon dioxide however carbon dioxide concentration low rubisco bind oxygen instead carbon dioxide process called photorespiration us energy produce sugar rubisco oxygenase activity disadvantageous plant several reason one product oxygenase activity phosphoglycolate carbon instead phosphoglycerate carbon phosphoglycolate cannot metabolized calvinbenson cycle represents carbon lost cycle high oxygenase activity therefore drain sugar required recycle ribulose bisphosphate continuation calvinbenson cycle phosphoglycolate quickly metabolized glycolate toxic plant high concentration inhibits photosynthesis salvaging glycolate energetically expensive process us glycolate pathway carbon returned calvinbenson cycle phosphoglycerate reaction also produce ammonia nh able diffuse plant leading loss nitrogen highly simplified summary glycolate atp phosphoglycerate carbon dioxide adp nh salvaging pathway product rubisco oxygenase activity commonly known photorespiration since characterized lightdependent oxygen consumption release carbon dioxide see also reference reading book paper external link collection photosynthesis page level renowned expert govindjee depth advanced treatment photosynthesis also govindjee science aid photosynthesis article appropriate high school science metabolism cellular respiration photosynthesis virtual library biochemistry cell biology overall examination photosynthesis intermediate level overall energetics photosynthesis source oxygen produced photosynthesis interactive animation textbook tutorial marshall j first practical artificial leaf make debut discovery news archived original retrieved photosynthesis light dependent light independent stage archived wayback machine khan academy video introduction', 'evolution change heritable characteristic biological population successive generation occurs evolutionary process natural selection genetic drift act genetic variation resulting certain characteristic becoming less common within population successive generation process evolution given rise biodiversity every level biological organisation scientific theory evolution natural selection conceived independently two british naturalist charles darwin alfred russel wallace midth century explanation organism adapted physical biological environment theory first set detail darwin book origin specie evolution natural selection established observable fact living organism offspring often produced possibly survive trait vary among individual respect morphology physiology behaviour different trait confer different rate survival reproduction differential fitness trait passed generation generation heritability fitness successive generation member population therefore likely replaced offspring parent favourable characteristic environment early th century competing idea evolution refuted evolution combined mendelian inheritance population genetics give rise modern evolutionary theory synthesis basis heredity dna molecule pas information generation generation process change dna population include natural selection genetic drift mutation gene flow life earthincluding humanityshares last universal common ancestor luca lived approximately billion year ago fossil record includes progression early biogenic graphite microbial mat fossil fossilised multicellular organism existing pattern biodiversity shaped repeated formation new specie speciation change within specie anagenesis loss specie extinction throughout evolutionary history life earth morphological biochemical trait tend similar among specie share recent common ancestor historically used reconstruct phylogenetic tree although direct comparison genetic sequence common method today evolutionary biologist continued study various aspect evolution forming testing hypothesis well constructing theory based evidence field laboratory data generated method mathematical theoretical biology discovery influenced development biology also field including agriculture medicine computer science heredity evolution organism occurs change heritable characteristicsthe inherited characteristic organism human example eye colour inherited characteristic individual might inherit browneye trait one parent inherited trait controlled gene complete set gene within organism genome genetic material called genotype complete set observable trait make structure behaviour organism called phenotype trait come interaction genotype environment others neutral observable characteristic inherited example suntanned skin come interaction person genotype sunlight thus suntan passed people child phenotype ability skin tan exposed sunlight however people tan easily others due difference genotypic variation striking example people inherited trait albinism tan sensitive sunburn heritable characteristic passed one generation next via dna molecule encodes genetic information dna long biopolymer composed four type base sequence base along particular dna molecule specifies genetic information manner similar sequence letter spelling sentence cell divide dna copied resulting two cell inherit dna sequence portion dna molecule specify single functional unit called gene different gene different sequence base within cell long strand dna called chromosome specific location dna sequence within chromosome known locus dna sequence locus varies individual different form sequence called allele dna sequence change mutation producing new allele mutation occurs within gene new allele may affect trait gene control altering phenotype organism however simple correspondence allele trait work case trait influenced multiple gene quantitative epistatic manner source variation evolution occur genetic variation within population variation come mutation genome reshuffling gene sexual reproduction migration population gene flow despite constant introduction new variation mutation gene flow genome specie similar among individual specie however discovery field evolutionary developmental biology demonstrated even relatively small difference genotype lead dramatic difference phenotype within specie individual organism phenotype result genotype influence environment lived modern evolutionary synthesis defines evolution change time genetic variation frequency one particular allele become less prevalent relative form gene variation disappears new allele reach point fixationwhen either disappears population replaces ancestral allele entirely mutation mutation change dna sequence cell genome ultimate source genetic variation organism mutation occur may alter product gene prevent gene functioning effect half mutation coding region proteincoding gene deleterious half neutral small percentage total mutation region confer fitness benefit mutation part genome deleterious vast majority neutral beneficial mutation involve large section chromosome becoming duplicated usually genetic recombination introduce extra copy gene genome extra copy gene major source raw material needed new gene evolve important new gene evolve within gene family preexisting gene share common ancestor example human eye us four gene make structure sense light three colour vision one night vision four descended single ancestral gene new gene generated ancestral gene duplicate copy mutates acquires new function process easier gene duplicated increase redundancy system one gene pair acquire new function copy continues perform original function type mutation even generate entirely new gene previously noncoding dna phenomenon termed de novo gene birth generation new gene also involve small part several gene duplicated fragment recombining form new combination new function exon shuffling new gene assembled shuffling preexisting part domain act module simple independent function mixed together produce new combination new complex function example polyketide synthases large enzyme make antibiotic contain independent domain catalyse one step overall process like step assembly line one example mutation wild boar piglet camouflage coloured show characteristic pattern dark light longitudinal stripe however mutation melanocortin receptor mcr disrupt pattern majority pig breed carry mcr mutation disrupting wildtype colour different mutation causing dominant black colouring sex recombination asexual organism gene inherited together linked cannot mix gene organism reproduction contrast offspring sexual organism contain random mixture parent chromosome produced independent assortment related process called homologous recombination sexual organism exchange dna two matching chromosome recombination reassortment alter allele frequency instead change allele associated producing offspring new combination allele sex usually increase genetic variation may increase rate evolution twofold cost sex first described john maynard smith first cost sexually dimorphic specie one two sex bear young cost apply hermaphroditic specie like plant many invertebrate second cost individual reproduces sexually pas gene individual offspring even less passed new generation pass yet sexual reproduction common mean reproduction among eukaryote multicellular organism red queen hypothesis used explain significance sexual reproduction mean enable continual evolution adaptation response coevolution specie everchanging environment another hypothesis sexual reproduction primarily adaptation promoting accurate recombinational repair damage germline dna increased diversity byproduct process may sometimes adaptively beneficial gene flow gene flow exchange gene population specie therefore source variation new population specie gene flow caused movement individual separate population organism might caused movement mouse inland coastal population movement pollen heavymetaltolerant heavymetalsensitive population grass gene transfer specie includes formation hybrid organism horizontal gene transfer horizontal gene transfer transfer genetic material one organism another organism offspring common among bacteria medicine contributes spread antibiotic resistance one bacteria acquires resistance gene rapidly transfer specie horizontal transfer gene bacteria eukaryote yeast saccharomyces cerevisiae adzuki bean weevil callosobruchus chinensis occurred example largerscale transfer eukaryotic bdelloid rotifer received range gene bacteria fungi plant virus also carry dna organism allowing transfer gene even across biological domain largescale gene transfer also occurred ancestor eukaryotic cell bacteria acquisition chloroplast mitochondrion possible eukaryote originated horizontal gene transfer bacteria archaea epigenetics heritable change cannot explained change sequence nucleotide dna phenomenon classed epigenetic inheritance system dna methylation marking chromatin selfsustaining metabolic loop gene silencing rna interference threedimensional conformation protein prion area epigenetic inheritance system discovered organismic level developmental biologist suggest complex interaction genetic network communication among cell lead heritable variation may underlay mechanic developmental plasticity canalisation heritability may also occur even larger scale example ecological inheritance process niche construction defined regular repeated activity organism environment generates legacy effect modify feed back selection regime subsequent generation example heritability evolution direct control gene include inheritance cultural trait symbiogenesis evolutionary force neodarwinian perspective evolution occurs change frequency allele within population interbreeding organism example allele black colour population moth becoming common mechanism lead change allele frequency include natural selection genetic drift mutation bias natural selection evolution natural selection process trait enhance survival reproduction become common successive generation population embodies three principle variation exists within population organism respect morphology physiology behaviour phenotypic variation different trait confer different rate survival reproduction differential fitness trait passed generation generation heritability fitness offspring produced possibly survive condition produce competition organism survival reproduction consequently organism trait give advantage competitor likely pas trait next generation trait confer advantage teleonomy quality whereby process natural selection creates preserve trait seemingly fitted functional role perform consequence selection include nonrandom mating genetic hitchhiking central concept natural selection evolutionary fitness organism fitness measured organism ability survive reproduce determines size genetic contribution next generation however fitness total number offspring instead fitness indicated proportion subsequent generation carry organism gene example organism could survive well reproduce rapidly offspring small weak survive organism would make little genetic contribution future generation would thus low fitness allele increase fitness allele gene generation allele higher probability becoming common within population trait said selected example trait increase fitness enhanced survival increased fecundity conversely lower fitness caused less beneficial deleterious allele result allele likely becoming rarerthey selected importantly fitness allele fixed characteristic environment change previously neutral harmful trait may become beneficial previously beneficial trait become harmful however even direction selection reverse way trait lost past may reevolve identical form however reactivation dormant gene long eliminated genome suppressed perhaps hundred generation lead reoccurrence trait thought lost like hindlegs dolphin teeth chicken wing wingless stick insect tail additional nipple human etc throwback known atavism natural selection within population trait vary across range value height categorised three different type first directional selection shift average value trait timefor example organism slowly getting taller secondly disruptive selection selection extreme trait value often result two different value becoming common selection average value would either short tall organism advantage medium height finally stabilising selection selection extreme trait value end cause decrease variance around average value less diversity would example cause organism eventually similar height natural selection generally make nature measure individual individual trait less likely survive nature sense refers ecosystem system organism interact every element physical well biological local environment eugene odum founder ecology defined ecosystem unit includes organismsin given area interacting physical environment flow energy lead clearly defined trophic structure biotic diversity material cycle ie exchange material living nonliving part within system population within ecosystem occupies distinct niche position distinct relationship part system relationship involve life history organism position food chain geographic range broad understanding nature enables scientist delineate specific force together comprise natural selection natural selection act different level organisation gene cell individual organism group organism specie selection act multiple level simultaneously example selection occurring level individual organism gene called transposon replicate spread throughout genome selection level individual group selection may allow evolution cooperation genetic drift genetic drift random fluctuation allele frequency within population one generation next selective force absent relatively weak allele frequency equally likely drift upward downward successive generation allele subject sampling error drift halt allele eventually becomes fixed either disappearing population replacing allele entirely genetic drift may therefore eliminate allele population due chance alone even absence selective force genetic drift cause two separate population begin genetic structure drift apart two divergent population different set allele according neutral theory molecular evolution evolutionary change result fixation neutral mutation genetic drift model genetic change population thus result constant mutation pressure genetic drift form neutral theory debated since seem fit genetic variation seen nature bettersupported version model nearly neutral theory according mutation would effectively neutral small population necessarily neutral large population theory propose genetic drift dwarfed stochastic force evolution genetic hitchhiking also known genetic draft another concept constructive neutral evolution cne explains complex system emerge spread population neutral transition due principle excess capacity presuppression ratcheting applied area ranging origin spliceosome complex interdependence microbial community time take neutral allele become fixed genetic drift depends population size fixation rapid smaller population number individual population critical instead measure known effective population size effective population usually smaller total population since take account factor level inbreeding stage lifecycle population smallest effective population size may every gene population usually difficult measure relative importance selection neutral process including drift comparative importance adaptive nonadaptive force driving evolutionary change area current research mutation bias mutation bias usually conceived difference expected rate two different kind mutation eg transitiontransversion bias gcat bias deletioninsertion bias related idea developmental bias j b haldane ronald fisher argued mutation weak pressure easily overcome selection tendency mutation would ineffectual except condition neutral evolution extraordinarily high mutation rate opposingpressures argument long used dismiss possibility internal tendency evolution molecular era prompted renewed interest neutral evolution noboru sueoka ernst freese proposed systematic bias mutation might responsible systematic difference genomic gc composition specie identification gcbiased e coli mutator strain along proposal neutral theory established plausibility mutational explanation molecular pattern common molecular evolution literature instance mutation bias frequently invoked model codon usage model also include effect selection following mutationselectiondrift model allows mutation bias differential selection based effect translation hypothesis mutation bias played important role development thinking evolution genome composition including isochores different insertion v deletion bias different taxon lead evolution different genome size hypothesis lynch regarding genome size relies mutational bias toward increase decrease genome size however mutational hypothesis evolution composition suffered reduction scope discovered gcbiased gene conversion make important contribution composition diploid organism mammal bacterial genome frequently atbiased mutation contemporary thinking role mutation bias reflects different theory haldane fisher recent work showed original pressure theory assumes evolution based standing variation evolution depends event mutation introduce new allele mutational developmental bias introduction variation arrival bias impose bias evolution without requiring neutral evolution high mutation rate several study report mutation implicated adaptation reflect common mutation bias though others dispute interpretation genetic hitchhiking recombination allows allele strand dna become separated however rate recombination low approximately two event per chromosome per generation result gene close together chromosome may always shuffled away gene close together tend inherited together phenomenon known linkage tendency measured finding often two allele occur together single chromosome compared expectation called linkage disequilibrium set allele usually inherited group called haplotype important one allele particular haplotype strongly beneficial natural selection drive selective sweep also cause allele haplotype become common population effect called genetic hitchhiking genetic draft genetic draft caused fact neutral gene genetically linked others selection partially captured appropriate effective population size sexual selection special case natural selection sexual selection selection trait increase mating success increasing attractiveness organism potential mate trait evolved sexual selection particularly prominent among male several animal specie although sexually favoured trait cumbersome antler mating call large body size bright colour often attract predation compromise survival individual male survival disadvantage balanced higher reproductive success male show hardtofake sexually selected trait natural outcome evolution influence every aspect form behaviour organism prominent specific behavioural physical adaptation outcome natural selection adaptation increase fitness aiding activity finding food avoiding predator attracting mate organism also respond selection cooperating usually aiding relative engaging mutually beneficial symbiosis longer term evolution produce new specie splitting ancestral population organism new group cannot interbreed outcome evolution distinguished based time scale macroevolution versus microevolution macroevolution refers evolution occurs level specie particular speciation extinction whereas microevolution refers smaller evolutionary change within specie population particular shift allele frequency adaptation macroevolution outcome long period microevolution thus distinction micro macroevolution fundamental onethe difference simply time involved however macroevolution trait entire specie may important instance large amount variation among individual allows specie rapidly adapt new habitat lessening chance going extinct wide geographic range increase chance speciation making likely part population become isolated sense microevolution macroevolution might involve selection different levelswith microevolution acting gene organism versus macroevolutionary process specie selection acting entire specie affecting rate speciation extinction common misconception evolution goal longterm plan innate tendency progress expressed belief orthogenesis evolutionism realistically however evolution longterm goal necessarily produce greater complexity although complex specie evolved occur side effect overall number organism increasing simple form life still remain common biosphere example overwhelming majority specie microscopic prokaryote form half world biomass despite small size constitute vast majority earth biodiversity simple organism therefore dominant form life earth throughout history continue main form life present day complex life appearing diverse noticeable indeed evolution microorganism particularly important evolutionary research since rapid reproduction allows study experimental evolution observation evolution adaptation real time adaptation adaptation process make organism better suited habitat also term adaptation may refer trait important organism survival example adaptation horse teeth grinding grass using term adaptation evolutionary process adaptive trait product bodily part function two sens word may distinguished adaptation produced natural selection following definition due theodosius dobzhansky adaptation evolutionary process whereby organism becomes better able live habitat habitat adaptedness state adapted degree organism able live reproduce given set habitat adaptive trait aspect developmental pattern organism enables enhances probability organism surviving reproducing adaptation may cause either gain new feature loss ancestral feature example show type change bacterial adaptation antibiotic selection genetic change causing antibiotic resistance modifying target drug increasing activity transporter pump drug cell striking example bacteria escherichia coli evolving ability use citric acid nutrient longterm laboratory experiment flavobacterium evolving novel enzyme allows bacteria grow byproduct nylon manufacturing soil bacterium sphingobium evolving entirely new metabolic pathway degrades synthetic pesticide pentachlorophenol interesting still controversial idea adaptation might increase ability organism generate genetic diversity adapt natural selection increasing organism evolvability adaptation occurs gradual modification existing structure consequently structure similar internal organisation may different function related organism result single ancestral structure adapted function different way bone within bat wing example similar mouse foot primate hand due descent structure common mammalian ancestor however since living organism related extent even organ appear little structural similarity arthropod squid vertebrate eye limb wing arthropod vertebrate depend common set homologous gene control assembly function called deep homology evolution structure may lose original function become vestigial structure structure may little function current specie yet clear function ancestral specie closely related specie example include pseudogenes nonfunctional remains eye blind cavedwelling fish wing flightless bird presence hip bone whale snake sexual trait organism reproduce via asexual reproduction example vestigial structure human include wisdom teeth coccyx vermiform appendix behavioural vestige goose bump primitive reflex however many trait appear simple adaptation fact exaptations structure originally adapted one function coincidentally became somewhat useful function process one example african lizard holaspis guentheri developed extremely flat head hiding crevice seen looking near relative however specie head become flattened assist gliding tree treean exaptation within cell molecular machine bacterial flagellum protein sorting machinery evolved recruitment several preexisting protein previously different function another example recruitment enzyme glycolysis xenobiotic metabolism serve structural protein called crystallins within lens organism eye area current investigation evolutionary developmental biology developmental basis adaptation exaptations research address origin evolution embryonic development modification development developmental process produce novel feature study shown evolution alter development produce new structure embryonic bone structure develop jaw animal instead forming part middle ear mammal also possible structure lost evolution reappear due change developmental gene mutation chicken causing embryo grow teeth similar crocodile becoming clear alteration form organism due change small set conserved gene coevolution interaction organism produce conflict cooperation interaction pair specie pathogen host predator prey specie develop matched set adaptation evolution one specie cause adaptation second specie change second specie turn cause new adaptation first specie cycle selection response called coevolution example production tetrodotoxin roughskinned newt evolution tetrodotoxin resistance predator common garter snake predatorprey pair evolutionary arm race produced high level toxin newt correspondingly high level toxin resistance snake cooperation coevolved interaction specie involve conflict many case mutually beneficial interaction evolved instance extreme cooperation exists plant mycorrhizal fungi grow root aid plant absorbing nutrient soil reciprocal relationship plant provide fungi sugar photosynthesis fungi actually grow inside plant cell allowing exchange nutrient host sending signal suppress plant immune system coalition organism specie also evolved extreme case eusociality found social insect bee termite ant sterile insect feed guard small number organism colony able reproduce even smaller scale somatic cell make body animal limit reproduction maintain stable organism support small number animal germ cell produce offspring somatic cell respond specific signal instruct whether grow remain die cell ignore signal multiply inappropriately uncontrolled growth cause cancer cooperation within specie may evolved process kin selection one organism act help raise relative offspring activity selected helping individual contains allele promote helping activity likely kin also contain allele thus allele passed process may promote cooperation include group selection cooperation provides benefit group organism speciation speciation process specie diverges two descendant specie multiple way define concept specie choice definition dependent particularity specie concerned example specie concept apply readily toward sexually reproducing organism others lend better toward asexual organism despite diversity various specie concept various concept placed one three broad philosophical approach interbreeding ecological phylogenetic biological specie concept bsc classic example interbreeding approach defined evolutionary biologist ernst mayr bsc state specie group actually potentially interbreeding natural population reproductively isolated group despite wide longterm use bsc like specie concept without controversy example genetic recombination among prokaryote intrinsic aspect reproduction called specie problem researcher attempted unifying monistic definition specie others adopt pluralistic approach suggest may different way logically interpret definition specie barrier reproduction two diverging sexual population required population become new specie gene flow may slow process spreading new genetic variant also population depending far two specie diverged since recent common ancestor may still possible produce offspring horse donkey mating produce mule hybrid generally infertile case closely related specie may regularly interbreed hybrid selected specie remain distinct however viable hybrid occasionally formed new specie either property intermediate parent specie possess totally new phenotype importance hybridisation producing new specie animal unclear although case seen many type animal grey tree frog particularly wellstudied example speciation observed multiple time controlled laboratory condition nature sexually reproducing organism speciation result reproductive isolation followed genealogical divergence four primary geographic mode speciation common animal allopatric speciation occurs population initially isolated geographically habitat fragmentation migration selection condition produce rapid change appearance behaviour organism selection drift act independently population isolated rest specie separation may eventually produce organism cannot interbreed second mode speciation peripatric speciation occurs small population organism become isolated new environment differs allopatric speciation isolated population numerically much smaller parental population founder effect cause rapid speciation increase inbreeding increase selection homozygote leading rapid genetic change third mode parapatric speciation similar peripatric speciation small population enters new habitat differs physical separation two population instead speciation result evolution mechanism reduce gene flow two population generally occurs drastic change environment within parental specie habitat one example grass anthoxanthum odoratum undergo parapatric speciation response localised metal pollution mine plant evolve resistance high level metal soil selection interbreeding metalsensitive parental population produced gradual change flowering time metalresistant plant eventually produced complete reproductive isolation selection hybrid two population may cause reinforcement evolution trait promote mating within specie well character displacement two specie become distinct appearance finally sympatric speciation specie diverge without geographic isolation change habitat form rare since even small amount gene flow may remove genetic difference part population generally sympatric speciation animal requires evolution genetic difference nonrandom mating allow reproductive isolation evolve one type sympatric speciation involves crossbreeding two related specie produce new hybrid specie common animal animal hybrid usually sterile meiosis homologous chromosome parent different specie cannot successfully pair however common plant plant often double number chromosome form polyploid allows chromosome parental specie form matching pair meiosis since parent chromosome represented pair already example speciation event plant specie arabidopsis thaliana arabidopsis arenosa crossbred give new specie arabidopsis suecica happened year ago speciation process repeated laboratory allows study genetic mechanism involved process indeed chromosome doubling within specie may common cause reproductive isolation half doubled chromosome unmatched breeding undoubled organism speciation event important theory punctuated equilibrium account pattern fossil record short burst evolution interspersed relatively long period stasis specie remain relatively unchanged theory speciation rapid evolution linked natural selection genetic drift acting strongly organism undergoing speciation novel habitat small population result period stasis fossil record correspond parental population organism undergoing speciation rapid evolution found small population geographically restricted habitat therefore rarely preserved fossil extinction extinction disappearance entire specie extinction unusual event specie regularly appear speciation disappear extinction nearly animal plant specie lived earth extinct extinction appears ultimate fate specie extinction happened continuously throughout history life although rate extinction spike occasional mass extinction event cretaceouspaleogene extinction event nonavian dinosaur became extinct wellknown earlier permiantriassic extinction event even severe approximately marine specie driven extinction holocene extinction event ongoing mass extinction associated humanity expansion across globe past thousand year presentday extinction rate time greater background rate current specie may extinct mid st century human activity primary cause ongoing extinction event global warming may accelerate future despite estimated extinction specie ever lived earth trillion specie estimated earth currently onethousandth described role extinction evolution well understood may depend type extinction considered cause continuous lowlevel extinction event form majority extinction may result competition specie limited resource competitive exclusion principle one specie outcompete another could produce specie selection fitter specie surviving specie driven extinction intermittent mass extinction also important instead acting selective force drastically reduce diversity nonspecific manner promote burst rapid evolution speciation survivor application concept model used evolutionary biology natural selection many application artificial selection intentional selection trait population organism used thousand year domestication plant animal recently selection become vital part genetic engineering selectable marker antibiotic resistance gene used manipulate dna protein valuable property evolved repeated round mutation selection example modified enzyme new antibody process called directed evolution understanding change occurred organism evolution reveal gene needed construct part body gene may involved human genetic disorder example mexican tetra albino cavefish lost eyesight evolution breeding together different population blind fish produced offspring functional eye since different mutation occurred isolated population evolved different cave helped identify gene required vision pigmentation evolutionary theory many application medicine many human disease static phenomenon capable evolution virus bacteria fungi cancer evolve resistant host immune defence well pharmaceutical drug problem occur agriculture pesticide herbicide resistance possible facing end effective life available antibiotic predicting evolution evolvability pathogen devising strategy slow circumvent requiring deeper knowledge complex force driving evolution molecular level computer science simulation evolution using evolutionary algorithm artificial life started extended simulation artificial selection artificial evolution became widely recognised optimisation method result work ingo rechenberg used evolution strategy solve complex engineering problem genetic algorithm particular became popular writing john henry holland practical application also include automatic evolution computer programme evolutionary algorithm used solve multidimensional problem efficiently software produced human designer also optimise design system evolutionary history life origin life earth billion year old earliest undisputed evidence life earth date least billion year ago eoarchean era geological crust started solidify following earlier molten hadean eon microbial mat fossil found billionyearold sandstone western australia early physical evidence biogenic substance graphite billionyearold metasedimentary rock discovered western greenland well remains biotic life found billionyearold rock western australia commenting australian finding stephen blair hedge wrote life arose relatively quickly earth could common universe july scientist reported identifying set gene last universal common ancestor luca organism living earth specie amounting five billion specie ever lived earth estimated extinct estimate number earth current specie range million million million estimated named million documented central database date leaving least yet described highly energetic chemistry thought produced selfreplicating molecule around billion year ago half billion year later last common ancestor life existed current scientific consensus complex biochemistry make life came simpler chemical reaction beginning life may included selfreplicating molecule rna assembly simple cell common descent organism earth descended common ancestor ancestral gene pool current specie stage process evolution diversity product long series speciation extinction event common descent organism first deduced four simple fact organism first geographic distribution cannot explained local adaptation second diversity life set completely unique organism organism share morphological similarity third vestigial trait clear purpose resemble functional ancestral trait fourth organism classified using similarity hierarchy nested group similar family tree due horizontal gene transfer tree life may complicated simple branching tree since gene spread independently distantly related specie solve problem others author prefer use coral life metaphor mathematical model illustrate evolution life view date back idea briefly mentioned darwin later abandoned past specie also left record evolutionary history fossil along comparative anatomy presentday organism constitute morphological anatomical record comparing anatomy modern extinct specie palaeontologist infer lineage specie however approach successful organism hard body part shell bone teeth prokaryote bacteria archaea share limited set common morphology fossil provide information ancestry recently evidence common descent come study biochemical similarity organism example living cell use basic set nucleotide amino acid development molecular genetics revealed record evolution left organism genome dating specie diverged molecular clock produced mutation example dna sequence comparison revealed human chimpanzee share genome analysing area differ help shed light common ancestor specie existed evolution life prokaryote inhabited earth approximately billion year ago obvious change morphology cellular organisation occurred organism next billion year eukaryotic cell emerged billion year ago next major change cell structure came bacteria engulfed eukaryotic cell cooperative association called endosymbiosis engulfed bacteria host cell underwent coevolution bacteria evolving either mitochondrion hydrogenosomes another engulfment cyanobacteriallike organism led formation chloroplast algae plant history life unicellular eukaryote prokaryote archaea around billion year ago multicellular organism began appear differentiated cell performing specialised function evolution multicellularity occurred multiple independent event organism diverse sponge brown algae cyanobacteria slime mould myxobacteria january scientist reported million year ago minor genetic change single molecule called gkpid may allowed organism go single cell organism one many cell approximately million year ago remarkable amount biological diversity appeared span around million year called cambrian explosion majority type modern animal appeared fossil record well unique lineage subsequently became extinct various trigger cambrian explosion proposed including accumulation oxygen atmosphere photosynthesis million year ago plant fungi colonised land soon followed arthropod animal insect particularly successful even today make majority animal specie amphibian first appeared around million year ago followed early amniote bird around million year ago reptilelike lineage mammal around million year ago homininae around million year ago modern human around year ago however despite evolution large animal smaller organism similar type evolved early process continue highly successful dominate earth majority biomass specie prokaryote history evolutionary thought classical antiquity proposal one type organism could descend another type go back first presocratic greek philosopher anaximander empedocles proposal survived roman time poet philosopher lucretius followed empedocles masterwork de rerum natura lit nature thing middle age contrast materialistic view aristotelianism considered natural thing actualisation fixed natural possibility known form became part medieval teleological understanding nature thing intended role play divine cosmic order variation idea became standard understanding middle age integrated christian learning aristotle demand real type organism always correspond oneforone exact metaphysical form specifically gave example new type living thing could come number arab muslim scholar wrote evolution notably ibn khaldun wrote book muqaddimah asserted human developed world monkey process specie become numerous predarwinian new science th century rejected aristotelian approach sought explain natural phenomenon term physical law visible thing require existence fixed natural category divine cosmic order however new approach slow take root biological science last bastion concept fixed natural type john ray applied one previously general term fixed natural type specie plant animal type strictly identified type living thing specie proposed specie could defined feature perpetuated generation generation biological classification introduced carl linnaeus explicitly recognised hierarchical nature specie relationship still viewed specie fixed according divine plan naturalist time speculated evolutionary change specie time according natural law pierre louis maupertuis wrote natural modification occurring reproduction accumulating many generation produce new specie georgeslouis leclerc comte de buffon suggested specie could degenerate different organism erasmus darwin proposed warmblooded animal could descended single microorganism filament first fullfledged evolutionary scheme jeanbaptiste lamarck transmutation theory envisaged spontaneous generation continually producing simple form life developed greater complexity parallel lineage inherent progressive tendency postulated local level lineage adapted environment inheriting change caused use disuse parent latter process later called lamarckism idea condemned established naturalist speculation lacking empirical support particular george cuvier insisted specie unrelated fixed similarity reflecting divine design functional need meantime ray idea benevolent design developed william paley natural theology evidence existence attribute deity proposed complex adaptation evidence divine design admired charles darwin darwinian revolution crucial break concept constant typological class type biology came theory evolution natural selection formulated charles darwin alfred wallace term variable population darwin used expression descent modification rather evolution partly influenced essay principle population thomas robert malthus darwin noted population growth would lead struggle existence favourable variation prevailed others perished generation many offspring fail survive age reproduction limited resource could explain diversity plant animal common ancestry working natural law way type organism darwin developed theory natural selection onwards writing big book subject alfred russel wallace sent version virtually theory separate paper presented together meeting linnean society london end darwin publication abstract origin specie explained natural selection detail way led increasingly wide acceptance darwin concept evolution expense alternative theory thomas henry huxley applied darwin idea human using palaeontology comparative anatomy provide strong evidence human ape shared common ancestry disturbed since implied human special place universe othniel c marsh america first palaeontologist first provide solid fossil evidence support darwin theory evolution unearthing ancestor modern horse marsh delivered influential speech annual meeting american association advancement science providing demonstrative argument evolution first time marsh traced evolution vertebrate fish way human sparing detail listed wealth fossil example past life form significance speech immediately recognised scientific community printed entirety several scientific journal marsh caught attention scientific world publication odontornithes monograph extinct bird north america included discovery bird teeth skeleton helped bridge gap dinosaur bird provided invaluable support darwin theory evolution darwin wrote marsh saying work old bird many fossil animal n america afforded best support theory evolution appeared within last year since darwin publication origin specie pangenesis heredity mechanism reproductive heritability origin new trait remained mystery towards end darwin developed provisional theory pangenesis gregor mendel reported trait inherited predictable manner independent assortment segregation element later known gene mendel law inheritance eventually supplanted darwin pangenesis theory august weismann made important distinction germ cell give rise gamete sperm egg cell somatic cell body demonstrating heredity pass germ line hugo de vries connected darwin pangenesis theory weismann germsoma cell distinction proposed darwin pangenes concentrated cell nucleus expressed could move cytoplasm change cell structure de vries also one researcher made mendel work well known believing mendelian trait corresponded transfer heritable variation along germline explain new variant originate de vries developed mutation theory led temporary rift accepted darwinian evolution biometricians allied de vries pioneer field population genetics ronald fisher sewall wright j b haldane set foundation evolution onto robust statistical philosophy false contradiction darwin theory genetic mutation mendelian inheritance thus reconciled modern synthesis modern synthesis connected natural selection population genetics based mendelian inheritance unified theory included random genetic drift mutation gene flow new version evolutionary theory focused change allele frequency population explained pattern observed across specie population fossil transition palaeontology synthesis since synthesis extended evolution explanatory power light numerous discovery cover biological phenomenon across whole biological hierarchy gene population publication structure dna james watson francis crick contribution rosalind franklin demonstrated physical mechanism inheritance molecular biology improved understanding relationship genotype phenotype advance also made phylogenetic systematics mapping transition trait comparative testable framework publication use evolutionary tree evolutionary biologist theodosius dobzhansky penned nothing biology make sense except light evolution brought light relation first seemed disjointed fact natural history coherent explanatory body knowledge describes predicts many observable fact life planet one extension known evolutionary developmental biology informally called evodevo emphasis change generation evolution act pattern change within individual organism development since beginning st century biologist argued extended evolutionary synthesis would account effect nongenetic inheritance mode epigenetics parental effect ecological inheritance cultural inheritance evolvability social cultural response th century particularly publication origin specie idea life evolved active source academic debate centred philosophical social religious implication evolution today modern evolutionary synthesis accepted vast majority scientist however evolution remains contentious concept theist various religion denomination reconciled belief evolution concept theistic evolution creationists believe evolution contradicted creation myth found religion raise various objection evolution demonstrated response publication vestige natural history creation controversial aspect evolutionary biology implication human evolution human share common ancestry ape mental moral faculty humanity type natural cause inherited trait animal country notably united state tension science religion fuelled current creationevolution controversy religious conflict focusing politics public education scientific field cosmology earth science also conflict literal interpretation many religious text evolutionary biology experience significantly opposition religious literalists teaching evolution american secondary school biology class uncommon first half th century scope trial decision caused subject become rare american secondary biology textbook generation gradually reintroduced later became legally protected epperson v arkansas decision since competing religious belief creationism legally disallowed secondary school curriculum various decision returned pseudoscientific form intelligent design id excluded kitzmiller v dover area school district case debate darwin idea generate significant controversy china see also devolution biology notion specie revert primitive form chronospecies reference bibliography reading external link general information evolution time bbc evolution resource national academy washington dc national academy science retrieved may understanding evolution onestop resource information evolution berkeley california university california berkeley retrieved may evolution evolution year darwin origin specie arlington county virginia national science foundation archived original may retrieved may human evolution timeline interactive smithsonian institution national museum natural history january retrieved july adobe flash required history evolution united state salon retrieved video cosmos animation evolution carl sagan youtube experiment lenski richard e experimental evolution east lansing michigan michigan state university retrieved july chastain erick livnat adi papadimitriou christos vazirani umesh july algorithm game evolution pnas bibcodepnasc doipnas issn pmc pmid online lecture evolution matter lecture series harvard online learning cambridge massachusetts harvard university archived original december retrieved july stearns stephen c eeb principle evolution ecology behavior open yale course new connecticut yale university archived original december retrieved july', 'machine learning ml field study artificial intelligence concerned development study statistical algorithm learn data generalise unseen data thus perform task without explicit instruction within subdiscipline machine learning advance field deep learning allowed neural network class statistical algorithm surpass many previous machine learning approach performance ml find application many field including natural language processing computer vision speech recognition email filtering agriculture medicine application ml business problem known predictive analytics statistic mathematical optimisation mathematical programming method comprise foundation machine learning data mining related field study focusing exploratory data analysis eda via unsupervised learning theoretical viewpoint probably approximately correct learning provides framework describing machine learning history term machine learning coined arthur samuel ibm employee pioneer field computer gaming artificial intelligence synonym selfteaching computer also used time period earliest machine learning program introduced arthur samuel invented computer program calculated winning chance checker side history machine learning root back decade human desire effort study human cognitive process canadian psychologist donald hebb published book organization behavior introduced theoretical neural structure formed certain interaction among nerve cell hebbs model neuron interacting one another set groundwork ai machine learning algorithm work node artificial neuron used computer communicate data researcher studied human cognitive system contributed modern machine learning technology well including logician walter pitt warren mcculloch proposed early mathematical model neural network come algorithm mirror human thought process early experimental learning machine punched tape memory called cybertron developed raytheon company analyse sonar signal electrocardiogram speech pattern using rudimentary reinforcement learning repetitively trained human operatorteacher recognise pattern equipped goof button cause reevaluate incorrect decision representative book research machine learning nilsson book learning machine dealing mostly machine learning pattern classification interest related pattern recognition continued described duda hart report given using teaching strategy artificial neural network learns recognise character letter digit special symbol computer terminal tom mitchell provided widely quoted formal definition algorithm studied machine learning field computer program said learn experience e respect class task performance measure p performance task measured p improves experience e definition task machine learning concerned offer fundamentally operational definition rather defining field cognitive term follows alan turing proposal paper computing machinery intelligence question machine think replaced question machine thinking entity modernday machine learning two objective one classify data based model developed purpose make prediction future outcome based model hypothetical algorithm specific classifying data may use computer vision mole coupled supervised learning order train classify cancerous mole machine learning algorithm stock trading may inform trader future potential prediction relationship field artificial intelligence scientific endeavour machine learning grew quest artificial intelligence ai early day ai academic discipline researcher interested machine learn data attempted approach problem various symbolic method well termed neural network mostly perceptrons model later found reinventions generalised linear model statistic probabilistic reasoning also employed especially automated medical diagnosis however increasing emphasis logical knowledgebased approach caused rift ai machine learning probabilistic system plagued theoretical practical problem data acquisition representation expert system come dominate ai statistic favour work symbolicknowledgebased learning continue within ai leading inductive logic programmingilp statistical line research outside field ai proper pattern recognition information retrieval neural network research abandoned ai computer science around time line continued outside aics field connectionism researcher discipline including john hopfield david rumelhart geoffrey hinton main success came mids reinvention backpropagation machine learning ml reorganised recognised field started flourish field changed goal achieving artificial intelligence tackling solvable problem practical nature shifted focus away symbolic approach inherited ai toward method model borrowed statistic fuzzy logic probability theory data compression data mining machine learning data mining often employ method overlap significantly machine learning focus prediction based known property learned training data data mining focus discovery previously unknown property data analysis step knowledge discovery database data mining us many machine learning method different goal hand machine learning also employ data mining method unsupervised learning preprocessing step improve learner accuracy much confusion two research community often separate conference separate journal ecml pkdd major exception come basic assumption work machine learning performance usually evaluated respect ability reproduce known knowledge knowledge discovery data mining kdd key task discovery previously unknown knowledge evaluated respect known knowledge uninformed unsupervised method easily outperformed supervised method typical kdd task supervised method cannot used due unavailability training data machine learning also intimate tie optimisation many learning problem formulated minimisation loss function training set example loss function express discrepancy prediction model trained actual problem instance example classification one want assign label instance model trained correctly predict preassigned label set example generalization characterizing generalisation various learning algorithm active topic current research especially deep learning algorithm statistic machine learning statistic closely related field term method distinct principal goal statistic draw population inference sample machine learning find generalisable predictive pattern according michael jordan idea machine learning methodological principle theoretical tool long prehistory statistic also suggested term data science placeholder call overall field conventional statistical analysis require priori selection model suitable study data set addition significant theoretically relevant variable based previous experience included analysis contrast machine learning built prestructured model rather data shape model detecting underlying pattern variable input used train model accurate ultimate model leo breiman distinguished two statistical modelling paradigm data model algorithmic model wherein algorithmic model mean less machine learning algorithm like random forest statistician adopted method machine learning leading combined field call statistical learning statistical physic analytical computational technique derived deeprooted physic disordered system extended largescale problem including machine learning eg analyse weight space deep neural network statistical physic thus finding application area medical diagnostics theory core objective learner generalise experience generalisation context ability learning machine perform accurately new unseen examplestasks experienced learning data set training example come generally unknown probability distribution considered representative space occurrence learner build general model space enables produce sufficiently accurate prediction new case computational analysis machine learning algorithm performance branch theoretical computer science known computational learning theory via probably approximately correct learning model training set finite future uncertain learning theory usually yield guarantee performance algorithm instead probabilistic bound performance quite common biasvariance decomposition one way quantify generalisation error best performance context generalisation complexity hypothesis match complexity function underlying data hypothesis less complex function model fitted data complexity model increased response training error decrease hypothesis complex model subject overfitting generalisation poorer addition performance bound learning theorist study time complexity feasibility learning computational learning theory computation considered feasible done polynomial time two kind time complexity result positive result show certain class function learned polynomial time negative result show certain class cannot learned polynomial time approach machine learning approach traditionally divided three broad category correspond learning paradigm depending nature signal feedback available learning system supervised learning computer presented example input desired output given teacher goal learn general rule map input output unsupervised learning label given learning algorithm leaving find structure input unsupervised learning goal discovering hidden pattern data mean towards end feature learning reinforcement learning computer program interacts dynamic environment must perform certain goal driving vehicle playing game opponent navigates problem space program provided feedback thats analogous reward try maximise although algorithm advantage limitation single algorithm work problem supervised learning supervised learning algorithm build mathematical model set data contains input desired output data known training data consists set training example training example one input desired output also known supervisory signal mathematical model training example represented array vector sometimes called feature vector training data represented matrix iterative optimisation objective function supervised learning algorithm learn function used predict output associated new input optimal function allows algorithm correctly determine output input part training data algorithm improves accuracy output prediction time said learned perform task type supervisedlearning algorithm include active learning classification regression classification algorithm used output restricted limited set value regression algorithm used output take numerical value within range example classification algorithm filter email input incoming email output folder file email contrast regression used task predicting person height based factor like age genetics forecasting future temperature based historical data similarity learning area supervised machine learning closely related regression classification goal learn example using similarity function measure similar related two object application ranking recommendation system visual identity tracking face verification speaker verification unsupervised learning unsupervised learning algorithm find structure data labelled classified categorised instead responding feedback unsupervised learning algorithm identify commonality data react based presence absence commonality new piece data central application unsupervised machine learning include clustering dimensionality reduction density estimation cluster analysis assignment set observation subset called cluster observation within cluster similar according one predesignated criterion observation drawn different cluster dissimilar different clustering technique make different assumption structure data often defined similarity metric evaluated example internal compactness similarity member cluster separation difference cluster method based estimated density graph connectivity special type unsupervised learning called selfsupervised learning involves training model generating supervisory signal data semisupervised learning semisupervised learning fall unsupervised learning without labelled training data supervised learning completely labelled training data training example missing training label yet many machinelearning researcher found unlabelled data used conjunction small amount labelled data produce considerable improvement learning accuracy weakly supervised learning training label noisy limited imprecise however label often cheaper obtain resulting larger effective training set reinforcement learning reinforcement learning area machine learning concerned software agent ought take action environment maximise notion cumulative reward due generality field studied many discipline game theory control theory operation research information theory simulationbased optimisation multiagent system swarm intelligence statistic genetic algorithm reinforcement learning environment typically represented markov decision process mdp many reinforcement learning algorithm use dynamic programming technique reinforcement learning algorithm assume knowledge exact mathematical model mdp used exact model infeasible reinforcement learning algorithm used autonomous vehicle learning play game human opponent dimensionality reduction dimensionality reduction process reducing number random variable consideration obtaining set principal variable word process reducing dimension feature set also called number feature dimensionality reduction technique considered either feature elimination extraction one popular method dimensionality reduction principal component analysis pca pca involves changing higherdimensional data eg smaller space eg manifold hypothesis proposes highdimensional data set lie along lowdimensional manifold many dimensionality reduction technique make assumption leading area manifold learning manifold regularisation type approach developed fit neatly threefold categorisation sometimes one used machine learning system example topic modelling metalearning selflearning selflearning machine learning paradigm introduced along neural network capable selflearning named crossbar adaptive array caa give solution problem learning without external reward introducing emotion internal reward emotion used state evaluation selflearning agent caa selflearning algorithm computes crossbar fashion decision action emotion feeling consequence situation system driven interaction cognition emotion selflearning algorithm update memory matrix w iteration executes following machine learning routine situation perform action receive consequence situation compute emotion consequence situation v update crossbar memory v system one input situation one output action behaviour neither separate reinforcement input advice input environment backpropagated value secondary reinforcement emotion toward consequence situation caa exists two environment one behavioural environment behaves genetic environment wherefrom initially receives initial emotion situation encountered behavioural environment receiving genome specie vector genetic environment caa learns goalseeking behaviour environment contains desirable undesirable situation feature learning several learning algorithm aim discovering better representation input provided training classic example include principal component analysis cluster analysis feature learning algorithm also called representation learning algorithm often attempt preserve information input also transform way make useful often preprocessing step performing classification prediction technique allows reconstruction input coming unknown datagenerating distribution necessarily faithful configuration implausible distribution replaces manual feature engineering allows machine learn feature use perform specific task feature learning either supervised unsupervised supervised feature learning feature learned using labelled input data example include artificial neural network multilayer perceptrons supervised dictionary learning unsupervised feature learning feature learned unlabelled input data example include dictionary learning independent component analysis autoencoders matrix factorisation various form clustering manifold learning algorithm attempt constraint learned representation lowdimensional sparse coding algorithm attempt constraint learned representation sparse meaning mathematical model many zero multilinear subspace learning algorithm aim learn lowdimensional representation directly tensor representation multidimensional data without reshaping higherdimensional vector deep learning algorithm discover multiple level representation hierarchy feature higherlevel abstract feature defined term generating lowerlevel feature argued intelligent machine one learns representation disentangles underlying factor variation explain observed data feature learning motivated fact machine learning task classification often require input mathematically computationally convenient process however realworld data image video sensory data yielded attempt algorithmically define specific feature alternative discover feature representation examination without relying explicit algorithm sparse dictionary learning sparse dictionary learning feature learning method training example represented linear combination basis function assumed sparse matrix method strongly nphard difficult solve approximately popular heuristic method sparse dictionary learning ksvd algorithm sparse dictionary learning applied several context classification problem determine class previously unseen training example belongs dictionary class already built new training example associated class best sparsely represented corresponding dictionary sparse dictionary learning also applied image denoising key idea clean image patch sparsely represented image dictionary noise cannot anomaly detection data mining anomaly detection also known outlier detection identification rare item event observation raise suspicion differing significantly majority data typically anomalous item represent issue bank fraud structural defect medical problem error text anomaly referred outlier novelty noise deviation exception particular context abuse network intrusion detection interesting object often rare object unexpected burst inactivity pattern adhere common statistical definition outlier rare object many outlier detection method particular unsupervised algorithm fail data unless aggregated appropriately instead cluster analysis algorithm may able detect microclusters formed pattern three broad category anomaly detection technique exist unsupervised anomaly detection technique detect anomaly unlabelled test data set assumption majority instance data set normal looking instance seem fit least remainder data set supervised anomaly detection technique require data set labelled normal abnormal involves training classifier key difference many statistical classification problem inherently unbalanced nature outlier detection semisupervised anomaly detection technique construct model representing normal behaviour given normal training data set test likelihood test instance generated model robot learning robot learning inspired multitude machine learning method starting supervised learning reinforcement learning finally metalearning eg maml association rule association rule learning rulebased machine learning method discovering relationship variable large database intended identify strong rule discovered database using measure interestingness rulebased machine learning general term machine learning method identifies learns evolves rule store manipulate apply knowledge defining characteristic rulebased machine learning algorithm identification utilisation set relational rule collectively represent knowledge captured system contrast machine learning algorithm commonly identify singular model universally applied instance order make prediction rulebased machine learning approach include learning classifier system association rule learning artificial immune system based concept strong rule rakesh agrawal tomasz imieliski arun swami introduced association rule discovering regularity product largescale transaction data recorded pointofsale po system supermarket example rule n n p e b u r g e r displaystyle mathrm onionspotatoes rightarrow mathrm burger found sale data supermarket would indicate customer buy onion potato together likely also buy hamburger meat information used basis decision marketing activity promotional pricing product placement addition market basket analysis association rule employed today application area including web usage mining intrusion detection continuous production bioinformatics contrast sequence mining association rule learning typically consider order item either within transaction across transaction learning classifier system lcs family rulebased machine learning algorithm combine discovery component typically genetic algorithm learning component performing either supervised learning reinforcement learning unsupervised learning seek identify set contextdependent rule collectively store apply knowledge piecewise manner order make prediction inductive logic programming ilp approach rule learning using logic programming uniform representation input example background knowledge hypothesis given encoding known background knowledge set example represented logical database fact ilp system derive hypothesized logic program entail positive negative example inductive programming related field considers kind programming language representing hypothesis logic programming functional program inductive logic programming particularly useful bioinformatics natural language processing gordon plotkin ehud shapiro laid initial theoretical foundation inductive machine learning logical setting shapiro built first implementation model inference system prolog program inductively inferred logic program positive negative example term inductive refers philosophical induction suggesting theory explain observed fact rather mathematical induction proving property member wellordered set model machine learning model type mathematical model trained given dataset used make prediction classification new data training learning algorithm iteratively adjusts model internal parameter minimise error prediction extension term model refer several level specificity general class model associated learning algorithm fully trained model internal parameter tuned various type model used researched machine learning system picking best model task called model selection artificial neural network artificial neural network anns connectionist system computing system vaguely inspired biological neural network constitute animal brain system learn perform task considering example generally without programmed taskspecific rule ann model based collection connected unit node called artificial neuron loosely model neuron biological brain connection like synapsis biological brain transmit information signal one artificial neuron another artificial neuron receives signal process signal additional artificial neuron connected common ann implementation signal connection artificial neuron real number output artificial neuron computed nonlinear function sum input connection artificial neuron called edge artificial neuron edge typically weight adjusts learning proceeds weight increase decrease strength signal connection artificial neuron may threshold signal sent aggregate signal cross threshold typically artificial neuron aggregated layer different layer may perform different kind transformation input signal travel first layer input layer last layer output layer possibly traversing layer multiple time original goal ann approach solve problem way human brain would however time attention moved performing specific task leading deviation biology artificial neural network used variety task including computer vision speech recognition machine translation social network filtering playing board video game medical diagnosis deep learning consists multiple hidden layer artificial neural network approach try model way human brain process light sound vision hearing successful application deep learning computer vision speech recognition decision tree decision tree learning us decision tree predictive model go observation item represented branch conclusion item target value represented leaf one predictive modelling approach used statistic data mining machine learning tree model target variable take discrete set value called classification tree tree structure leaf represent class label branch represent conjunction feature lead class label decision tree target variable take continuous value typically real number called regression tree decision analysis decision tree used visually explicitly represent decision decision making data mining decision tree describes data resulting classification tree input decisionmaking random forest regression random forest regression rfr fall umbrella decision treebased model rfr ensemble learning method build multiple decision tree average prediction improve accuracy avoid overfitting build decision tree rfr us bootstrapped sampling instance decision tree trained random data training set random selection rfr training enables model reduce bias prediction achieve accuracy rfr generates independent decision tree work single output data well multiple regressor task make rfr compatible used various application supportvector machine supportvector machine svms also known supportvector network set related supervised learning method used classification regression given set training example marked belonging one two category svm training algorithm build model predicts whether new example fall one category svm training algorithm nonprobabilistic binary linear classifier although method platt scaling exist use svm probabilistic classification setting addition performing linear classification svms efficiently perform nonlinear classification using called kernel trick implicitly mapping input highdimensional feature space regression analysis regression analysis encompasses large variety statistical method estimate relationship input variable associated feature common form linear regression single line drawn best fit given data according mathematical criterion ordinary least square latter often extended regularisation method mitigate overfitting bias ridge regression dealing nonlinear problem goto model include polynomial regression example used trendline fitting microsoft excel logistic regression often used statistical classification even kernel regression introduces nonlinearity taking advantage kernel trick implicitly map input variable higherdimensional space multivariate linear regression extends concept linear regression handle multiple dependent variable simultaneously approach estimate relationship set input variable several output variable fitting multidimensional linear model particularly useful scenario output interdependent share underlying pattern predicting multiple economic indicator reconstructing image inherently multidimensional bayesian network bayesian network belief network directed acyclic graphical model probabilistic graphical model represents set random variable conditional independence directed acyclic graph dag example bayesian network could represent probabilistic relationship disease symptom given symptom network used compute probability presence various disease efficient algorithm exist perform inference learning bayesian network model sequence variable like speech signal protein sequence called dynamic bayesian network generalisation bayesian network represent solve decision problem uncertainty called influence diagram gaussian process gaussian process stochastic process every finite collection random variable process multivariate normal distribution relies predefined covariance function kernel model pair point relate depending location given set observed point inputoutput example distribution unobserved output new point function input data directly computed looking like observed point covariance point new unobserved point gaussian process popular surrogate model bayesian optimisation used hyperparameter optimisation genetic algorithm genetic algorithm ga search algorithm heuristic technique mimic process natural selection using method mutation crossover generate new genotype hope finding good solution given problem machine learning genetic algorithm used conversely machine learning technique used improve performance genetic evolutionary algorithm belief function theory belief function also referred evidence theory dempstershafer theory general framework reasoning uncertainty understood connection framework probability possibility imprecise probability theory theoretical framework thought kind learner analogous property evidence combined eg dempsters rule combination like pmfbased bayesian approach would combine probability however many caveat belief function compared bayesian approach order incorporate ignorance uncertainty quantification belief function approach implemented within machine learning domain typically leverage fusion approach various ensemble method better handle learner decision boundary low sample ambiguous class issue standard machine learning approach tend difficulty resolving however computational complexity algorithm dependent number proposition class lead much higher computation time compared machine learning approach rulebased model rulebased machine learning rbml branch machine learning automatically discovers learns rule data provides interpretable model making useful decisionmaking field like healthcare fraud detection cybersecurity key rbml technique includes learning classifier system association rule learning artificial immune system similar model method extract pattern data evolve rule time training model typically machine learning model require high quantity reliable data perform accurate prediction training machine learning model machine learning engineer need target collect large representative sample data data training set varied corpus text collection image sensor data data collected individual user service overfitting something watch training machine learning model trained model derived biased nonevaluated data result skewed undesired prediction biased model may result detrimental outcome thereby furthering negative impact society objective algorithmic bias potential result data fully prepared training machine learning ethic becoming field study notably becoming integrated within machine learning engineering team federated learning federated learning adapted form distributed artificial intelligence training machine learning model decentralises training process allowing user privacy maintained needing send data centralised server also increase efficiency decentralising training process many device example gboard us federated machine learning train search query prediction model user mobile phone without send individual search back google application many application machine learning including mediaservices provider netflix held first netflix prize competition find program better predict user preference improve accuracy existing cinematch movie recommendation algorithm least joint team made researcher att labsresearch collaboration team big chaos pragmatic theory built ensemble model win grand prize million shortly prize awarded netflix realised viewer rating best indicator viewing pattern everything recommendation changed recommendation engine accordingly article wall street journal noted use machine learning rebellion research predict financial crisis cofounder sun microsystems vinod khosla predicted medical doctor job would lost next two decade automated machine learning medical diagnostic software reported machine learning algorithm applied field art history study fine art painting may revealed previously unrecognised influence among artist springer nature published first research book created using machine learning machine learning technology used help make diagnosis aid researcher developing cure covid machine learning recently applied predict proenvironmental behaviour traveller recently machine learning technology also applied optimise smartphones performance thermal behaviour based user interaction phone applied correctly machine learning algorithm mlas utilise wide range company characteristic predict stock return without overfitting employing effective feature engineering combining forecast mlas generate result far surpass obtained basic linear technique like ols recent advancement machine learning extended field quantum chemistry novel algorithm enable prediction solvent effect chemical reaction thereby offering new tool chemist tailor experimental condition optimal outcome machine learning becoming useful tool investigate predict evacuation decision making large scale small scale disaster different solution tested predict householder decide evacuate wildfire hurricane application focusing pre evacuation decision building fire machine learning also emerging promising tool geotechnical engineering used support task ground classification hazard prediction site characterization recent research emphasizes move toward datacentric method field machine learning replacement engineering judgment way enhance using sitespecific data pattern limitation although machine learning transformative field machinelearning program often fail deliver expected result reason numerous lack suitable data lack access data data bias privacy problem badly chosen task algorithm wrong tool people lack resource evaluation problem black box theory pose another yet significant challenge black box refers situation algorithm process producing output entirely opaque meaning even coder algorithm cannot audit pattern machine extracted data house lord select committee claimed intelligence system could substantial impact individual life would considered acceptable unless provided full satisfactory explanation decision make selfdriving car uber failed detect pedestrian killed collision attempt use machine learning healthcare ibm watson system failed deliver even year time billion dollar invested microsofts bing chat chatbot reported produce hostile offensive response user machine learning used strategy update evidence related systematic review increased reviewer burden related growth biomedical literature improved training set yet developed sufficiently reduce workload burden without limiting necessary sensitivity finding research explainability explainable ai xai interpretable ai explainable machine learning xml artificial intelligence ai human understand decision prediction made ai contrast black box concept machine learning even designer cannot explain ai arrived specific decision refining mental model user aipowered system dismantling misconception xai promise help user perform effectively xai may implementation social right explanation overfitting settling bad overly complex theory gerrymandered fit past training data known overfitting many system attempt reduce overfitting rewarding theory accordance well fit data penalising theory accordance complex theory limitation vulnerability learner also disappoint learning wrong lesson toy example image classifier trained picture brown horse black cat might conclude brown patch likely horse realworld example unlike human current image classifier often primarily make judgement spatial relationship component picture learn relationship pixel human oblivious still correlate image certain type real object modifying pattern legitimate image result adversarial image system misclassifies adversarial vulnerability also result nonlinear system nonpattern perturbation system possible change output changing single adversarially chosen pixel machine learning model often vulnerable manipulation evasion via adversarial machine learning researcher demonstrated backdoor placed undetectably classifying eg category spam wellvisible spam post machine learning model often developed trained third party party change classification input including case type datasoftware transparency provided possibly including whitebox access model assessment classification machine learning model validated accuracy estimation technique like holdout method split data training test set conventionally training set test set designation evaluates performance training model test set comparison kfoldcrossvalidation method randomly partition data k subset k experiment performed respectively considering subset evaluation remaining k subset training model addition holdout crossvalidation method bootstrap sample n instance replacement dataset used assess model accuracy addition overall accuracy investigator frequently report sensitivity specificity meaning true positive rate tpr true negative rate tnr respectively similarly investigator sometimes report false positive rate fpr well false negative rate fnr however rate ratio fail reveal numerator denominator receiver operating characteristic roc along accompanying area roc curve auc offer additional tool classification model assessment higher auc associated better performing model ethic bias different machine learning approach suffer different data bias machine learning system trained specifically current customer may able predict need new customer group represented training data trained humanmade data machine learning likely pick constitutional unconscious bias already present society system trained datasets collected bias may exhibit bias upon use algorithmic bias thus digitising cultural prejudice example uk commission racial equality found st george medical school using computer program trained data previous admission staff program denied nearly candidate found either woman noneuropean sounding name using job hiring data firm racist hiring policy may lead machine learning system duplicating bias scoring job applicant similarity previous successful applicant another example includes predictive policing company geoliticas predictive algorithm resulted disproportionately high level overpolicing lowincome minority community trained historical crime data responsible collection data documentation algorithmic rule used system considered critical part machine learning researcher blame lack participation representation minority population field ai machine learning vulnerability bias fact according research carried computing research association cra female faculty merely make faculty member focus ai among several university around world furthermore among group new u resident ai phd graduate identified white asian hispanic african american demonstrates lack diversity field ai language model learned data shown contain humanlike bias human language contain bias machine trained language corpus necessarily also learn bias microsoft tested tay chatbot learned twitter quickly picked racist sexist language experiment carried propublica investigative journalism organisation machine learning algorithm insight recidivism rate among prisoner falsely flagged black defendant high risk twice often white defendant google photo tagged couple black people gorilla caused controversy gorilla label subsequently removed still cannot recognise gorilla similar issue recognising nonwhite people found many system challenge effective use machine learning may take longer adopted domain concern fairness machine learning reducing bias machine learning propelling use human good increasingly expressed artificial intelligence scientist including feifei li said there nothing artificial ai inspired people created people andmost importantlyit impact people powerful tool beginning understand profound responsibility financial incentive concern among health care professional system might designed public interest incomegenerating machine especially true united state longstanding ethical dilemma improving health care also increasing profit example algorithm could designed provide patient unnecessary test medication algorithm proprietary owner hold stake potential machine learning health care provide professional additional tool diagnose medicate plan recovery path patient requires bias mitigated hardware since advance machine learning algorithm computer hardware led efficient method training deep neural network particular narrow subdomain machine learning contain many layer nonlinear hidden unit graphic processing unit gpus often aispecific enhancement displaced cpu dominant method training largescale commercial cloud ai openai estimated hardware compute used largest deep learning project alexnet alphazero found fold increase amount compute required doublingtime trendline month tensor processing unit tpus tensor processing unit tpus specialised hardware accelerator developed google specifically machine learning workload unlike generalpurpose gpus fpgas tpus optimised tensor computation making particularly efficient deep learning task training inference widely used google cloud ai service largescale machine learning model like google deepmind alphafold large language model tpus leverage matrix multiplication unit highbandwidth memory accelerate computation maintaining energy efficiency since introduction tpus become key component ai infrastructure especially cloudbased environment neuromorphic computing neuromorphic computing refers class computing system designed emulate structure functionality biological neural network system may implemented softwarebased simulation conventional hardware specialised hardware architecture physical neural network physical neural network specific type neuromorphic hardware relies electrically adjustable material memristors emulate function neural synapsis term physical neural network highlight use physical hardware computation opposed softwarebased implementation broadly refers artificial neural network use material adjustable resistance replicate neural synapsis embedded machine learning embedded machine learning subfield machine learning model deployed embedded system limited computing resource wearable computer edge device microcontrollers running model directly device eliminates need transfer store data cloud server processing thereby reducing risk data breach privacy leak theft intellectual property personal data business secret embedded machine learning achieved various technique hardware acceleration approximate computing model optimisation common optimisation technique include pruning quantization knowledge distillation lowrank factorisation network architecture search parameter sharing software software suite containing variety machine learning algorithm include following free opensource software proprietary software free opensource edition knime rapidminer proprietary software journal journal machine learning research machine learning nature machine intelligence neural computation ieee transaction pattern analysis machine intelligence conference aaai conference artificial intelligence association computational linguistics acl european conference machine learning principle practice knowledge discovery database ecml pkdd international conference computational intelligence method bioinformatics biostatistics cibb international conference machine learning icml international conference learning representation iclr international conference intelligent robot system iros conference knowledge discovery data mining kdd conference neural information processing system neurips see also automated machine learning process automating application machine learning big data extremely large complex datasets deep learning branch ml concerned artificial neural network differentiable programming programming paradigm list datasets machinelearning research mtheory learning framework machine unlearning solomonoffs theory inductive inference mathematical theory reference source domingo pedro september master algorithm quest ultimate learning machine remake world basic book isbn nilsson nil artificial intelligence new synthesis morgan kaufmann isbn archived original july retrieved november poole david mackworth alan goebel randy computational intelligence logical approach new york oxford university press isbn archived original july retrieved august russell stuart j norvig peter artificial intelligence modern approach nd ed upper saddle river new jersey prentice hall isbn reading external link international machine learning society mloss academic database opensource machine learning software', 'artificial intelligence ai capability computational system perform task typically associated human intelligence learning reasoning problemsolving perception decisionmaking field research computer science develops study method software enable machine perceive environment use learning intelligence take action maximize chance achieving defined goal highprofile application ai include advanced web search engine eg google search recommendation system used youtube amazon netflix virtual assistant eg google assistant siri alexa autonomous vehicle eg waymo generative creative tool eg language model ai art superhuman play analysis strategy game eg chess go however many ai application perceived ai lot cutting edge ai filtered general application often without called ai something becomes useful enough common enough labeled ai anymore various subfields ai research centered around particular goal use particular tool traditional goal ai research include learning reasoning knowledge representation planning natural language processing perception support robotics reach goal ai researcher adapted integrated wide range technique including search mathematical optimization formal logic artificial neural network method based statistic operation research economics ai also draw upon psychology linguistics philosophy neuroscience field company openai google deepmind meta aim create artificial general intelligence agiai complete virtually cognitive task least well human artificial intelligence founded academic discipline field went multiple cycle optimism throughout history followed period disappointment loss funding known ai winter funding interest vastly increased graphic processing unit started used accelerate neural network deep learning outperformed previous ai technique growth accelerated transformer architecture ongoing period rapid progress advanced generative ai became known ai boom generative ai ability create modify content led several unintended consequence harm raised ethical concern ai longterm effect potential existential risk prompting discussion regulatory policy ensure safety benefit technology goal general problem simulating creating intelligence broken subproblems consist particular trait capability researcher expect intelligent system display trait described received attention cover scope ai research reasoning problemsolving early researcher developed algorithm imitated stepbystep reasoning human use solve puzzle make logical deduction late method developed dealing uncertain incomplete information employing concept probability economics many algorithm insufficient solving large reasoning problem experience combinatorial explosion become exponentially slower problem grow even human rarely use stepbystep deduction early ai research could model solve problem using fast intuitive judgment accurate efficient reasoning unsolved problem knowledge representation knowledge representation knowledge engineering allow ai program answer question intelligently make deduction realworld fact formal knowledge representation used contentbased indexing retrieval scene interpretation clinical decision support knowledge discovery mining interesting actionable inference large database area knowledge base body knowledge represented form used program ontology set object relation concept property used particular domain knowledge knowledge base need represent thing object property category relation object situation event state time cause effect knowledge knowledge know people know default reasoning thing human assume true told differently remain true even fact changing many aspect domain knowledge among difficult problem knowledge representation breadth commonsense knowledge set atomic fact average person know enormous subsymbolic form commonsense knowledge much people know represented fact statement could express verbally also difficulty knowledge acquisition problem obtaining knowledge ai application planning decisionmaking agent anything perceives take action world rational agent goal preference take action make happen automated planning agent specific goal automated decisionmaking agent preferencesthere situation would prefer situation trying avoid decisionmaking agent assigns number situation called utility measure much agent prefers possible action calculate expected utility utility possible outcome action weighted probability outcome occur choose action maximum expected utility classical planning agent know exactly effect action realworld problem however agent may certain situation unknown unobservable may know certain happen possible action deterministic must choose action making probabilistic guess reassess situation see action worked problem agent preference may uncertain especially agent human involved learned eg inverse reinforcement learning agent seek information improve preference information value theory used weigh value exploratory experimental action space possible future action situation typically intractably large agent must take action evaluate situation uncertain outcome markov decision process transition model describes probability particular action change state particular way reward function supply utility state cost action policy associate decision possible state policy could calculated eg iteration heuristic learned game theory describes rational behavior multiple interacting agent used ai program make decision involve agent learning machine learning study program improve performance given task automatically part ai beginning several kind machine learning unsupervised learning analyzes stream data find pattern make prediction without guidance supervised learning requires labeling training data expected answer come two main variety classification program must learn predict category input belongs regression program must deduce numeric function based numeric input reinforcement learning agent rewarded good response punished bad one agent learns choose response classified good transfer learning knowledge gained one problem applied new problem deep learning type machine learning run input biologically inspired artificial neural network type learning computational learning theory assess learner computational complexity sample complexity much data required notion optimization natural language processing natural language processing nlp allows program read write communicate human language specific problem include speech recognition speech synthesis machine translation information extraction information retrieval question answering early work based noam chomsky generative grammar semantic network difficulty wordsense disambiguation unless restricted small domain called microworlds due common sense knowledge problem margaret masterman believed meaning grammar key understanding language thesaurus dictionary basis computational language structure modern deep learning technique nlp include word embedding representing word typically vector encoding meaning transformer deep learning architecture using attention mechanism others generative pretrained transformer gpt language model began generate coherent text model able get humanlevel score bar exam sat test gre test many realworld application perception machine perception ability use input sensor camera microphone wireless signal active lidar sonar radar tactile sensor deduce aspect world computer vision ability analyze visual input field includes speech recognition image classification facial recognition object recognition object tracking robotic perception social intelligence affective computing field comprises system recognize interpret process simulate human feeling emotion mood example virtual assistant programmed speak conversationally even banter humorously make appear sensitive emotional dynamic human interaction otherwise facilitate humancomputer interaction however tends give nave user unrealistic conception intelligence existing computer agent moderate success related affective computing include textual sentiment analysis recently multimodal sentiment analysis wherein ai classifies effect displayed videotaped subject general intelligence machine artificial general intelligence would able solve wide variety problem breadth versatility similar human intelligence technique ai research us wide variety technique accomplish goal search optimization ai solve many problem intelligently searching many possible solution two different kind search used ai state space search local search state space search state space search search tree possible state try find goal state example planning algorithm search tree goal subgoals attempting find path target goal process called meansends analysis simple exhaustive search rarely sufficient realworld problem search space number place search quickly grows astronomical number result search slow never completes heuristic rule thumb help prioritize choice likely reach goal adversarial search used gameplaying program chess go search tree possible move countermove looking winning position local search local search us mathematical optimization find solution problem begin form guess refines incrementally gradient descent type local search optimizes set numerical parameter incrementally adjusting minimize loss function variant gradient descent commonly used train neural network backpropagation algorithm another type local search evolutionary computation aim iteratively improve set candidate solution mutating recombining selecting fittest survive generation distributed search process coordinate via swarm intelligence algorithm two popular swarm algorithm used search particle swarm optimization inspired bird flocking ant colony optimization inspired ant trail logic formal logic used reasoning knowledge representation formal logic come two main form propositional logic operates statement true false us logical connective implies predicate logic also operates object predicate relation us quantifier every x x y deductive reasoning logic process proving new statement conclusion statement given assumed true premise proof structured proof tree node labelled sentence child node connected parent node inference rule given problem set premise problemsolving reduces searching proof tree whose root node labelled solution problem whose leaf node labelled premise axiom case horn clause problemsolving search performed reasoning forward premise backwards problem general case clausal form firstorder logic resolution single axiomfree rule inference problem solved proving contradiction premise include negation problem solved inference horn clause logic firstorder logic undecidable therefore intractable however backward reasoning horn clause underpins computation logic programming language prolog turing complete moreover efficiency competitive computation symbolic programming language fuzzy logic assigns degree truth therefore handle proposition vague partially true nonmonotonic logic including logic programming negation failure designed handle default reasoning specialized version logic developed describe many complex domain probabilistic method uncertain reasoning many problem ai including reasoning planning learning perception robotics require agent operate incomplete uncertain information ai researcher devised number tool solve problem using method probability theory economics precise mathematical tool developed analyze agent make choice plan using decision theory decision analysis information value theory tool include model markov decision process dynamic decision network game theory mechanism design bayesian network tool used reasoning using bayesian inference algorithm learning using expectationmaximization algorithm planning using decision network perception using dynamic bayesian network probabilistic algorithm also used filtering prediction smoothing finding explanation stream data thus helping perception system analyze process occur time eg hidden markov model kalman filter classifier statistical learning method simplest ai application divided two type classifier eg shiny diamond one hand controller eg diamond pick hand classifier function use pattern matching determine closest match finetuned based chosen example using supervised learning pattern also called observation labeled certain predefined class observation combined class label known data set new observation received observation classified based previous experience many kind classifier use decision tree simplest widely used symbolic machine learning algorithm knearest neighbor algorithm widely used analogical ai mids kernel method support vector machine svm displaced knearest neighbor naive bayes classifier reportedly widely used learner google due part scalability neural network also used classifier artificial neural network artificial neural network based collection node also known artificial neuron loosely model neuron biological brain trained recognise pattern trained recognise pattern fresh data input least one hidden layer node output node applies function weight cross specified threshold data transmitted next layer network typically called deep neural network least hidden layer learning algorithm neural network use local search choose weight get right output input training common training technique backpropagation algorithm neural network learn model complex relationship input output find pattern data theory neural network learn function feedforward neural network signal pass one direction term perceptron typically refers singlelayer neural network contrast deep learning us many layer recurrent neural network rnns feed output signal back input allows shortterm memory previous input event long shortterm memory network lstms recurrent neural network better preserve longterm dependency less sensitive vanishing gradient problem convolutional neural network cnns use layer kernel efficiently process local pattern local processing especially important image processing early cnn layer typically identify simple local pattern edge curve subsequent layer detecting complex pattern like texture eventually whole object deep learning deep learning us several layer neuron network input output multiple layer progressively extract higherlevel feature raw input example image processing lower layer may identify edge higher layer may identify concept relevant human digit letter face deep learning profoundly improved performance program many important subfields artificial intelligence including computer vision speech recognition natural language processing image classification others reason deep learning performs well many application known sudden success deep learning occur new discovery theoretical breakthrough deep neural network backpropagation described many people far back two factor incredible increase computer power including hundredfold increase speed switching gpus availability vast amount training data especially giant curated datasets used benchmark testing imagenet gpt generative pretrained transformer gpt large language model llm generate text based semantic relationship word sentence textbased gpt model pretrained large corpus text internet pretraining consists predicting next token token usually word subword punctuation throughout pretraining gpt model accumulate knowledge world generate humanlike text repeatedly predicting next token typically subsequent training phase make model truthful useful harmless usually technique called reinforcement learning human feedback rlhf current gpt model prone generating falsehood called hallucination reduced rlhf quality data problem getting worse reasoning system system used chatbots allow people ask question request task simple text current model service include chatgpt claude gemini copilot meta ai multimodal gpt model process different type data modality image video sound text hardware software late graphic processing unit gpus increasingly designed aispecific enhancement used specialized tensorflow software replaced previously used central processing unit cpu dominant mean largescale commercial academic machine learning model training specialized programming language prolog used early ai research generalpurpose programming language like python become predominant transistor density integrated circuit observed roughly double every monthsa trend known moore law named intel cofounder gordon moore first identified improvement gpus even faster trend sometimes called huangs law named nvidia cofounder ceo jensen huang application ai machine learning technology used essential application including search engine google search targeting online advertisement recommendation system offered netflix youtube amazon driving internet traffic targeted advertising adsense facebook virtual assistant siri alexa autonomous vehicle including drone ada selfdriving car automatic language translation microsoft translator google translate facial recognition apple faceid microsofts deepface google facenet image labeling used facebook apple photo tiktok deployment ai may overseen chief automation officer cao health medicine application ai medicine medical research potential increase patient care quality life lens hippocratic oath medical professional ethically compelled use ai application accurately diagnose treat patient medical research ai important tool processing integrating big data particularly important organoid tissue engineering development use microscopy imaging key technique fabrication suggested ai overcome discrepancy funding allocated different field research new ai tool deepen understanding biomedically relevant pathway example alphafold demonstrated ability approximate hour rather month structure protein reported aiguided drug discovery helped find class antibiotic capable killing two different type drugresistant bacteria researcher used machine learning accelerate search parkinson disease drug treatment aim identify compound block clumping aggregation alphasynuclein protein characterises parkinson disease able speed initial screening process tenfold reduce cost thousandfold game game playing program used since demonstrate test ai advanced technique deep blue became first computer chessplaying system beat reigning world chess champion garry kasparov may jeopardy quiz show exhibition match ibms question answering system watson defeated two greatest jeopardy champion brad rutter ken jennings significant margin march alphago game go match go champion lee sedol becoming first computer goplaying system beat professional go player without handicap defeated ke jie best go player world program handle imperfectinformation game pokerplaying program pluribus deepmind developed increasingly generalistic reinforcement learning model muzero could trained play chess go atari game deepminds alphastar achieved grandmaster level starcraft ii particularly challenging realtime strategy game involves incomplete knowledge happens map ai agent competed playstation gran turismo competition winning four world best gran turismo driver using deep reinforcement learning google deepmind introduced sima type ai capable autonomously playing nine previously unseen openworld video game observing screen output well executing short specific task response natural language instruction mathematics large language model gpt gemini claude llama mistral increasingly used mathematics probabilistic model versatile also produce wrong answer form hallucination sometimes need large database mathematical problem learn also method supervised finetuning trained classifier humanannotated data improve answer new problem learn correction february study showed performance language model reasoning capability solving math problem included training data low even problem minor deviation trained data one technique improve performance involves training model produce correct reasoning step rather correct result alibaba group developed version qwen model called qwenmath achieved stateoftheart performance several mathematical benchmark including accuracy math dataset competition mathematics problem january microsoft proposed technique rstarmath leverage monte carlo tree search stepbystep reasoning enabling relatively small language model like qwenb solve aime math benchmark problem alternatively dedicated model mathematical problem solving higher precision outcome including proof theorem developed alphatensor alphageometry alphaproof alphaevolve google deepmind llemma eleutherai julius natural language used describe mathematical problem converter transform prompt formal language lean define mathematical task experimental model gemini deep think accepts natural language prompt directly achieved gold medal result international math olympiad model developed solve challenging problem reach good result benchmark test others serve educational tool mathematics topological deep learning integrates various topological approach finance finance one fastest growing sector applied ai tool deployed retail online banking investment advice insurance automated robot adviser use year according nicolas firzli director world pension investment forum may early see emergence highly innovative aiinformed financial product service argues deployment ai tool simply automatise thing destroying ten thousand job banking financial planning pension advice process im sure unleash new wave eg sophisticated pension innovation military various country deploying ai military application main application enhance command control communication sensor integration interoperability research targeting intelligence collection analysis logistics cyber operation information operation semiautonomous autonomous vehicle ai technology enable coordination sensor effector threat detection identification marking enemy position target acquisition coordination deconfliction distributed joint fire networked combat vehicle humanoperated autonomous ai used military operation iraq syria israel ukraine generative ai agent ai agent software entity designed perceive environment make decision take action autonomously achieve specific goal agent interact user environment agent ai agent used various application including virtual assistant chatbots autonomous vehicle gameplaying system industrial robotics ai agent operate within constraint programming available computational resource hardware limitation mean restricted performing task within defined scope finite memory processing capability realworld application ai agent often face time constraint decisionmaking action execution many ai agent incorporate learning algorithm enabling improve performance time experience training using machine learning ai agent adapt new situation optimise behaviour designated task sexuality application ai domain include aienabled menstruation fertility tracker analyze user data offer prediction aiintegrated sex toy eg teledildonics aigenerated sexual education content ai agent simulate sexual romantic partner eg replika ai also used production nonconsensual deepfake pornography raising significant ethical legal concern ai technology also used attempt identify online genderbased violence online sexual grooming minor industryspecific task also thousand successful ai application used solve specific problem specific industry institution survey one five company reported incorporated ai offering process example energy storage medical diagnosis military logistics application predict result judicial decision foreign policy supply chain management ai application evacuation disaster management growing ai used investigate pattern largescale smallscale evacuation using historical data gps video social medium furthermore ai provide realtime information evacuation condition agriculture ai helped farmer increase yield identify area need irrigation fertilization pesticide treatment agronomist use ai conduct research development ai used predict ripening time crop tomato monitor soil moisture operate agricultural robot conduct predictive analytics classify livestock pig call emotion automate greenhouse detect disease pest save water artificial intelligence used astronomy analyze increasing amount available data application mainly classification regression clustering forecasting generation discovery development new scientific insight example used discovering exoplanets forecasting solar activity distinguishing signal instrumental effect gravitational wave astronomy additionally could used activity space space exploration including analysis data space mission realtime science decision spacecraft space debris avoidance autonomous operation indian election u million spent authorized aigenerated content notably creating deepfakes allied including sometimes deceased politician better engage voter translating speech various local language ethic ai potential benefit potential risk ai may able advance science find solution serious problem demis hassabis deepmind hope solve intelligence use solve everything else however use ai become widespread several unintended consequence risk identified inproduction system sometimes factor ethic bias ai training process especially ai algorithm inherently unexplainable deep learning risk harm privacy copyright machine learning algorithm require large amount data technique used acquire data raised concern privacy surveillance copyright aipowered device service virtual assistant iot product continuously collect personal information raising concern intrusive data gathering unauthorized access third party loss privacy exacerbated ai ability process combine vast amount data potentially leading surveillance society individual activity constantly monitored analyzed without adequate safeguard transparency sensitive user data collected may include online activity record geolocation data video audio example order build speech recognition algorithm amazon recorded million private conversation allowed temporary worker listen transcribe opinion widespread surveillance range see necessary evil clearly unethical violation right privacy ai developer argue way deliver valuable application developed several technique attempt preserve privacy still obtaining data data aggregation deidentification differential privacy since privacy expert cynthia dwork begun view privacy term fairness brian christian wrote expert pivoted question know question theyre generative ai often trained unlicensed copyrighted work including domain image computer code output used rationale fair use expert disagree well circumstance rationale hold court law relevant factor may include purpose character use copyrighted work effect upon potential market copyrighted work website owner wish content scraped indicate robotstxt file leading author including john grisham jonathan franzen sued ai company using work train generative ai another discussed approach envision separate sui generis system protection creation generated ai ensure fair attribution compensation human author dominance tech giant commercial ai scene dominated big tech company alphabet inc amazon apple inc meta platform microsoft player already vast majority existing cloud infrastructure computing power data center allowing entrench marketplace power need environmental impact january international energy agency iea released electricity analysis forecast forecasting electric power use first iea report make projection data center power consumption artificial intelligence cryptocurrency report state power demand us might double additional electric power usage equal electricity used whole japanese nation prodigious power consumption ai responsible growth fossil fuel use might delay closing obsolete carbonemitting coal energy facility feverish rise construction data center throughout u making large technology firm eg microsoft meta google amazon voracious consumer electric power projected electric consumption immense concern fulfilled matter source chatgpt search involves use time electrical energy google search large firm haste find power source nuclear energy geothermal fusion tech firm argue long view ai eventually kinder environment need energy ai make power grid efficient intelligent assist growth nuclear power track overall carbon emission according technology firm goldman sachs research paper ai data center coming u power demand surge found u power demand likely experience growth seen generation forecast u data center consume u power opposed presaging growth electrical power generation industry variety mean data center need electrical power might max electrical grid big tech company counter ai used maximize utilization grid wall street journal reported big ai company begun negotiation u nuclear power provider provide electricity data center march amazon purchased pennsylvania nuclearpowered data center u million nvidia ceo jensen huang said nuclear power good option data center september microsoft announced agreement constellation energy reopen three mile island nuclear power plant provide microsoft electric power produced plant year reopening plant suffered partial nuclear meltdown unit reactor require constellation get strict regulatory process include extensive safety scrutiny u nuclear regulatory commission approved first ever u recommissioning nuclear plant megawatt power enough home energy produced cost reopening upgrading estimated u billion dependent tax break nuclear power contained u inflation reduction act u government state michigan investing almost u billion reopen palisade nuclear reactor lake michigan closed since plant planned reopened october three mile island facility renamed crane clean energy center chris crane nuclear proponent former ceo exelon responsible exelons spinoff constellation last approval september taiwan suspended approval data center north taoyuan capacity mw due power supply shortage taiwan aim phase nuclear power hand singapore imposed ban opening data center due electric power lifted ban although nuclear plant japan shut fukushima nuclear accident according october bloomberg article japanese cloud gaming service company ubitus nvidia stake looking land japan near nuclear power plant new data center generative ai ubitus ceo wesley kuo said nuclear power plant efficient cheap stable power ai november federal energy regulatory commission ferc rejected application submitted talen energy approval supply electricity nuclear power station susquehanna amazon data center according commission chairman willie l phillips burden electricity grid well significant cost shifting concern household business sector report prepared international energy agency estimated greenhouse gas emission energy consumption ai million ton emission could rise million tonne depending measure taken energy sector emission emission reduction potential ai estimated energy sector emission rebound effect example people switch public transport autonomous car reduce misinformation youtube facebook others use recommender system guide user content ai program given goal maximizing user engagement goal keep people watching ai learned user tended choose misinformation conspiracy theory extreme partisan content keep watching ai recommended user also tended watch content subject ai led people filter bubble received multiple version misinformation convinced many user misinformation true ultimately undermined trust institution medium government ai program correctly learned maximize goal result harmful society u election major technology company took step mitigate problem early generative ai began create image audio text virtually indistinguishable real photograph recording human writing realistic aigenerated video became feasible mids possible bad actor use technology create massive amount misinformation propaganda one potential malicious use deepfakes computational propaganda ai pioneer geoffrey hinton expressed concern ai enabling authoritarian leader manipulate electorate large scale among risk ai researcher microsoft openai university organisation suggested using personhood credential way overcome online deception enabled ai model algorithmic bias fairness machine learning application biased learn biased data developer may aware bias exists bias introduced way training data selected way model deployed biased algorithm used make decision seriously harm people medicine finance recruitment housing policing algorithm may cause discrimination field fairness study prevent harm algorithmic bias june google photoss new image labeling feature mistakenly identified jacky alcine friend gorilla black system trained dataset contained image black people problem called sample size disparity google fixed problem preventing system labelling anything gorilla eight year later google photo still could identify gorilla neither could similar product apple facebook microsoft amazon compas commercial program widely used u court assess likelihood defendant becoming recidivist julia angwin propublica discovered compas exhibited racial bias despite fact program told race defendant although error rate white black calibrated equal exactly error race differentthe system consistently overestimated chance black person would reoffend would underestimate chance white person would reoffend several researcher showed mathematically impossible compas accommodate possible measure fairness base rate reoffense different white black data program make biased decision even data explicitly mention problematic feature race gender feature correlate feature like address shopping history first name program make decision based feature would race gender moritz hardt said robust fact research area fairness blindness doesnt work criticism compas highlighted machine learning model designed make prediction valid assume future resemble past trained data includes result racist decision past machine learning model must predict racist decision made future application us prediction recommendation recommendation likely racist thus machine learning well suited help make decision area hope future better past descriptive rather prescriptive bias unfairness may go undetected developer overwhelmingly white male among ai engineer black woman various conflicting definition mathematical model fairness notion depend ethical assumption influenced belief society one broad category distributive fairness focus outcome often identifying group seeking compensate statistical disparity representational fairness try ensure ai system reinforce negative stereotype render certain group invisible procedural fairness focus decision process rather outcome relevant notion fairness may depend context notably type ai application stakeholder subjectivity notion bias fairness make difficult company operationalize access sensitive attribute race gender also considered many ai ethicist necessary order compensate bias may conflict antidiscrimination law conference fairness accountability transparency acm facct association computing machinery seoul south korea presented published finding recommend ai robotics system demonstrated free bias mistake unsafe use selflearning neural network trained vast unregulated source flawed internet data curtailed lack transparency many ai system complex designer cannot explain reach decision particularly deep neural network many nonlinear relationship input output popular explainability technique exist impossible certain program operating correctly one know exactly work many case machine learning program passed rigorous test nevertheless learned something different programmer intended example system could identify skin disease better medical professional found actually strong tendency classify image ruler cancerous picture malignancy typically include ruler show scale another machine learning system designed help effectively allocate medical resource found classify patient asthma low risk dying pneumonia asthma actually severe risk factor since patient asthma would usually get much medical care relatively unlikely die according training data correlation asthma low risk dying pneumonia real misleading people harmed algorithm decision right explanation doctor example expected clearly completely explain colleague reasoning behind decision make early draft european union general data protection regulation included explicit statement right exists industry expert noted unsolved problem solution sight regulator argued nevertheless harm real problem solution tool used darpa established xai explainable artificial intelligence program try solve problem several approach aim address transparency problem shap enables visualise contribution feature output lime locally approximate model output simpler interpretable model multitask learning provides large number output addition target classification output help developer deduce network learned deconvolution deepdream generative method allow developer see different layer deep network computer vision learned produce output suggest network learning generative pretrained transformer anthropic developed technique based dictionary learning associate pattern neuron activation humanunderstandable concept bad actor weaponized ai artificial intelligence provides number tool useful bad actor authoritarian government terrorist criminal rogue state lethal autonomous weapon machine locates selects engages human target without human supervision widely available ai tool used bad actor develop inexpensive autonomous weapon produced scale potentially weapon mass destruction even used conventional warfare currently cannot reliably choose target could potentially kill innocent person nation including china supported ban autonomous weapon united nation convention certain conventional weapon however united state others disagreed fifty country reported researching battlefield robot ai tool make easier authoritarian government efficiently control citizen several way face voice recognition allow widespread surveillance machine learning operating data classify potential enemy state prevent hiding recommendation system precisely target propaganda misinformation maximum effect deepfakes generative ai aid producing misinformation advanced ai make authoritarian centralized decisionmaking competitive liberal decentralized system market lower cost difficulty digital warfare advanced spyware technology available since earlierai facial recognition system already used mass surveillance china many way ai expected help bad actor foreseen example machinelearning ai able design ten thousand toxic molecule matter hour technological unemployment economist frequently highlighted risk redundancy ai speculated unemployment adequate social policy full employment past technology tended increase rather reduce total employment economist acknowledge uncharted territory ai survey economist showed disagreement whether increasing use robot ai cause substantial increase longterm unemployment generally agree could net benefit productivity gain redistributed risk estimate vary example michael osborne carl benedikt frey estimated u job high risk potential automation oecd report classified u job high risk methodology speculating future employment level criticised lacking evidential foundation implying technology rather social policy creates unemployment opposed redundancy april reported job chinese video game illustrator eliminated generative artificial intelligence unlike previous wave automation many middleclass job may eliminated artificial intelligence economist stated worry ai could whitecollar job steam power bluecollar one industrial revolution worth taking seriously job extreme risk range paralegal fast food cook job demand likely increase carerelated profession ranging personal healthcare clergy early day development artificial intelligence argument example put forward joseph weizenbaum whether task done computer actually done given difference computer human quantitative calculation qualitative valuebased judgement existential risk argued ai become powerful humanity may irreversibly lose control could physicist stephen hawking stated spell end human race scenario common science fiction computer robot suddenly develops humanlike selfawareness sentience consciousness becomes malevolent character scifi scenario misleading several way first ai require humanlike sentience existential risk modern ai program given specific goal use learning intelligence achieve philosopher nick bostrom argued one give almost goal sufficiently powerful ai may choose destroy humanity achieve used example paperclip maximizer stuart russell give example household robot try find way kill owner prevent unplugged reasoning cant fetch coffee youre dead order safe humanity superintelligence would genuinely aligned humanity morality value fundamentally side second yuval noah harari argues ai require robot body physical control pose existential risk essential part civilization physical thing like ideology law government money economy built language exist story billion people believe current prevalence misinformation suggests ai could use language convince people believe anything even take action destructive opinion amongst expert industry insider mixed sizable fraction concerned unconcerned risk eventual superintelligent ai personality stephen hawking bill gate elon musk well ai pioneer yoshua bengio stuart russell demis hassabis sam altman expressed concern existential risk ai may geoffrey hinton announced resignation google order able freely speak risk ai without considering impact google notably mentioned risk ai takeover stressed order avoid worst outcome establishing safety guideline require cooperation among competing use ai many leading ai expert endorsed joint statement mitigating risk extinction ai global priority alongside societalscale risk pandemic nuclear war researcher optimistic ai pioneer jrgen schmidhuber sign joint statement emphasising case ai research making human life longer healthier easier tool used improve life also used bad actor also used bad actor andrew ng also argued mistake fall doomsday hype aiand regulator benefit vested interest yann lecun scoff peer dystopian scenario supercharged misinformation even eventually human extinction early expert argued risk distant future warrant research human valuable perspective superintelligent machine however study current future risk possible solution became serious area research ethical machine alignment friendly ai machine designed beginning minimize risk make choice benefit human eliezer yudkowsky coined term argues developing friendly ai higher research priority may require large investment must completed ai becomes existential risk machine intelligence potential use intelligence make ethical decision field machine ethic provides machine ethical principle procedure resolving ethical dilemma field machine ethic also called computational morality founded aaai symposium approach include wendell wallachs artificial moral agent stuart j russell three principle developing provably beneficial machine open source active organization ai opensource community include hugging face google eleutherai meta various ai model llama mistral stable diffusion made openweight meaning architecture trained parameter weight publicly available openweight model freely finetuned allows company specialize data usecase openweight model useful research innovation also misused since finetuned builtin security measure objecting harmful request trained away becomes ineffective researcher warn future ai model may develop dangerous capability potential drastically facilitate bioterrorism released internet cannot deleted everywhere needed recommend prerelease audit costbenefit analysis framework artificial intelligence project guided ethical consideration design development implementation ai system ai framework care act framework developed alan turing institute based sum value outline four main ethical dimension defined follows respect dignity individual people connect people sincerely openly inclusively care wellbeing everyone protect social value justice public interest development ethical framework include decided upon asilomar conference montreal declaration responsible ai ieees ethic autonomous system initiative among others however principle without criticism especially regarding people chosen contribute framework promotion wellbeing people community technology affect requires consideration social ethical implication stage ai system design development implementation collaboration job role data scientist product manager data engineer domain expert delivery manager uk ai safety institute released testing toolset called inspect ai safety evaluation available mit opensource licence freely available github improved thirdparty package used evaluate ai model range area including core knowledge ability reason autonomous capability regulation regulation artificial intelligence development public sector policy law promoting regulating ai therefore related broader regulation algorithm regulatory policy landscape ai emerging issue jurisdiction globally according ai index stanford annual number airelated law passed survey country jumped one passed passed alone country adopted dedicated strategy ai eu member state released national ai strategy canada china india japan mauritius russian federation saudi arabia united arab emirate u vietnam others process elaborating ai strategy including bangladesh malaysia tunisia global partnership artificial intelligence launched june stating need ai developed accordance human right democratic value ensure public confidence trust technology henry kissinger eric schmidt daniel huttenlocher published joint statement november calling government commission regulate ai openai leader published recommendation governance superintelligence believe may happen less year united nation also launched advisory body provide recommendation ai governance body comprises technology company executive government official academic council europe created first international legally binding treaty ai called framework convention artificial intelligence human right democracy rule law adopted european union united state united kingdom signatory ipsos survey attitude towards ai varied greatly country chinese citizen american agreed product service using ai benefit drawback reutersipsos poll found american agree disagree ai pose risk humanity fox news poll american thought important additional thought somewhat important federal government regulate ai versus responding important responding important november first global ai safety summit held bletchley park uk discus near far term risk ai possibility mandatory voluntary regulatory framework country including united state china european union issued declaration start summit calling international cooperation manage challenge risk artificial intelligence may ai seoul summit global ai tech company agreed safety commitment development ai history study mechanical formal reasoning began philosopher mathematician antiquity study logic led directly alan turing theory computation suggested machine shuffling symbol simple could simulate conceivable form mathematical reasoning along concurrent discovery cybernetics information theory neurobiology led researcher consider possibility building electronic brain developed several area research would become part ai mcculloch pitt design artificial neuron turing influential paper computing machinery intelligence introduced turing test showed machine intelligence plausible field ai research founded workshop dartmouth college attendee became leader ai research student produced program press described astonishing computer learning checker strategy solving word problem algebra proving logical theorem speaking english artificial intelligence laboratory set number british u university latter early researcher convinced method would eventually succeed creating machine general intelligence considered goal field herbert simon predicted machine capable within twenty year work man marvin minsky agreed writing within generation problem creating artificial intelligence substantially solved however underestimated difficulty problem u british government cut exploratory research response criticism sir james lighthill ongoing pressure u congress fund productive project minsky paperts book perceptrons understood proving artificial neural network would never useful solving realworld task thus discrediting approach altogether ai winter period obtaining funding ai project difficult followed early ai research revived commercial success expert system form ai program simulated knowledge analytical skill human expert market ai reached billion dollar time japan fifth generation computer project inspired u british government restore funding academic research however beginning collapse lisp machine market ai fell disrepute second longerlasting winter began point ai funding gone project used highlevel symbol represent mental object like plan goal belief known fact researcher began doubt approach would able imitate process human cognition especially perception robotics learning pattern recognition began look subsymbolic approach rodney brook rejected representation general focussed directly engineering machine move survive judea pearl lotfi zadeh others developed method handled incomplete uncertain information making reasonable guess rather precise logic important development revival connectionism including neural network research geoffrey hinton others yann lecun successfully showed convolutional neural network recognize handwritten digit first many successful application neural network ai gradually restored reputation late early st century exploiting formal mathematical method finding specific solution specific problem narrow formal focus allowed researcher produce verifiable result collaborate field statistic economics mathematics solution developed ai researcher widely used although rarely described artificial intelligence tendency known ai effect however several academic researcher became concerned ai longer pursuing original goal creating versatile fully intelligent machine beginning around founded subfield artificial general intelligence agi several wellfunded institution deep learning began dominate industry benchmark adopted throughout field many specific task method abandoned deep learning success based hardware improvement faster computer graphic processing unit cloud computing access large amount data including curated datasets imagenet deep learning success led enormous increase interest funding ai amount machine learning research measured total publication increased year issue fairness misuse technology catapulted center stage machine learning conference publication vastly increased funding became available many researcher refocussed career issue alignment problem became serious field academic study late early agi company began deliver program created enormous interest alphago developed deepmind beat world champion go player program taught game rule developed strategy gpt large language model released openai capable generating highquality humanlike text chatgpt launched november became fastestgrowing consumer software application history gaining million user two month marked widely regarded ai breakout year bringing public consciousness program others inspired aggressive ai boom large company began investing billion dollar ai research according ai impact u billion annually invested ai around u alone new u computer science phd graduate specialized ai airelated u job opening existed according pitchbook research newly funded startup claimed ai company philosophy philosophical debate historically sought determine nature intelligence make intelligent machine another major focus whether machine conscious associated ethical implication many topic philosophy relevant ai epistemology free rapid advancement intensified public discussion philosophy ethic ai defining artificial intelligence alan turing wrote propose consider question machine think advised changing question whether machine think whether possible machinery show intelligent behaviour devised turing test measure ability machine simulate human conversation since observe behavior machine matter actually thinking literally mind turing note determine thing people usual polite convention everyone think russell norvig agree turing intelligence must defined term external behavior internal structure however critical test requires machine imitate human aeronautical engineering text wrote define goal field making machine fly exactly like pigeon fool pigeon ai founder john mccarthy agreed writing artificial intelligence definition simulation human intelligence mccarthy defines intelligence computational part ability achieve goal world another ai founder marvin minsky similarly describes ability solve hard problem leading ai textbook defines study agent perceive environment take action maximize chance achieving defined goal definition view intelligence term welldefined problem welldefined solution difficulty problem performance program direct measure intelligence machineand philosophical discussion required may even possible another definition adopted google major practitioner field ai definition stipulates ability system synthesize information manifestation intelligence similar way defined biological intelligence author suggested practice definition ai vague difficult define contention whether classical algorithm categorised ai many company early ai boom using term marketing buzzword often even actually use ai material way debate whether large language model exhibit genuine intelligence merely simulate imitating human text evaluating approach ai established unifying theory paradigm guided ai research history unprecedented success statistical machine learning eclipsed approach much source especially business world use term artificial intelligence mean machine learning neural network approach mostly subsymbolic soft narrow critic argue question may revisited future generation ai researcher symbolic ai limit symbolic ai gofai simulated highlevel conscious reasoning people use solve puzzle express legal reasoning mathematics highly successful intelligent task algebra iq test newell simon proposed physical symbol system hypothesis physical symbol system necessary sufficient mean general intelligent action however symbolic approach failed many task human solve easily learning recognizing object commonsense reasoning moravecs paradox discovery highlevel intelligent task easy ai low level instinctive task extremely difficult philosopher hubert dreyfus argued since human expertise depends unconscious instinct rather conscious symbol manipulation feel situation rather explicit symbolic knowledge although argument ridiculed ignored first presented eventually ai research came agree issue resolved subsymbolic reasoning make many inscrutable mistake human intuition algorithmic bias critic noam chomsky argue continuing research symbolic ai still necessary attain general intelligence part subsymbolic ai move away explainable ai difficult impossible understand modern statistical ai program made particular decision emerging field neurosymbolic artificial intelligence attempt bridge two approach neat v scruffy neats hope intelligent behavior described using simple elegant principle logic optimization neural network scruffies expect necessarily requires solving large number unrelated problem neats defend program theoretical rigor scruffies rely mainly incremental testing see work issue actively discussed eventually seen irrelevant modern ai element soft v hard computing finding provably correct optimal solution intractable many important problem soft computing set technique including genetic algorithm fuzzy logic neural network tolerant imprecision uncertainty partial truth approximation soft computing introduced late successful ai program st century example soft computing neural network narrow v general ai ai researcher divided whether pursue goal artificial general intelligence superintelligence directly solve many specific problem possible narrow ai hope solution lead indirectly field longterm goal general intelligence difficult define difficult measure modern ai verifiable success focusing specific problem specific solution subfield artificial general intelligence study area exclusively machine consciousness sentience mind settled consensus philosophy mind whether machine mind consciousness mental state sense human being issue considers internal experience machine rather external behavior mainstream ai research considers issue irrelevant affect goal field build machine solve problem using intelligence russell norvig add additional project making machine conscious exactly way human one equipped take however question become central philosophy mind also typically central question issue artificial intelligence fiction consciousness david chalmers identified two problem understanding mind named hard easy problem consciousness easy problem understanding brain process signal make plan control behavior hard problem explaining feel feel like anything assuming right thinking truly feel like something dennetts consciousness illusionism say illusion human information processing easy explain human subjective experience difficult explain example easy imagine colorblind person learned identify object field view red clear would required person know red look like computationalism functionalism computationalism position philosophy mind human mind information processing system thinking form computing computationalism argues relationship mind body similar identical relationship software hardware thus may solution mindbody problem philosophical position inspired work ai researcher cognitive scientist originally proposed philosopher jerry fodor hilary putnam philosopher john searle characterized position strong ai appropriately programmed computer right input output would thereby mind exactly sense human being mind searle challenge claim chinese room argument attempt show even computer capable perfectly simulating human behavior would mind ai welfare right difficult impossible reliably evaluate whether advanced ai sentient ability feel degree significant chance given machine feel suffer may entitled certain right welfare protection measure similarly animal sapience set capacity related high intelligence discernment selfawareness may provide another moral basis ai right robot right also sometimes proposed practical way integrate autonomous agent society european union considered granting electronic personhood capable ai system similarly legal status company would conferred right also responsibility critic argued granting right ai system would downplay importance human right legislation focus user need rather speculative futuristic scenario also noted robot lacked autonomy take part society progress ai increased interest topic proponent ai welfare right often argue ai sentience emerges would particularly easy deny warn may moral blind spot analogous slavery factory farming could lead largescale suffering sentient ai created carelessly exploited future superintelligence singularity superintelligence hypothetical agent would possess intelligence far surpassing brightest gifted human mind research artificial general intelligence produced sufficiently intelligent software might able reprogram improve improved software would even better improving leading j good called intelligence explosion vernor vinge called singularity however technology cannot improve exponentially indefinitely typically follow sshaped curve slowing reach physical limit technology transhumanism robot designer han moravec cyberneticist kevin warwick inventor ray kurzweil predicted human machine may merge future cyborg capable powerful either idea called transhumanism root writing aldous huxley robert ettinger edward fredkin argues artificial intelligence next step evolution idea first proposed samuel butler darwin among machine far back expanded upon george dyson book darwin among machine evolution global intelligence fiction thoughtcapable artificial being appeared storytelling device since antiquity persistent theme science fiction common trope work began mary shelley frankenstein human creation becomes threat master includes work arthur c clarkes stanley kubrick space odyssey hal murderous computer charge discovery one spaceship well terminator matrix contrast rare loyal robot gort day earth stood still bishop alien less prominent popular culture isaac asimov introduced three law robotics many story notably multivac superintelligent computer asimov law often brought lay discussion machine ethic almost artificial intelligence researcher familiar asimov law popular culture generally consider law useless many reason one ambiguity several work use ai force u confront fundamental question make u human showing u artificial being ability feel thus suffer appears karel apeks rur film ai artificial intelligence ex machina well novel android dream electric sheep philip k dick dick considers idea understanding human subjectivity altered technology created artificial intelligence see also artificial consciousness field cognitive science artificial intelligence election use impact ai political election artificial intelligence content detection software detect aigenerated content association advancement artificial intelligence aaai behavior selection algorithm algorithm selects action intelligent agent business process automation automation business process casebased reasoning process solving new problem based solution similar past problem computational intelligence ability computer learn specific task data experimental observation digital immortality hypothetical concept storing personality digital form emergent algorithm algorithm exhibiting emergent behavior female gendering ai technology gender bias digital technologypages displaying short description redirect target glossary artificial intelligence list definition term concept commonly used study artificial intelligence intelligence amplification use information technology augment human intelligence intelligent agent software agent act autonomously intelligent automation software process combine robotic process automation artificial intelligence list artificial intelligence journal list artificial intelligence project mind uploading hypothetical process digitally emulating brain organoid intelligence use brain cell brain organoids intelligent computing robotic process automation form business process automation technology last day welsh science fiction novel wetware computer computer composed organic material darwin eu european union initiative coordinated european medicine agency ema generate utilize realworld evidence rwe support evaluation supervision medicine across eu explanatory note reference ai textbook two widely used textbook see open syllabus russell stuart j norvig peter artificial intelligence modern approach th ed hoboken pearson isbn lccn rich elaine knight kevin nair shivashankar b artificial intelligence rd ed new delhi tata mcgraw hill india isbn four widely used ai textbook textbook ertel wolfgang introduction artificial intelligence nd ed springer isbn ciaramella alberto ciaramella marco introduction artificial intelligence data analysis generative ai st ed intellisemantic edition isbn history ai source reading external link artificial intelligence internet encyclopedia philosophy', 'computer science study computation information automation computer science span theoretical discipline algorithm theory computation information theory applied discipline including design implementation hardware software algorithm data structure central computer science theory computation concern abstract model computation general class problem solved using field cryptography computer security involve studying mean secure communication preventing security vulnerability computer graphic computational geometry address generation image programming language theory considers different way describe computational process database theory concern management repository data humancomputer interaction investigates interface human computer interact software engineering focus design principle behind developing software area operating system network embedded system investigate principle design behind complex system computer architecture describes construction computer component computeroperated equipment artificial intelligence machine learning aim synthesize goalorientated process problemsolving decisionmaking environmental adaptation planning learning found human animal within artificial intelligence computer vision aim understand process image video data natural language processing aim understand process textual linguistic data fundamental concern computer science determining cannot automated turing award generally recognized highest distinction computer science history earliest foundation would become computer science predate invention modern digital computer machine calculating fixed numerical task abacus existed since antiquity aiding computation multiplication division algorithm performing computation existed since antiquity even development sophisticated computing equipment wilhelm schickard designed constructed first working mechanical calculator gottfried leibniz demonstrated digital mechanical calculator called stepped reckoner leibniz may considered first computer scientist information theorist various reason including fact documented binary number system thomas de colmar launched mechanical calculator industry invented simplified arithmometer first calculating machine strong enough reliable enough used daily office environment charles babbage started design first automatic mechanical calculator difference engine eventually gave idea first programmable mechanical calculator analytical engine started developing machine less two year sketched many salient feature modern computer crucial step adoption punched card system derived jacquard loom making infinitely programmable translation french article analytical engine ada lovelace wrote one many note included algorithm compute bernoulli number considered first published algorithm ever specifically tailored implementation computer around herman hollerith invented tabulator used punched card process statistical information eventually company became part ibm following babbage although unaware earlier work percy ludgate published nd two design mechanical analytical engine history spanish engineer leonardo torres quevedo published essay automatic designed inspired babbage theoretical electromechanical calculating machine controlled readonly program paper also introduced idea floatingpoint arithmetic celebrate th anniversary invention arithmometer torres presented paris electromechanical arithmometer prototype demonstrated feasibility electromechanical analytical engine command could typed result printed automatically one hundred year babbages impossible dream howard aiken convinced ibm making kind punched card equipment also calculator business develop giant programmable calculator asccharvard mark based babbages analytical engine used card central computing unit machine finished hailed babbages dream come true development new powerful computing machine atanasoffberry computer eniac term computer came refer machine rather human predecessor became clear computer could used mathematical calculation field computer science broadened study computation general ibm founded watson scientific computing laboratory columbia university new york city renovated fraternity house manhattan west side ibms first laboratory devoted pure science lab forerunner ibms research division today operates research facility around world ultimately close relationship ibm columbia university instrumental emergence new scientific discipline columbia offering one first academiccredit course computer science computer science began established distinct academic discipline early world first computer science degree program cambridge diploma computer science began university cambridge computer laboratory first computer science department united state formed purdue university since practical computer became available many application computing become distinct area study right etymology scope although first proposed term computer science appears article communication acm louis fein argues creation graduate school computer science analogous creation harvard business school louis justifies name arguing like management science subject applied interdisciplinary nature characteristic typical academic discipline effort others numerical analyst george forsythe successful university went create department starting purdue despite name significant amount computer science involve study computer several alternative name proposed certain department major university prefer term computing science emphasize precisely difference danish scientist peter naur suggested term datalogy reflect fact scientific discipline revolves around data data treatment necessarily involving computer first scientific institution use term department datalogy university copenhagen founded peter naur first professor datalogy term used mainly scandinavian country alternative term also proposed naur data science used multidisciplinary field data analysis including statistic database early day computing number term practitioner field computing suggested albeit facetiously communication acmturingineer turologist flowchartsman applied metamathematician applied epistemologist three month later journal comptologist suggested followed next year hypologist term computics also suggested europe term derived contracted translation expression automatic information eg informazione automatica italian information mathematics often used eg informatique french informatik german informatica italian dutch informtica spanish portuguese informatika slavic language hungarian pliroforiki mean informatics greek similar word also adopted uk school informatics university edinburgh u however informatics linked applied computing computing context another domain folkloric quotation often attributed tobut almost certainly first formulated byedsger dijkstra state computer science computer astronomy telescope design deployment computer computer system generally considered province discipline computer science example study computer hardware usually considered part computer engineering study commercial computer system deployment often called information technology information system however exchange idea various computerrelated discipline computer science research also often intersects discipline cognitive science linguistics mathematics physic biology earth science statistic philosophy logic computer science considered much closer relationship mathematics many scientific discipline observer saying computing mathematical science early computer science strongly influenced work mathematician kurt gdel alan turing john von neumann rzsa pter alonzo church continues useful interchange idea two field area mathematical logic category theory domain theory algebra relationship computer science software engineering contentious issue muddied dispute term software engineering mean computer science defined david parnas taking cue relationship engineering science discipline claimed principal focus computer science studying property computation general principal focus software engineering design specific computation achieve practical goal making two separate complementary discipline academic political funding aspect computer science tend depend whether department formed mathematical emphasis engineering emphasis computer science department mathematics emphasis numerical orientation consider alignment computational science type department tend make effort bridge field educationally across research philosophy epistemology computer science despite word science name debate whether computer science discipline science mathematics engineering allen newell herbert simon argued computer science empirical discipline would called experimental science like astronomy economics geology unique form observation experience fit narrow stereotype experimental method nonetheless experiment new machine built experiment actually constructing machine pose question nature listen answer observing machine operation analyzing analytical measurement mean available since argued computer science classified empirical science since make use empirical testing evaluate correctness program problem remains defining law theorem computer science exist defining nature experiment computer science proponent classifying computer science engineering discipline argue reliability computational system investigated way bridge civil engineering airplane aerospace engineering also argue empirical science observe presently exists computer science observes possible exist scientist discover law observation proper law found computer science instead concerned creating phenomenon proponent classifying computer science mathematical discipline argue computer program physical realization mathematical entity program deductively reasoned mathematical formal method computer scientist edsger w dijkstra tony hoare regard instruction computer program mathematical sentence interpret formal semantics programming language mathematical axiomatic system paradigm computer science number computer scientist argued distinction three separate paradigm computer science peter wegner argued paradigm science technology mathematics peter dennings working group argued theory abstraction modeling design amnon h eden described rationalist paradigm treat computer science branch mathematics prevalent theoretical computer science mainly employ deductive reasoning technocratic paradigm might found engineering approach prominently software engineering scientific paradigm approach computerrelated artifact empirical perspective natural science identifiable branch artificial intelligence computer science focus method involved design specification programming verification implementation testing humanmade computing system field discipline computer science span range topic theoretical study algorithm limit computation practical issue implementing computing system hardware software csab formerly called computing science accreditation boardwhich made representative association computing machinery acm ieee computer society ieee csidentifies four area considers crucial discipline computer science theory computation algorithm data structure programming methodology language computer element architecture addition four area csab also identifies field software engineering artificial intelligence computer networking communication database system parallel computation distributed computation humancomputer interaction computer graphic operating system numerical symbolic computation important area computer science theoretical computer science theoretical computer science mathematical abstract spirit derives motivation practical everyday computation aim understand nature computation consequence understanding provide efficient methodology theory computation according peter denning fundamental question underlying computer science automated theory computation focused answering fundamental question computed amount resource required perform computation effort answer first question computability theory examines computational problem solvable various theoretical model computation second question addressed computational complexity theory study time space cost associated different approach solving multitude computational problem famous p np problem one millennium prize problem open problem theory computation information coding theory information theory closely related probability statistic related quantification information developed claude shannon find fundamental limit signal processing operation compressing data reliably storing communicating data coding theory study property code system converting information one form another fitness specific application code used data compression cryptography error detection correction recently also network coding code studied purpose designing efficient reliable data transmission method data structure algorithm data structure algorithm study commonly used computational method computational efficiency programming language theory formal method programming language theory branch computer science deal design implementation analysis characterization classification programming language individual feature fall within discipline computer science depending affecting mathematics software engineering linguistics active research area numerous dedicated academic journal formal method particular kind mathematically based technique specification development verification software hardware system use formal method software hardware design motivated expectation engineering discipline performing appropriate mathematical analysis contribute reliability robustness design form important theoretical underpinning software engineering especially safety security involved formal method useful adjunct software testing since help avoid error also give framework testing industrial use tool support required however high cost using formal method mean usually used development highintegrity lifecritical system safety security utmost importance formal method best described application fairly broad variety theoretical computer science fundamental particular logic calculus formal language automaton theory program semantics also type system algebraic data type problem software hardware specification verification applied computer science computer graphic visualization computer graphic study digital visual content involves synthesis manipulation image data study connected many field computer science including computer vision image processing computational geometry heavily applied field special effect video game image sound processing information take form image sound video multimedia bit information streamed via signal processing central notion informatics european view computing study information processing algorithm independently type information carrier whether electrical mechanical biological field play important role information theory telecommunication information engineering application medical image computing speech synthesis among others lower bound complexity fast fourier transform algorithm one unsolved problem theoretical computer science computational science finance engineering scientific computing computational science field study concerned constructing mathematical model quantitative analysis technique using computer analyze solve scientific problem major usage scientific computing simulation various process including computational fluid dynamic physical electrical electronic system circuit society social situation notably war game along habitat interaction among biological cell modern computer enable optimization design complete aircraft notable electrical electronic circuit design spice well software physical realization new modified design latter includes essential design software integrated circuit humancomputer interaction humancomputer interaction hci field study research concerned design use computer system mainly based analysis interaction human computer interface hci several subfields focus relationship emotion social behavior brain activity computer software engineering software engineering study designing implementing modifying software order ensure high quality affordable maintainable fast build systematic approach software design involving application engineering practice software software engineering deal organizing analyzing softwareit deal creation manufacture new software internal arrangement maintenance example software testing system engineering technical debt software development process artificial intelligence artificial intelligence ai aim required synthesize goalorientated process problemsolving decisionmaking environmental adaptation learning communication found human animal origin cybernetics dartmouth conference artificial intelligence research necessarily crossdisciplinary drawing area expertise applied mathematics symbolic logic semiotics electrical engineering philosophy mind neurophysiology social intelligence ai associated popular mind robotic development main field practical application embedded component area software development require computational understanding starting point late alan turing question computer think question remains effectively unanswered although turing test still used assess computer output scale human intelligence automation evaluative predictive task increasingly successful substitute human monitoring intervention domain computer application involving complex realworld data computer system computer architecture microarchitecture computer architecture digital computer organization conceptual design fundamental operational structure computer system focus largely way central processing unit performs internally access address memory computer engineer study computational logic design computer hardware individual processor component microcontrollers personal computer supercomputer embedded system term architecture computer literature traced work lyle r johnson frederick p brook jr member machine organization department ibms main research center concurrent parallel distributed computing concurrency property system several computation executing simultaneously potentially interacting number mathematical model developed general concurrent computation including petri net process calculus parallel random access machine model multiple computer connected network using concurrency known distributed system computer within distributed system private memory information exchanged achieve common goal computer network branch computer science aim study construction behavior computer network address performance resilience security scalability costeffectiveness along variety service provide computer security cryptography computer security branch computer technology objective protecting information unauthorized access disruption modification maintaining accessibility usability system intended user historical cryptography art writing deciphering secret message modern cryptography scientific study problem relating distributed computation attacked technology studied modern cryptography include symmetric asymmetric encryption digital signature cryptographic hash function keyagreement protocol blockchain zeroknowledge proof garbled circuit database data mining database intended organize store retrieve large amount data easily digital database managed using database management system store create maintain search data database model query language data mining process discovering pattern large data set discovery philosopher computing bill rapaport noted three great insight computer science gottfried wilhelm leibniz george boole alan turing claude shannon samuel mors insight two object computer deal order represent anything information computable problem represented using bistable pair flipflop two easily distinguishable state onoff magnetizeddemagnetized highvoltagelowvoltage etc alan turing insight five action computer perform order anything every algorithm expressed language computer consisting five basic instruction move left one location move right one location read symbol current location print current location print current location corrado bhm giuseppe jacopinis insight three way combining action complex one needed order computer anything three rule needed combine set basic instruction complex one sequence first selection suchandsuch case else repetition suchandsuch case three rule boehm jacopinis insight simplified use goto mean elementary structured programming programming paradigm programming language used accomplish different task different way common programming paradigm include functional programming style building structure element computer program treat computation evaluation mathematical function avoids state mutable data declarative programming paradigm mean programming done expression declaration instead statement imperative programming programming paradigm us statement change program state much way imperative mood natural language express command imperative program consists command computer perform imperative programming focus describing program operates objectoriented programming programming paradigm based concept object may contain data form field often known attribute code form procedure often known method feature object object procedure access often modify data field object associated thus objectoriented computer program made object interact one another serviceoriented programming programming paradigm us service unit computer work design implement integrated business application mission critical software program many language offer support multiple paradigm making distinction matter style technical capability research conference important event computer science research conference researcher public private sector present recent work meet unlike academic field computer science prestige conference paper greater journal publication one proposed explanation quick development relatively new field requires rapid review distribution result task better handled conference journal see also note reference reading external link dblp computer science bibliography association computing machinery institute electrical electronics engineer']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmeatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    processed_words = [lemmeatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(processed_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a369c099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5238 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 382)\t0.8697331752334779\n",
      "  (0, 903)\t0.011600328464334625\n",
      "  (0, 866)\t0.28342742451698\n",
      "  (0, 871)\t0.02500830216326294\n",
      "  (0, 776)\t0.004910265858239363\n",
      "  (0, 472)\t0.027006462220316493\n",
      "  (0, 385)\t0.06295149987608824\n",
      "  (0, 270)\t0.029461595149436175\n",
      "  (0, 215)\t0.0392821268659149\n",
      "  (0, 552)\t0.03205636263284271\n",
      "  (0, 109)\t0.008763036744009368\n",
      "  (0, 932)\t0.011803406226766544\n",
      "  (0, 402)\t0.016672201442175294\n",
      "  (0, 992)\t0.004808454394926405\n",
      "  (0, 572)\t0.1361563117777649\n",
      "  (0, 761)\t0.002900082116083656\n",
      "  (0, 982)\t0.07105201184404958\n",
      "  (0, 193)\t0.005320825107854087\n",
      "  (0, 848)\t0.013144555116014053\n",
      "  (0, 310)\t0.009836171855638787\n",
      "  (0, 573)\t0.017630999448063487\n",
      "  (0, 738)\t0.004808454394926405\n",
      "  (0, 839)\t0.021639578082405332\n",
      "  (0, 271)\t0.055574004807250985\n",
      "  (0, 510)\t0.007250205290209141\n",
      "  :\t:\n",
      "  (8, 492)\t0.10921317898566449\n",
      "  (8, 118)\t0.023402824068356677\n",
      "  (8, 869)\t0.023402824068356677\n",
      "  (8, 718)\t0.16381976847849672\n",
      "  (8, 575)\t0.015601882712237785\n",
      "  (8, 25)\t0.015601882712237785\n",
      "  (8, 943)\t0.046805648136713354\n",
      "  (8, 183)\t0.1716207098346156\n",
      "  (8, 254)\t0.15601882712237786\n",
      "  (8, 901)\t0.015601882712237785\n",
      "  (8, 751)\t0.007800941356118893\n",
      "  (8, 527)\t0.039004706780594464\n",
      "  (8, 189)\t0.039004706780594464\n",
      "  (8, 181)\t0.11701412034178338\n",
      "  (8, 26)\t0.046805648136713354\n",
      "  (8, 749)\t0.007800941356118893\n",
      "  (8, 462)\t0.007800941356118893\n",
      "  (8, 223)\t0.015601882712237785\n",
      "  (8, 961)\t0.007800941356118893\n",
      "  (8, 186)\t0.023402824068356677\n",
      "  (8, 413)\t0.054606589492832244\n",
      "  (8, 58)\t0.039004706780594464\n",
      "  (8, 74)\t0.017944106612896367\n",
      "  (8, 551)\t0.08074847975803366\n",
      "  (8, 574)\t0.017944106612896367\n",
      "TF-IDF maxtrix created successfully\n",
      "Shape of the matrix: (9, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"TF-IDF maxtrix created successfully\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6761de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "## The below line is functionality identical to the above line\n",
    "# kmeans = KMeans(nclusters= k, init=\"k-means++, random_state=42, n_init=10\")\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "#Get the cluster assignments for each document\n",
    "labels = kmeans.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c06cb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The new document belongs to cluster: 1\n"
     ]
    }
   ],
   "source": [
    "sets = \"An algorithm is a set of well-defined instructions designed to perform a specific task or solve a computational problem. In computer science,\"\n",
    "\" the study of algorithms is fundamental to creating efficient and scalable software. Data structures, such as arrays and hash tables, are used to organize\"\n",
    "\" data in a way that allows these algorithms to access and manipulate it effectively.\"\n",
    "\n",
    "processed_sets = preprocess_text(sets)\n",
    "new_tfidf_vector = vectorizer.transform([processed_sets])\n",
    "predicted_label = kmeans.predict(new_tfidf_vector)\n",
    "\n",
    "print(f\"\\nThe new document belongs to cluster: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
